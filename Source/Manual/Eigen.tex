
\chapter{BLAS Support (based on Eigen)}
\label{BLAS Support} 


The Eigen reference is \cite{Guennebaud2010}


The Basic Linear Algebra Subprograms (BLAS) define a set of fundamental operations on vectors and matrices which can be used to create optimized higher-level linear algebra functionality. 
Specifications for Level 1, Level 2 and Level 3 BLAS can be found in \cite{Lawson_1979, Dongarra_1988, Dongarra_1990}.

Based on BLAS in LAPACK, which is described in \cite{Anderson_1999, Barker_2001}.

\vspace{0.3cm}
The library provides high-level interface for blas operations on vectors and matrices. This should satisfy the needs of most users. Note that currently matrices are implemented using dense-storage so the interface only includes the corresponding dense-storage blas functions. The full blas functionality for band-format and packed-format matrices will be available in later versions of the library. 

\vspace{0.3cm}
There are three levels of blas operations, 

Level 1: Vector operations, e.g. y = ax + y 

Level 2: Matrix-vector operations, e.g. y = αAx + βy 

Level 3: Matrix-matrix operations, e.g. C = αAB + C 

\vspace{0.3cm}
Each routine has a name which specifies the operation, the type of matrices involved and their precisions. Some of the most common operations and their names are given below, 

\vspace{0.3cm}
DOT scalar product, xT y 

AXPY vector sum, αx + y 

MV matrix-vector product, Ax 

SV matrix-vector solve, inv(A)x 

MM matrix-matrix product, AB 

SM matrix-matrix solve, inv(A)B 

\vspace{0.3cm}
The types of matrices are, 

GE general 

GB general band 

SY symmetric 

SB symmetric band 

SP symmetric packed 

HE hermitian 

HB hermitian band 

HP hermitian packed 

TR triangular 

TB triangular band 

TP triangular packed 

Each operation is defined for four precisions, 

S single real 

D double real 

C single complex 

Z double complex 

\vspace{0.3cm}
Thus, for example, the name sgemm stands for single-precision general matrix-matrix multiply and zgemm stands for double-precision complex matrix-matrix multiply.

Book reference: \cite{Golub1996}

Book reference: \cite{Bernstein_2009}

Book reference: \cite{Seber_2008}




\section{BLAS Level 1 Support and related Functions} 
\label{BLASSupportLevel1}

\subsection{Vector-Vector Product}

\begin{mpFunctionsExtract}
	\mpFunctionTwo
	{RDot? mpNum? the real scalar product $\boldsymbol{x}^T \boldsymbol{y}$ for the real vectors $\boldsymbol{x}$ and $\boldsymbol{y}$.}
	{x? mpNum[]? A vector of real numbers.}
	{y? mpNum[]? A vector of real numbers.}
\end{mpFunctionsExtract}

\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionTwo
	{cplxDotu? mpNum? the complex  scalar product $\boldsymbol{x}^T \boldsymbol{y}$ for the complex  vectors $\boldsymbol{x}$ and $\boldsymbol{y}$.}
	{x? mpNum[]? A vector of complex numbers.}
	{y? mpNum[]? A vector of complex numbers.}
\end{mpFunctionsExtract}


\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionTwo
	{cplxDotc? mpNum? the complex conjugate scalar product $\boldsymbol{x}^H \boldsymbol{y}$ for the complex  vectors $\boldsymbol{x}$ and $\boldsymbol{y}$.}
	{x? mpNum[]? A vector of complex numbers.}
	{y? mpNum[]? A vector of complex numbers.}
\end{mpFunctionsExtract}




\subsection{Euclidian Norm}

\begin{mpFunctionsExtract}
	\mpFunctionTwo
	{RNrm2? mpNum? the Euclidean norm $||\boldsymbol{x}||_2$ of the real vector $\boldsymbol{x}$.}
	{x? mpNum[]? A vector of real numbers.}
	{y? mpNum[]? A vector of real numbers.}
\end{mpFunctionsExtract}

\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionTwo
	{cplxNrm2? mpNum? the Euclidean norm $||\boldsymbol{x}||_2$ of the complex vector $\boldsymbol{x}$.}
	{x? mpNum[]? A vector of complex numbers.}
	{y? mpNum[]? A vector of complex numbers.}
\end{mpFunctionsExtract}




\subsection{Absolute Sum}

\begin{mpFunctionsExtract}
	\mpFunctionTwo
	{RAsum? mpNum? the the absolute sum of the elements of the real vector $\boldsymbol{x}$.}
	{x? mpNum[]? A vector of real numbers.}
	{y? mpNum[]? A vector of real numbers.}
\end{mpFunctionsExtract}

\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionTwo
	{cplxAsum? mpNum? the  sum of the magnitudes of the real and imaginary parts of the complex vector $\boldsymbol{x}$.}
	{x? mpNum[]? A vector of complex numbers.}
	{y? mpNum[]? A vector of complex numbers.}
\end{mpFunctionsExtract}




\subsection{Addition}

\begin{mpFunctionsExtract}
	\mpFunctionThree
	{RAxpy? mpNum?  the sum $\alpha \boldsymbol{x} + \boldsymbol{y}$ for the real scalar $\alpha$ and the real vectors $\boldsymbol{x}$ and $\boldsymbol{y}$.}
	{$\alpha$? mpNum? A real scalar.}
	{x? mpNum[]? A vector of real numbers.}
	{y? mpNum[]? A vector of real numbers.}
\end{mpFunctionsExtract}

\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionThree
	{cplxAxpy? mpNum? the sum $\alpha \boldsymbol{x} + \boldsymbol{y}$ for the complex scalar $\alpha$ and the complex vectors $\boldsymbol{x}$ and $\boldsymbol{y}$.}
	{$\alpha$? mpNum? A complex scalar.}
	{x? mpNum[]? A vector of complex numbers.}
	{y? mpNum[]? A vector of complex numbers.}
\end{mpFunctionsExtract}




\section{BLAS Level 2 Support}
\label{BLASSupportLevel2}

\subsection{Matrix-Vector Product and Sum (General Matrix)}

\begin{mpFunctionsExtract}
	\mpFunctionSix
	{RGemv? mpNum? the matrix-vector product and sum for a general matrix.}
	{TransA? Integer? An indicator specifying the multiplication.}
	{$\alpha$? mpNum? A real scalar.}
	{A? mpNum[,]? A matrix of real numbers.}
	{x? mpNum[]? A vector of real numbers.}
	{$\beta$? mpNum? A real scalar.}
	{y? mpNum[]? A vector of real numbers.}
\end{mpFunctionsExtract}

\vspace{0.3cm}
For the real scalars $\alpha$ and $\beta$, the real vectors $\boldsymbol{x}$ and $\boldsymbol{y}$, and the real matrix $\boldsymbol{A}$, the function \textsf{RGemv} computes the matrix-vector product and sum 
\begin{equation}
\textsf{RGemv}=\begin{cases}
\alpha \boldsymbol{A} \boldsymbol{x} + \beta \boldsymbol{y}, & \text{for } \textsf{TransA = mpBlasNoTrans},\\
\alpha \boldsymbol{A}^T \boldsymbol{x} + \beta \boldsymbol{y}, & \text{for } \textsf{TransA = mpBlasTrans}.
\end{cases}
\end{equation}


\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionSix
	{cplxGemv? mpNum? the matrix-vector product and sum for a general matrix.}
	{TransA? Integer? An indicator specifying the multiplication.}
	{$\alpha$? mpNum? A complex scalar.}
	{A? mpNum[,]? A matrix of complex numbers.}
	{x? mpNum[]? A vector of complex numbers.}
	{$\beta$? mpNum? A complex scalar.}
	{y? mpNum[]? A vector of complex numbers.}
\end{mpFunctionsExtract}


\vspace{0.3cm}
For the complex scalars $\alpha$ and $\beta$, the complex vectors $\boldsymbol{x}$ and $\boldsymbol{y}$, and the complex matrix $\boldsymbol{A}$, the function \textsf{cplxGemv} computes the matrix-vector product and sum 
\begin{equation}
\textsf{cplxGemv}=\begin{cases}
\alpha \boldsymbol{A} \boldsymbol{x} + \beta \boldsymbol{y}, & \text{for } \textsf{TransA = mpBlasNoTrans},\\
\alpha \boldsymbol{A}^T \boldsymbol{x} + \beta \boldsymbol{y}, & \text{for } \textsf{TransA = mpBlasTrans},\\
\alpha \boldsymbol{A}^H \boldsymbol{x} + \beta \boldsymbol{y}, & \text{for } \textsf{TransA = mpBlasConjTrans}.
\end{cases}
\end{equation}





\newpage
\subsection{Matrix-Vector Product (Triangular Matrix)}

\begin{mpFunctionsExtract}
	\mpFunctionFive
	{RTrmv? mpNum?  the matrix-vector product for a triangular matrix.}
	{Uplo? Integer? An indicator specifying whether the upper or lower triangle will be used.}
	{TransA? Integer? An indicator specifying the multiplication.}
	{Diag? Integer? An indicator specifying the use of the diagonal.}
	{A? mpNum[,]? A matrix of real numbers.}
	{x? mpNum[]? A vector of real numbers.}
\end{mpFunctionsExtract}

\vspace{0.3cm}
For the real vector $\boldsymbol{x}$ and the real triangular matrix $\boldsymbol{A}$, the function \textsf{RTrmv} computes the matrix-vector product
\begin{equation}
\textsf{RTrmv}=\begin{cases}
\boldsymbol{A} \boldsymbol{x}, & \text{for } \textsf{TransA = mpBlasNoTrans},\\
\boldsymbol{A}^T \boldsymbol{x}, & \text{for } \textsf{TransA = mpBlasTrans}.
\end{cases}
\end{equation}

When \textsf{Uplo} is 0 then the upper triangle of $\boldsymbol{A}$ is used, and when \textsf{Uplo} is 1 then the lower triangle of $\boldsymbol{A}$ is used. If \textsf{Diag} is 0 then the diagonal of the matrix is used, but if \textsf{Diag} is 1 then the diagonal elements of the matrix $\boldsymbol{A}$ are taken as unity and are not referenced. 



\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionFive
	{cplxTrmv? mpNum?  the matrix-vector product for a triangular matrix.}
	{Uplo? Integer? An indicator specifying whether the upper or lower triangle will be used.}
	{TransA? Integer? An indicator specifying the multiplication.}
	{Diag? Integer? An indicator specifying the use of the diagonal.}
	{A? mpNum[,]? A matrix of complex numbers.}
	{x? mpNum[]? A vector of complex numbers.}
\end{mpFunctionsExtract}


\vspace{0.3cm}
For the complex vector $\boldsymbol{x}$ and the complex triangular matrix $\boldsymbol{A}$, the function \textsf{cplxTrmv} computes the matrix-vector product
\begin{equation}
\textsf{cplxTrmv}=\begin{cases}
\boldsymbol{A} \boldsymbol{x}, & \text{for } \textsf{TransA = mpBlasNoTrans},\\
\boldsymbol{A}^T \boldsymbol{x}, & \text{for } \textsf{TransA = mpBlasTrans},\\
\boldsymbol{A}^H \boldsymbol{x}, & \text{for } \textsf{TransA = mpBlasConjTrans}.
\end{cases}
\end{equation}

When \textsf{Uplo} is 0 then the upper triangle of $\boldsymbol{A}$ is used, and when \textsf{Uplo} is 1 then the lower triangle of $\boldsymbol{A}$ is used. If \textsf{Diag} is 0 then the diagonal of the matrix is used, but if \textsf{Diag} is 1 then the diagonal elements of the matrix $\boldsymbol{A}$ are taken as unity and are not referenced. 





\newpage
\subsection{Inverse Matrix-Vector Product (Triangular Matrix)}

\begin{mpFunctionsExtract}
	\mpFunctionFive
	{RTrsv? mpNum?  the inverse matrix-vector product for a triangular matrix.}
	{Uplo? Integer? An indicator specifying whether the upper or lower triangle will be used.}
	{TransA? Integer? An indicator specifying the multiplication.}
	{Diag? Integer? An indicator specifying the use of the diagonal.}
	{A? mpNum[,]? A matrix of real numbers.}
	{x? mpNum[]? A vector of real numbers.}
\end{mpFunctionsExtract}

\vspace{0.3cm}
For the real vector $\boldsymbol{x}$ and the real triangular matrix $\boldsymbol{A}$, the function \textsf{RTrsv} computes the matrix-vector product
\begin{equation}
\textsf{RTrsv}=\begin{cases}
\boldsymbol{A}^{-1} \boldsymbol{x}, & \text{for } \textsf{TransA = mpBlasNoTrans},\\
\left(\boldsymbol{A}^T\right)^{-1} \boldsymbol{x}, & \text{for } \textsf{TransA = mpBlasTrans}.
\end{cases}
\end{equation}

When \textsf{Uplo} is 0 then the upper triangle of $\boldsymbol{A}$ is used, and when \textsf{Uplo} is 1 then the lower triangle of $\boldsymbol{A}$ is used. If \textsf{Diag} is 0 then the diagonal of the matrix is used, but if \textsf{Diag} is 1 then the diagonal elements of the matrix $\boldsymbol{A}$ are taken as unity and are not referenced. 


\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionFive
	{cplxTrsv? mpNum?  the inverse matrix-vector product for a triangular matrix.}
	{Uplo? Integer? An indicator specifying whether the upper or lower triangle will be used.}
	{TransA? Integer? An indicator specifying the multiplication.}
	{Diag? Integer? An indicator specifying the use of the diagonal.}
	{A? mpNum[,]? A matrix of complex numbers.}
	{x? mpNum[]? A vector of complex numbers.}
\end{mpFunctionsExtract}
\vspace{0.3cm}
For the complex vector $\boldsymbol{x}$ and the complex triangular matrix $\boldsymbol{A}$, the function \textsf{cplxTrsv} computes the matrix-vector product
\begin{equation}
\textsf{cplxTrsv}=\begin{cases}
\boldsymbol{A}^{-1} \boldsymbol{x} & \text{for } \textsf{TransA = mpBlasNoTrans},\\
\left(\boldsymbol{A}^T\right)^{-1} \boldsymbol{x}, & \text{for } \textsf{TransA = mpBlasTrans},\\
\left(\boldsymbol{A}^H\right)^{-1} \boldsymbol{x}, & \text{for } \textsf{TransA = mpBlasConjTrans}.
\end{cases}
\end{equation}

When \textsf{Uplo} is 0 then the upper triangle of $\boldsymbol{A}$ is used, and when \textsf{Uplo} is 1 then the lower triangle of $\boldsymbol{A}$ is used. If \textsf{Diag} is 0 then the diagonal of the matrix is used, but if \textsf{Diag} is 1 then the diagonal elements of the matrix $\boldsymbol{A}$ are taken as unity and are not referenced. 



\newpage
\subsection{Matrix-Vector Product and Sum (Symmetric/Hermitian Matrix)}

\begin{mpFunctionsExtract}
	\mpFunctionSix
	{RSymv? mpNum? the matrix-vector product and sum for a symmetric matrix.}
	{Uplo? Integer? An indicator specifying whether the upper or lower triangle will be used.}
	{$\alpha$? mpNum? A real scalar.}
	{A? mpNum[,]? A matrix of real numbers.}
	{x? mpNum[]? A vector of real numbers.}
	{$\beta$? mpNum? A real scalar.}
	{y? mpNum[]? A vector of real numbers.}
\end{mpFunctionsExtract}

\vspace{0.3cm}
For the real scalars $\alpha$ and $\beta$, the real vectors $\boldsymbol{x}$ and $\boldsymbol{y}$, and the real symmetric matrix $\boldsymbol{A}$, the function \textsf{RSymv} computes the matrix-vector product and sum 
\begin{equation}
\textsf{RSymv}= \alpha \boldsymbol{A} \boldsymbol{x} + \beta \boldsymbol{y}.
\end{equation}

When \textsf{Uplo} is 0 then the upper triangle and diagonal of $\boldsymbol{A}$ are used, and when \textsf{Uplo} is 1 then the lower triangle and diagonal of $\boldsymbol{A}$ are used. 


\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionSix
	{cplxHemv? mpNum? the matrix-vector product and sum for a hermitian matrix.}
	{Uplo? Integer? An indicator specifying whether the upper or lower triangle will be used.}
	{$\alpha$? mpNum? A complex scalar.}
	{A? mpNum[,]? A matrix of complex numbers.}
	{x? mpNum[]? A vector of complex numbers.}
	{$\beta$? mpNum? A complex scalar.}
	{y? mpNum[]? A vector of complex numbers.}
\end{mpFunctionsExtract}


\vspace{0.3cm}
For the complex scalars $\alpha$ and $\beta$, the complex vectors $\boldsymbol{x}$ and $\boldsymbol{y}$, and the complex hermitian matrix $\boldsymbol{A}$, the function \textsf{cplxHemv} computes the matrix-vector product and sum 
\begin{equation}
\textsf{cplxHemv}= \alpha \boldsymbol{A} \boldsymbol{x} + \beta \boldsymbol{y}.
\end{equation}

When \textsf{Uplo} is 0 then the upper triangle and diagonal of $\boldsymbol{A}$ are used, and when \textsf{Uplo} is 1 then the lower triangle and diagonal of $\boldsymbol{A}$ are used. 

In \textsf{cplxHemv}, the imaginary elements of the diagonal are automatically assumed to be zero and are not referenced. 






\newpage
\subsection{Rank-1 update (General Matrix)}

\begin{mpFunctionsExtract}
	\mpFunctionFour
	{RGer? mpNum? the rank-1 update for a general matrix}
	{$\alpha$? mpNum? A real scalar.}
	{x? mpNum[]? A vector of real numbers.}
	{y? mpNum[]? A vector of real numbers.}
	{A? mpNum[,]? A matrix of real numbers.}
\end{mpFunctionsExtract}

\vspace{0.3cm}
For the real scalar $\alpha$, the real vectors $\boldsymbol{x}$ and $\boldsymbol{y}$, and the real general matrix $\boldsymbol{A}$, the function \textsf{RGer} computes the rank-1 update of the matrix $\boldsymbol{A}$, defined as
\begin{equation}
\textsf{RGer}= \alpha \boldsymbol{x}  \boldsymbol{y}^T +\boldsymbol{A} .
\end{equation}


\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionFour
	{cplxGeru? mpNum? the rank-1 update for a general matrix}
	{$\alpha$? mpNum? A complex scalar.}
	{x? mpNum[]? A vector of complex numbers.}
	{y? mpNum[]? A vector of complex numbers.}
	{A? mpNum[,]? A matrix of complex numbers.}
\end{mpFunctionsExtract}

\vspace{0.3cm}
For the complex scalar $\alpha$, the complex vectors $\boldsymbol{x}$ and $\boldsymbol{y}$, and the complex general matrix $\boldsymbol{A}$, the function \textsf{cplxGeru} computes the rank-1 update of the matrix $\boldsymbol{A}$, defined as
\begin{equation}
\textsf{cplxGeru}= \alpha \boldsymbol{x}  \boldsymbol{y}^T +\boldsymbol{A} .
\end{equation}


\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionFour
	{cplxGerc? mpNum? the rank-1 update for a general matrix}
	{$\alpha$? mpNum? A complex scalar.}
	{x? mpNum[]? A vector of complex numbers.}
	{y? mpNum[]? A vector of complex numbers.}
	{A? mpNum[,]? A matrix of complex numbers.}
\end{mpFunctionsExtract}

\vspace{0.3cm}
For the complex scalar $\alpha$, the complex vectors $\boldsymbol{x}$ and $\boldsymbol{y}$, and the complex general matrix $\boldsymbol{A}$, the function \textsf{cplxGerc} computes the rank-1 update of the matrix $\boldsymbol{A}$, defined as
\begin{equation}
\textsf{cplxGerc}= \alpha \boldsymbol{x}  \boldsymbol{y}^H +\boldsymbol{A} .
\end{equation}





\newpage
\subsection{Rank-1 update (Symmetric/Hermitian Matrix)}

\begin{mpFunctionsExtract}
	\mpFunctionFour
	{RSyr? mpNum? the Rank-1 update for a symmetric matrix.}
	{Uplo? Integer? An indicator specifying whether the upper or lower triangle will be used.}
	{$\alpha$? mpNum? A real scalar.}
	{x? mpNum[]? A vector of real numbers.}
	{A? mpNum[,]? A matrix of real numbers.}
\end{mpFunctionsExtract}

\vspace{0.3cm}
For the real scalar $\alpha$, the real vector $\boldsymbol{x}$, and the real symmetric matrix $\boldsymbol{A}$, the function \textsf{RSyr} computes the symmetric rank-1 update of the matrix $\boldsymbol{A}$, defined as
\begin{equation}
\textsf{RSyr}= \alpha \boldsymbol{x}  \boldsymbol{x}^T +\boldsymbol{A} .
\end{equation}

Since the matrix $\boldsymbol{A}$ is symmetric, only its upper half or lower half need to be stored. When \textsf{Uplo} is 0 then the upper triangle and diagonal of $\boldsymbol{A}$ are used, and when \textsf{Uplo} is 1 then the lower triangle and diagonal of $\boldsymbol{A}$ are used. 



\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionFour
	{cplxHer? mpNum? the Rank-1 update for a hermitian matrix.}
	{Uplo? Integer? An indicator specifying whether the upper or lower triangle will be used.}
	{$\alpha$? mpNum? A complex scalar.}
	{x? mpNum[]? A vector of complex numbers.}
	{A? mpNum[,]? A matrix of complex numbers.}
\end{mpFunctionsExtract}

\vspace{0.3cm}
For the complex scalar $\alpha$, the complex vector $\boldsymbol{x}$, and the complex hermitian matrix $\boldsymbol{A}$, the function \textsf{cplxHer} computes the hermitian rank-1 update of the matrix $\boldsymbol{A}$, defined as
\begin{equation}
\textsf{cplxHer}= \alpha \boldsymbol{x}  \boldsymbol{x}^H +\boldsymbol{A} .
\end{equation}

Since the matrix $\boldsymbol{A}$ is hermitian, only its upper half or lower half need to be stored. When \textsf{Uplo} is 0 then the upper triangle and diagonal of $\boldsymbol{A}$ are used, and when \textsf{Uplo} is 1 then the lower triangle and diagonal of $\boldsymbol{A}$ are used. 
The imaginary elements of the diagonal are automatically set to zero. 








\newpage
\subsection{Rank-2 update (Symmetric/Hermitian Matrix)}


\begin{mpFunctionsExtract}
	\mpFunctionFive
	{RSyr2? mpNum? the Rank-1 update for a symmetric matrix.}
	{Uplo? Integer? An indicator specifying whether the upper or lower triangle will be used.}
	{$\alpha$? mpNum? A real scalar.}
	{x? mpNum[]? A vector of real numbers.}
	{y? mpNum[]? A vector of real numbers.}
	{A? mpNum[,]? A matrix of real numbers.}
\end{mpFunctionsExtract}

\vspace{0.3cm}
For the real scalar $\alpha$, the real vectors $\boldsymbol{x}$ and $\boldsymbol{y}$, and the real symmetric matrix $\boldsymbol{A}$, the function \textsf{RSyr2} computes the symmetric rank-1 update of the matrix $\boldsymbol{A}$, defined as
\begin{equation}
\textsf{RSyr2}= \alpha \boldsymbol{x} \boldsymbol{y}^T +  \alpha \boldsymbol{y} \boldsymbol{x}^T +\boldsymbol{A}.
\end{equation}

Since the matrix $\boldsymbol{A}$ is symmetric, only its upper half or lower half need to be stored. When \textsf{Uplo} is 0 then the upper triangle and diagonal of $\boldsymbol{A}$ are used, and when \textsf{Uplo} is 1 then the lower triangle and diagonal of $\boldsymbol{A}$ are used. 


\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionFive
	{cplxHer2? mpNum? the Rank-1 update for a hermitian matrix.}
	{Uplo? Integer? An indicator specifying whether the upper or lower triangle will be used.}
	{$\alpha$? mpNum? A complex scalar.}
	{x? mpNum[]? A vector of complex numbers.}
	{y? mpNum[]? A vector of complex numbers.}
	{A? mpNum[,]? A matrix of complex numbers.}
\end{mpFunctionsExtract}

\vspace{0.3cm}
For the complex scalar $\alpha$, the complex vectors $\boldsymbol{x}$ and $\boldsymbol{y}$, and the complex hermitian matrix $\boldsymbol{A}$, the function \textsf{cplxHer2} computes the hermitian rank-1 update of the matrix $\boldsymbol{A}$, defined as
\begin{equation}
\textsf{cplxHer2}=  \alpha \boldsymbol{x} \boldsymbol{y}^H +  \alpha^* \boldsymbol{y} \boldsymbol{x}^H +\boldsymbol{A}.
\end{equation}

Since the matrix $\boldsymbol{A}$ is hermitian, only its upper half or lower half need to be stored. When \textsf{Uplo} is 0 then the upper triangle and diagonal of $\boldsymbol{A}$ are used, and when \textsf{Uplo} is 1 then the lower triangle and diagonal of $\boldsymbol{A}$ are used. 
The imaginary elements of the diagonal are automatically set to zero. 






\section{BLAS Level 3 Support}
\label{BLASSupportLevel3}


\subsection{Matrix-Matrix-Product and Sum (General Matrix A)}


\begin{mpFunctionsExtract}
	\mpFunctionSeven
	{RGemm? mpNum? the matrix-matrix product and sum for a general matrix.}
	{TransA? Integer? An indicator specifying the multiplication.}
	{TransB? Integer? An indicator specifying the multiplication.}
	{$\alpha$? mpNum? A real scalar.}
	{A? mpNum[,]? A matrix of real numbers.}
	{B? mpNum[,]? A matrix of real numbers.}
	{$\beta$? mpNum? A real scalar.}
	{C? mpNum[,]? A matrix of real numbers.}
\end{mpFunctionsExtract}

\vspace{0.3cm}
For the real scalars $\alpha$ and $\beta$, and the real matrices $\boldsymbol{A}$, $\boldsymbol{B}$, $\boldsymbol{C}$, the function \textsf{RGemm} computes the matrix-matrix product and sum 
\begin{equation}
\textsf{RGemm}=\begin{cases}
\alpha \boldsymbol{A} \boldsymbol{B} + \beta \boldsymbol{C}, & \text{for } \textsf{TransA = mpBlasNoTrans, TransB = mpBlasNoTrans},\\
\alpha \boldsymbol{A} \boldsymbol{B}^T + \beta \boldsymbol{C}, & \text{for } \textsf{TransA = mpBlasNoTrans, TransB = mpBlasTrans},\\		
\alpha \boldsymbol{A}^T \boldsymbol{B} + \beta \boldsymbol{C}, & \text{for } \textsf{TransA = mpBlasTrans, TransB = mpBlasNoTrans},\\
\alpha \boldsymbol{A}^T \boldsymbol{B}^T + \beta \boldsymbol{C}, & \text{for } \textsf{TransA = mpBlasTrans, TransB = mpBlasTrans},\\
\end{cases}
\end{equation}


\newpage
\begin{mpFunctionsExtract}
	\mpFunctionSeven
	{cplxGemm? mpNum? the matrix-matrix product and sum for a general matrix.}
	{TransA? Integer? An indicator specifying the multiplication.}
	{TransB? Integer? An indicator specifying the multiplication.}
	{$\alpha$? mpNum? A real scalar.}
	{A? mpNum[,]? A matrix of real numbers.}
	{B? mpNum[,]? A matrix of real numbers.}
	{$\beta$? mpNum? A real scalar.}
	{C? mpNum[,]? A matrix of real numbers.}
\end{mpFunctionsExtract}

\vspace{0.3cm}
For the complex scalars $\alpha$ and $\beta$, and the complex matrices $\boldsymbol{A}$, $\boldsymbol{B}$, $\boldsymbol{C}$, the function \textsf{cplxGemm} computes the matrix-matrix product and sum 
\begin{equation}
\textsf{cplxGemm}=\begin{cases}
\alpha \boldsymbol{A} \boldsymbol{B} + \beta \boldsymbol{C}, & \text{for } \textsf{TransA = mpBlasNoTrans, TransB = mpBlasNoTrans},\\
\alpha \boldsymbol{A} \boldsymbol{B}^T + \beta \boldsymbol{C}, & \text{for } \textsf{TransA = mpBlasNoTrans, TransB = mpBlasTrans},\\		
\alpha \boldsymbol{A} \boldsymbol{B}^H + \beta \boldsymbol{C}, & \text{for } \textsf{TransA = mpBlasNoTrans, TransB = mpBlasConjTrans},\\
\alpha \boldsymbol{A}^T \boldsymbol{B} + \beta \boldsymbol{C}, & \text{for } \textsf{TransA = mpBlasTrans, TransB = mpBlasNoTrans},\\
\alpha \boldsymbol{A}^T \boldsymbol{B}^T + \beta \boldsymbol{C}, & \text{for } \textsf{TransA = mpBlasTrans, TransB = mpBlasTrans},\\
\alpha \boldsymbol{A}^T \boldsymbol{B}^H + \beta \boldsymbol{C}, & \text{for } \textsf{TransA = mpBlasTrans, TransB = mpBlasConjTrans},\\
\alpha \boldsymbol{A}^H \boldsymbol{B} + \beta \boldsymbol{C}, & \text{for } \textsf{TransA = mpBlasConjTrans, TransB = mpBlasNoTrans},\\
\alpha \boldsymbol{A}^H \boldsymbol{B}^T + \beta \boldsymbol{C}, & \text{for } \textsf{TransA = mpBlasConjTrans, TransB = mpBlasTrans},\\
\alpha \boldsymbol{A}^H \boldsymbol{B}^H + \beta \boldsymbol{C}, & \text{for } \textsf{TransA = mpBlasConjTrans, TransB = mpBlasConjTrans},\\
\end{cases}
\end{equation}




%\begin{tabular}{p{481pt}}
%\toprule
%\textsf{Function \textbf{RGemm}(\textbf{TransA} As Integer, \textbf{TransB} As Integer, $\boldsymbol{\alpha}$ As mpNum, $\boldsymbol{A}$ As mpNum[,], $\boldsymbol{B}$ As mpNum[,], $\boldsymbol{\beta}$ As mpNum, $\boldsymbol{C}$ As mpNum[,]) As mpNum[,]}\index{Multiprecision Functions!RGemm} \\
%\midrule
%\textsf{Function \textbf{cplxGemm}(\textbf{TransA} As Integer, \textbf{TransB} As Integer, $\boldsymbol{\alpha}$ As mpNum, $\boldsymbol{A}$ As mpNum[,], $\boldsymbol{B}$ As mpNum[,], $\boldsymbol{\beta}$ As mpNum, $\boldsymbol{C}$ As mpNum[,]) As mpNum[,]}\index{Multiprecision Functions!cplxGemm} \\
%\bottomrule
%\end{tabular}
%





\newpage
\subsection{Matrix-Matrix-Product and Sum (Symmetric/Hermitian Matrix A)}


\begin{mpFunctionsExtract}
	\mpFunctionSeven
	{RSymm? mpNum? the matrix-matrix product and sum for a symmetric matrix.}
	{Side? Integer? An indicator specifying the order of the multiplication.}
	{Uplo? Integer? An indicator specifying whether the upper or lower triangle will be used.}
	{$\alpha$? mpNum? A real scalar.}
	{A? mpNum[,]? A matrix of real numbers.}
	{B? mpNum[,]? A matrix of real numbers.}
	{$\beta$? mpNum? A real scalar.}
	{C? mpNum[,]? A matrix of real numbers.}
\end{mpFunctionsExtract}

\vspace{0.3cm}
For the real scalars $\alpha$ and $\beta$, the real symmetric matrix $\boldsymbol{A}$, and the real general matrices $\boldsymbol{B}$ and $\boldsymbol{C}$, the function \textsf{RSymm} computes the matrix-matrix product and sum 
\begin{equation}
\textsf{RSymm}=\begin{cases}
\alpha \boldsymbol{A} \boldsymbol{B} + \beta \boldsymbol{C}, & \text{for } \textsf{Side = mpBlasLeft} \\
\alpha \boldsymbol{B} \boldsymbol{A} + \beta \boldsymbol{C}, & \text{for } \textsf{Side = mpBlasRight} \\		
\end{cases}
\end{equation}

When \textsf{Uplo} is 0 then the upper triangle and diagonal of $\boldsymbol{A}$ are used, and when \textsf{Uplo} is 1 then the lower triangle and diagonal of $\boldsymbol{A}$ are used. 

\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionSeven
	{cplxSymm? mpNum? the matrix-matrix product and sum for a symmetric matrix.}
	{Side? Integer? An indicator specifying the order of the multiplication.}
	{Uplo? Integer? An indicator specifying whether the upper or lower triangle will be used.}
	{$\alpha$? mpNum? A complex scalar.}
	{A? mpNum[,]? A matrix of complex numbers.}
	{B? mpNum[,]? A matrix of complex numbers.}
	{$\beta$? mpNum? A complex scalar.}
	{C? mpNum[,]? A matrix of complex numbers.}
\end{mpFunctionsExtract}

\vspace{0.3cm}
For the complex scalars $\alpha$ and $\beta$, the complex symmetric matrix $\boldsymbol{A}$, and the complex general matrices $\boldsymbol{B}$ and $\boldsymbol{C}$, the function \textsf{cplxSymm} computes the matrix-matrix product and sum 
\begin{equation}
\textsf{cplxSymm}=\begin{cases}
\alpha \boldsymbol{A} \boldsymbol{B} + \beta \boldsymbol{C}, & \text{for } \textsf{Side = mpBlasLeft} \\
\alpha \boldsymbol{B} \boldsymbol{A} + \beta \boldsymbol{C}, & \text{for } \textsf{Side = mpBlasRight} \\		
\end{cases}
\end{equation}

When \textsf{Uplo} is 0 then the upper triangle and diagonal of $\boldsymbol{A}$ are used, and when \textsf{Uplo} is 1 then the lower triangle and diagonal of $\boldsymbol{A}$ are used. 


\newpage
\begin{mpFunctionsExtract}
	\mpFunctionSeven
	{cplxHemm? mpNum? the matrix-matrix product and sum for a hermitian matrix.}
	{Side? Integer? An indicator specifying the order of the multiplication.}
	{Uplo? Integer? An indicator specifying whether the upper or lower triangle will be used.}
	{$\alpha$? mpNum? A complex scalar.}
	{A? mpNum[,]? A matrix of complex numbers.}
	{B? mpNum[,]? A matrix of complex numbers.}
	{$\beta$? mpNum? A complex scalar.}
	{C? mpNum[,]? A matrix of complex numbers.}
\end{mpFunctionsExtract}

\vspace{0.3cm}
For the complex scalars $\alpha$ and $\beta$, the complex hermitian matrix $\boldsymbol{A}$, and the complex general matrices $\boldsymbol{B}$ and $\boldsymbol{C}$, the function \textsf{cplxHemm} computes the matrix-matrix product and sum 
\begin{equation}
\textsf{cplxHemm}=\begin{cases}
\alpha \boldsymbol{A} \boldsymbol{B} + \beta \boldsymbol{C}, & \text{for } \textsf{Side = mpBlasLeft} \\
\alpha \boldsymbol{B} \boldsymbol{A} + \beta \boldsymbol{C}, & \text{for } \textsf{Side = mpBlasRight} \\		
\end{cases}
\end{equation}

When \textsf{Uplo} is 0 then the upper triangle and diagonal of $\boldsymbol{A}$ are used, and when \textsf{Uplo} is 1 then the lower triangle and diagonal of $\boldsymbol{A}$ are used. The imaginary elements of the diagonal are automatically assumed to be zero and are not referenced. 






\newpage
\subsection{Matrix-Matrix-Product (Triangular Matrix A)}

\begin{mpFunctionsExtract}
	\mpFunctionSix
	{RTrmm? mpNum? the matrix-matrix produc for a triangular matrix.}
	{Side? Integer? An indicator specifying the order of the multiplication.}
	{Uplo? Integer? An indicator specifying whether the upper or lower triangle will be used.}
	{TransA? Integer? An indicator specifying the multiplication.}
	{$\alpha$? mpNum? A real scalar.}
	{A? mpNum[,]? A matrix of real numbers.}
	{B? mpNum[,]? A matrix of real numbers.}
\end{mpFunctionsExtract}

\vspace{0.3cm}
For the real scalar $\alpha$, the real triangular matrix $\boldsymbol{A}$, and  the real general matrix $\boldsymbol{B}$, the function \textsf{RTrmm} computes the matrix-matrix product
\begin{equation}
\textsf{RTrmm}=\begin{cases}
\alpha \boldsymbol{A} \boldsymbol{B}, & \text{for } \textsf{Side = mpBlasLeft, TransA = mpBlasNoTrans},\\
\alpha \boldsymbol{B} \boldsymbol{A}, & \text{for } \textsf{Side = mpBlasRight, TransA = mpBlasNoTrans},\\
\alpha \boldsymbol{A}^T \boldsymbol{B}, & \text{for } \textsf{Side = mpBlasLeft, TransA = mpBlasTrans},\\
\alpha \boldsymbol{B} \boldsymbol{A}^T, & \text{for } \textsf{Side = mpBlasRight, TransA = mpBlasTrans},\\
\end{cases}
\end{equation}

When \textsf{Uplo} is 0 then the upper triangle of $\boldsymbol{A}$ is used, and when \textsf{Uplo} is 1 then the lower triangle of $\boldsymbol{A}$ is used. If \textsf{Diag} is 0 then the diagonal of $\boldsymbol{A}$ is used, but if \textsf{Diag} is 1 then the diagonal elements of the matrix $\boldsymbol{A}$ are taken as unity and are not referenced. 


\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionSix
	{cplxTrmm? mpNum? the matrix-matrix product for a triangular matrix.}
	{Side? Integer? An indicator specifying the order of the multiplication.}
	{Uplo? Integer? An indicator specifying whether the upper or lower triangle will be used.}
	{TransA? Integer? An indicator specifying the multiplication.}
	{$\alpha$? mpNum? A complex scalar.}
	{A? mpNum[,]? A matrix of complex numbers.}
	{B? mpNum[,]? A matrix of complex numbers.}
\end{mpFunctionsExtract}

\vspace{0.3cm}
For the complex scalar $\alpha$, the complex triangular matrix $\boldsymbol{A}$, and  the complex general matrix $\boldsymbol{B}$, the function \textsf{cplxTrmm} computes the matrix-matrix product
\begin{equation}
\textsf{cplxTrmm}=\begin{cases}
\alpha \boldsymbol{A} \boldsymbol{B}, & \text{for } \textsf{Side = mpBlasLeft, TransA = mpBlasNoTrans},\\
\alpha \boldsymbol{B} \boldsymbol{A}, & \text{for } \textsf{Side = mpBlasRight, TransA = mpBlasNoTrans},\\
\alpha \boldsymbol{A}^T \boldsymbol{B}, & \text{for } \textsf{Side = mpBlasLeft, TransA = mpBlasTrans},\\
\alpha \boldsymbol{B} \boldsymbol{A}^T, & \text{for } \textsf{Side = mpBlasRight, TransA = mpBlasTrans},\\
\alpha \boldsymbol{A}^H \boldsymbol{B}, & \text{for } \textsf{Side = mpBlasLeft, TransA = mpBlasConjTrans},\\
\alpha \boldsymbol{B} \boldsymbol{A}^H, & \text{for } \textsf{Side = mpBlasRight, TransA = mpBlasConjTrans},\\
\end{cases}
\end{equation}

When \textsf{Uplo} is 0 then the upper triangle of $\boldsymbol{A}$ is used, and when \textsf{Uplo} is 1 then the lower triangle of $\boldsymbol{A}$ is used. If \textsf{Diag} is 0 then the diagonal of $\boldsymbol{A}$ is used, but if \textsf{Diag} is 1 then the diagonal elements of the matrix $\boldsymbol{A}$ are taken as unity and are not referenced. 






\newpage
\subsection{Inverse Matrix-Matrix-Product (Triangular Matrix A)}

\begin{mpFunctionsExtract}
	\mpFunctionSix
	{RTrsm? mpNum? the inverse matrix-matrix product for a triangular matrix.}
	{Side? Integer? An indicator specifying the order of the multiplication.}
	{Uplo? Integer? An indicator specifying whether the upper or lower triangle will be used.}
	{TransA? Integer? An indicator specifying the multiplication.}
	{$\alpha$? mpNum? A real scalar.}
	{A? mpNum[,]? A matrix of real numbers.}
	{B? mpNum[,]? A matrix of real numbers.}
\end{mpFunctionsExtract}

\vspace{0.3cm}
For the real scalar $\alpha$, the real triangular matrix $\boldsymbol{A}$, and  the real general matrix $\boldsymbol{B}$, the function \textsf{RTrsm} computes the matrix-matrix product
\begin{equation}
\textsf{RTrsm}=\begin{cases}
\alpha \boldsymbol{A}^{-1} \boldsymbol{B}, & \text{for } \textsf{Side = mpBlasLeft, TransA = mpBlasNoTrans},\\
\alpha \boldsymbol{B} \boldsymbol{A}^{-1}, & \text{for } \textsf{Side = mpBlasRight, TransA = mpBlasNoTrans},\\
\alpha \left(\boldsymbol{A}^T \right)^{-1} \boldsymbol{B}, & \text{for } \textsf{Side = mpBlasLeft, TransA = mpBlasTrans},\\
\alpha \boldsymbol{B} \left(\boldsymbol{A}^T \right)^{-1}, & \text{for } \textsf{Side = mpBlasRight, TransA = mpBlasTrans},\\
\end{cases}
\end{equation}

When \textsf{Uplo} is 0 then the upper triangle of $\boldsymbol{A}$ is used, and when \textsf{Uplo} is 1 then the lower triangle of $\boldsymbol{A}$ is used. If \textsf{Diag} is 0 then the diagonal of $\boldsymbol{A}$ is used, but if \textsf{Diag} is 1 then the diagonal elements of the matrix $\boldsymbol{A}$ are taken as unity and are not referenced. 



\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionSix
	{cplxTrsm? mpNum? the inverse matrix-matrix product for a triangular matrix.}
	{Side? Integer? An indicator specifying the order of the multiplication.}
	{Uplo? Integer? An indicator specifying whether the upper or lower triangle will be used.}
	{TransA? Integer? An indicator specifying the multiplication.}
	{$\alpha$? mpNum? A complex scalar.}
	{A? mpNum[,]? A matrix of complex numbers.}
	{B? mpNum[,]? A matrix of complex numbers.}
\end{mpFunctionsExtract}

\vspace{0.3cm}
For the complex scalar $\alpha$, the complex triangular matrix $\boldsymbol{A}$, and  the complex general matrix $\boldsymbol{B}$, the function \textsf{cplxTrsm} computes the matrix-matrix product
\begin{equation}
\textsf{cplxTrsm}=\begin{cases}
\alpha \boldsymbol{A}^{-1} \boldsymbol{B}, & \text{for } \textsf{Side = mpBlasLeft, TransA = mpBlasNoTrans},\\
\alpha \boldsymbol{B} \boldsymbol{A}^{-1}, & \text{for } \textsf{Side = mpBlasRight, TransA = mpBlasNoTrans},\\
\alpha \left(\boldsymbol{A}^T \right)^{-1} \boldsymbol{B}, & \text{for } \textsf{Side = mpBlasLeft, TransA = mpBlasTrans},\\
\alpha \boldsymbol{B} \left(\boldsymbol{A}^T \right)^{-1}, & \text{for } \textsf{Side = mpBlasRight, TransA = mpBlasTrans},\\
\alpha \left(\boldsymbol{A}^H \right)^{-1} \boldsymbol{B}, & \text{for } \textsf{Side = mpBlasLeft, TransA = mpBlasConjTrans},\\
\alpha \boldsymbol{B} \left(\boldsymbol{A}^H \right)^{-1}, & \text{for } \textsf{Side = mpBlasRight, TransA = mpBlasConjTrans},\\
\end{cases}
\end{equation}

When \textsf{Uplo} is 0 then the upper triangle of $\boldsymbol{A}$ is used, and when \textsf{Uplo} is 1 then the lower triangle of $\boldsymbol{A}$ is used. If \textsf{Diag} is 0 then the diagonal of $\boldsymbol{A}$ is used, but if \textsf{Diag} is 1 then the diagonal elements of the matrix $\boldsymbol{A}$ are taken as unity and are not referenced. 






\newpage
\subsection{Rank-k update (Symmetric/Hermitian Matrix C))}

\begin{mpFunctionsExtract}
	\mpFunctionSix
	{Rsyrk? mpNum? a rank-k update for a symmetric matrix.}
	{Uplo? Integer? An indicator specifying whether the upper or lower triangle will be used.}
	{Trans? Integer? An indicator specifying the multiplication.}
	{$\alpha$? mpNum? A real scalar.}
	{A? mpNum[,]? A matrix of real numbers.}
	{$\beta$? mpNum? A real scalar.}
	{C? mpNum[,]? A matrix of real numbers.}
\end{mpFunctionsExtract}

\vspace{0.3cm}
For the real scalars $\alpha$ and $\beta$, the real symmetric matrix $\boldsymbol{C}$, and the real general matrix $\boldsymbol{A}$, the function \textsf{Rsyrk} computes a rank-k update of the symmetric matrix $\boldsymbol{C}$, defined as 
\begin{equation}
\textsf{Rsyrk}=\begin{cases}
\alpha \boldsymbol{A} \boldsymbol{A}^T + \beta \boldsymbol{C}, & \text{for } \textsf{Trans = mpNoTrans} \\
\alpha \boldsymbol{A}^T \boldsymbol{A} + \beta \boldsymbol{C}, & \text{for } \textsf{Trans = mpTrans} \\
\end{cases}
\end{equation}

Since the matrix $\boldsymbol{C}$ is symmetric, only its upper half or lower half need to be stored. When \textsf{Uplo} is 0 then the upper triangle and diagonal of $\boldsymbol{C}$ are used, and when \textsf{Uplo} is 1 then the lower triangle and diagonal of $\boldsymbol{C}$ are used. 


\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionSix
	{cplxSyrk? mpNum? a rank-k update for a symmetric matrix.}
	{Uplo? Integer? An indicator specifying whether the upper or lower triangle will be used.}
	{Trans? Integer? An indicator specifying the multiplication.}
	{$\alpha$? mpNum? A complex scalar.}
	{A? mpNum[,]? A matrix of complex numbers.}
	{$\beta$? mpNum? A complex scalar.}
	{C? mpNum[,]? A matrix of complex numbers.}
\end{mpFunctionsExtract}

\vspace{0.3cm}
For the complex scalars $\alpha$ and $\beta$, the complex symmetric matrix $\boldsymbol{C}$, and the complex general matrix $\boldsymbol{A}$, the function \textsf{cplxSyrk} computes a rank-k update of the complex matrix $\boldsymbol{C}$, defined as 
\begin{equation}
\textsf{cplxSyrk}=\begin{cases}
\alpha \boldsymbol{A} \boldsymbol{A}^T + \beta \boldsymbol{C}, & \text{for } \textsf{Trans = mpNoTrans} \\
\alpha \boldsymbol{A}^T \boldsymbol{A} + \beta \boldsymbol{C}, & \text{for } \textsf{Trans = mpTrans} \\
\end{cases}
\end{equation}

Since the matrix $\boldsymbol{C}$ is symmetric, only its upper half or lower half need to be stored. When \textsf{Uplo} is 0 then the upper triangle and diagonal of $\boldsymbol{C}$ are used, and when \textsf{Uplo} is 1 then the lower triangle and diagonal of $\boldsymbol{C}$ are used. 


\newpage
\begin{mpFunctionsExtract}
	\mpFunctionSix
	{cplxHerk? mpNum? a rank-k update for a hermitian matrix.}
	{Uplo? Integer? An indicator specifying whether the upper or lower triangle will be used.}
	{Trans? Integer? An indicator specifying the multiplication.}
	{$\alpha$? mpNum? A complex scalar.}
	{A? mpNum[,]? A matrix of complex numbers.}
	{$\beta$? mpNum? A complex scalar.}
	{C? mpNum[,]? A matrix of complex numbers.}
\end{mpFunctionsExtract}

\vspace{0.3cm}
For the complex scalars $\alpha$ and $\beta$, the complex hermitian matrix $\boldsymbol{C}$, and the complex general matrix $\boldsymbol{A}$, the function \textsf{cplxHerk} computes a rank-k update of the hermitian matrix $\boldsymbol{C}$, defined as 
\begin{equation}
\textsf{cplxHerk}=\begin{cases}
\alpha \boldsymbol{A} \boldsymbol{A}^H + \beta \boldsymbol{C}, & \text{for } \textsf{Trans = mpNoTrans} \\
\alpha \boldsymbol{A}^H \boldsymbol{A} + \beta \boldsymbol{C}, & \text{for } \textsf{Trans = mpTrans} \\
\end{cases}
\end{equation}

Since the matrix $\boldsymbol{C}$ is hermitian, only its upper half or lower half need to be stored. When \textsf{Uplo} is 0 then the upper triangle and diagonal of $\boldsymbol{C}$ are used, and when \textsf{Uplo} is 1 then the lower triangle and diagonal of $\boldsymbol{C}$ are used. 





\newpage
\subsection{Rank-2k update (Symmetric/Hermitian Matrix C)}


\begin{mpFunctionsExtract}
	\mpFunctionSeven
	{Rsyr2k? mpNum? a rank-k update for a symmetric matrix.}
	{Uplo? Integer? An indicator specifying whether the upper or lower triangle will be used.}
	{Trans? Integer? An indicator specifying the multiplication.}
	{$\alpha$? mpNum? A real scalar.}
	{A? mpNum[,]? A matrix of real numbers.}
	{B? mpNum[,]? A matrix of real numbers.}
	{$\beta$? mpNum? A real scalar.}
	{C? mpNum[,]? A matrix of real numbers.}
\end{mpFunctionsExtract}

\vspace{0.3cm}
For the real scalars $\alpha$ and $\beta$, the real symmetric matrix $\boldsymbol{C}$, and the real general matrices $\boldsymbol{A}$ and $\boldsymbol{B}$, the function \textsf{Rsyr2k} computes a rank-k update of the symmetric matrix $\boldsymbol{C}$, defined as 
\begin{equation}
\textsf{Rsyr2k}=\begin{cases}
\alpha \boldsymbol{A} \boldsymbol{B}^T+ \alpha \boldsymbol{B} \boldsymbol{A}^T + \beta \boldsymbol{C}, & \text{for } \textsf{Trans = mpNoTrans} \\
\alpha \boldsymbol{A}^T \boldsymbol{B} + \alpha \boldsymbol{B}^T \boldsymbol{A} + \beta \boldsymbol{C}, & \text{for } \textsf{Trans = mpTrans} \\
\end{cases}
\end{equation}

Since the matrix $\boldsymbol{C}$ is symmetric/hermitian, only its upper half or lower half need to be stored. When \textsf{Uplo} is 0 then the upper triangle and diagonal of $\boldsymbol{C}$ are used, and when \textsf{Uplo} is 1 then the lower triangle and diagonal of $\boldsymbol{C}$ are used. 


\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionSeven
	{cplxSyr2k? mpNum? a rank-k update for a symmetric matrix.}
	{Uplo? Integer? An indicator specifying whether the upper or lower triangle will be used.}
	{Trans? Integer? An indicator specifying the multiplication.}
	{$\alpha$? mpNum? A complex scalar.}
	{A? mpNum[,]? A matrix of complex numbers.}
	{B? mpNum[,]? A matrix of complex numbers.}
	{$\beta$? mpNum? A complex scalar.}
	{C? mpNum[,]? A matrix of complex numbers.}
\end{mpFunctionsExtract}

\vspace{0.3cm}
For the complex scalars $\alpha$ and $\beta$, the complex symmetric matrix $\boldsymbol{C}$, and the complex general matrices $\boldsymbol{A}$ and $\boldsymbol{B}$, the function \textsf{cplxSyrk} computes a rank-k update of the complex matrix $\boldsymbol{C}$, defined as 
\begin{equation}
\textsf{cplxSyrk}=\begin{cases}
\alpha \boldsymbol{A} \boldsymbol{B}^T+ \alpha \boldsymbol{B} \boldsymbol{A}^T + \beta \boldsymbol{C}, & \text{for } \textsf{Trans = mpNoTrans} \\
\alpha \boldsymbol{A}^T \boldsymbol{B} + \alpha \boldsymbol{B}^T \boldsymbol{A} + \beta \boldsymbol{C}, & \text{for } \textsf{Trans = mpTrans} \\
\end{cases}
\end{equation}

Since the matrix $\boldsymbol{C}$ is symmetric/hermitian, only its upper half or lower half need to be stored. When \textsf{Uplo} is 0 then the upper triangle and diagonal of $\boldsymbol{C}$ are used, and when \textsf{Uplo} is 1 then the lower triangle and diagonal of $\boldsymbol{C}$ are used. 


\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionSeven
	{cplxHer2k? mpNum? a rank-k update for a hermitian matrix.}
	{Uplo? Integer? An indicator specifying whether the upper or lower triangle will be used.}
	{Trans? Integer? An indicator specifying the multiplication.}
	{$\alpha$? mpNum? A complex scalar.}
	{A? mpNum[,]? A matrix of complex numbers.}
	{B? mpNum[,]? A matrix of complex numbers.}
	{$\beta$? mpNum? A complex scalar.}
	{C? mpNum[,]? A matrix of complex numbers.}
\end{mpFunctionsExtract}

\vspace{0.3cm}
For the complex scalars $\alpha$ and $\beta$, the complex hermitian matrix $\boldsymbol{C}$, and the complex general matrices $\boldsymbol{A}$ and $\boldsymbol{B}$, the function \textsf{cplxHer2k} computes a rank-k update of the hermitian matrix $\boldsymbol{C}$, defined as 
\begin{equation}
\textsf{cplxHer2k}=\begin{cases}
\alpha \boldsymbol{A} \boldsymbol{B}^H+ \alpha \boldsymbol{B} \boldsymbol{A}^H + \beta \boldsymbol{C}, & \text{for } \textsf{Trans = mpNoTrans} \\
\alpha \boldsymbol{A}^H \boldsymbol{B} + \alpha \boldsymbol{B}^H \boldsymbol{A} + \beta \boldsymbol{C}, & \text{for } \textsf{Trans = mpTrans} \\
\end{cases}
\end{equation}

Since the matrix $\boldsymbol{C}$ is symmetric/hermitian, only its upper half or lower half need to be stored. When \textsf{Uplo} is 0 then the upper triangle and diagonal of $\boldsymbol{C}$ are used, and when \textsf{Uplo} is 1 then the lower triangle and diagonal of $\boldsymbol{C}$ are used. 







\chapter{Linear Solvers (based on Eigen)}
\label{LinearAlgebraChapter} % So I can \ref{altrings3} later.
%\lipsum[2-3]

Book reference: \cite{Golub1996}





\section{Cholesky Decomposition without Pivoting}
\label{Cholesky Decomposition without Pivoting}

\subsection{Decomposition}

\begin{mpFunctionsExtract}
	\mpFunctionFour
	{DecompCholeskyLLT? mpNumList? the Cholesky decomposition $A = LL^* = U^*U$ of a matrix.}
	{A? mpNum[,]? the real matrix of which we are computing the $LL^T$ Cholesky decomposition.}
	{B? mpNum[,]? A vector or matrix of real numbers.}
	{UpLo? Integer? the triangular part that will be used for the decompositon: Lower (default) or Upper. The other triangular part won't be read.}
	{Output? String? A string specifying the output options.}
\end{mpFunctionsExtract}

\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionFour
	{cplxDecompCholeskyLLT? mpNumList? the Cholesky decomposition $A = LL^* = U^*U$ of a matrix.}
	{A? mpNum[,]? the complex matrix of which we are computing the $LL^T$ Cholesky decomposition.}
	{B? mpNum[,]? A vector or complex of real numbers.}
	{UpLo? Integer? the triangular part that will be used for the decompositon: Lower (default) or Upper. The other triangular part won't be read.}
	{Output? String? A string specifying the output options.}
\end{mpFunctionsExtract}


\vspace{0.3cm}
These functions perform a $LL^T$ Cholesky decomposition of a symmetric, positive definite matrix $A$ such that $A = LL^* = U^*U$, where $L$ is lower triangular.
While the Cholesky decomposition is particularly useful to solve selfadjoint problems like $D^*D x = b$, for that purpose, we recommend the Cholesky decomposition without square root which is more stable and even faster. Nevertheless, this standard Cholesky decomposition remains useful in many other situations like generalised eigen problems with hermitian matrices.

Remember that Cholesky decompositions are not rank-revealing. This LLT decomposition is only stable on positive definite matrices, use LDLT instead for the semidefinite case. Also, do not use a Cholesky decomposition to determine whether a system of equations has a solution.




\vspace{0.3cm}
LLT< MatrixType, \_UpLo > \& compute  ( const MatrixType \&  a)   

Computes / recomputes the Cholesky decomposition $A = LL^* = U^*U$ of matrix 

Returns: a reference to *this


\vspace{0.3cm}
\textbf{ComputationInfo} info  ( )  const: Reports whether previous computation was successful. 

Returns: Success if computation was succesful, NumericalIssue if the matrix.appears to be negative. 

\vspace{0.3cm}
Traits::MatrixL \textbf{matrixL}  ( )  const 

Returns: a view of the lower triangular matrix L 


\vspace{0.3cm}
const MatrixType\& \textbf{matrixLLT}  ( )  const 
inline  

Returns: the LLT decomposition matrix

TODO: document the storage layout 


\vspace{0.3cm}
Traits::MatrixU \textbf{matrixU}  ( )  const 

Returns: a view of the upper triangular matrix U 


\vspace{0.3cm}
LLT<\_MatrixType,\_UpLo> \textbf{rankUpdate}  ( const VectorType \&  v, const RealScalar \&  sigma )   

Performs a rank one update (or dowdate) of the current decomposition. 
If $A = LL^*$ before the rank one update, then after it we have $LL^* = A + \sigma \times v v^*$ where $v$ must be a vector of same dimension. 
References Eigen::NumericalIssue, and Eigen::Success.

\vspace{0.3cm}
MatrixType \textbf{reconstructedMatrix}  ( )  const 

Returns: the matrix represented by the decomposition, i.e., it returns the product: $L L^*$. This function is provided for debug purpose. 

\vspace{0.3cm}
const internal::solve\_retval<LLT, Rhs> \textbf{solve}  ( const MatrixBase< Rhs > \&  b)  const 

Returns: the solution $x$ of $A x = b$ using the current decomposition of $A$.
Since this LLT class assumes anyway that the matrix $A$ is invertible, the solution theoretically exists and is unique regardless of $b$.



\subsection{Linear Solver}

\begin{mpFunctionsExtract}
	\mpFunctionThree
	{SolveCholeskyLLT? mpNum[]? the solution $x$ of $A x = b$ , based on a Cholesky decomposition.}
	{A? mpNum[,]? A symmetric positive definite real matrix.}
	{B? mpNum[,]? A real vector or matrix.}
	{UpLo? Integer? the triangular part that will be used for the decompositon: Lower (default) or Upper. The other triangular part won't be read.}
\end{mpFunctionsExtract}

\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionThree
	{cplxSolveCholeskyLLT? mpNum[]? the solution $x$ of $A x = b$ , based on a Cholesky decomposition.}
	{A? mpNum[,]? A symmetric positive definite complex matrix.}
	{B? mpNum[,]? A complex vector or matrix.}
	{UpLo? Integer? the triangular part that will be used for the decompositon: Lower (default) or Upper. The other triangular part won't be read.}
\end{mpFunctionsExtract}






\subsection{Matrix Inversion}

\begin{mpFunctionsExtract}
	\mpFunctionTwo
	{InvertCholeskyLLT? mpNum[]? $A^{-1}$, the inverse of $A$, based on a Cholesky decomposition.}
	{A? mpNum[,]? A symmetric positive definite real matrix.}
	{UpLo? Integer? the triangular part that will be used for the decompositon: Lower (default) or Upper. The other triangular part won't be read.}
\end{mpFunctionsExtract}


\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionTwo
	{cplxInvertCholeskyLLT? mpNum[]? $A^{-1}$, the inverse of $A$, based on a Cholesky decomposition.}
	{A? mpNum[,]? A symmetric positive definite complex matrix.}
	{UpLo? Integer? the triangular part that will be used for the decompositon: Lower (default) or Upper. The other triangular part won't be read.}
\end{mpFunctionsExtract}




\subsection{Determinant}

\begin{mpFunctionsExtract}
	\mpFunctionTwo
	{DetCholeskyLLT? mpNum? $|A|$, the determinant of $A$, based on a Cholesky decomposition.}
	{A? mpNum[,]? A symmetric positive definite real matrix.}
	{UpLo? Integer? the triangular part that will be used for the decompositon: Lower (default) or Upper. The other triangular part won't be read.}
\end{mpFunctionsExtract}


\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionTwo
	{cplxDetCholeskyLLT? mpNum? $|A|$, the determinant of $A$, based on a Cholesky decomposition.}
	{A? mpNum[,]? A symmetric positive definite complex matrix.}
	{UpLo? Integer? the triangular part that will be used for the decompositon: Lower (default) or Upper. The other triangular part won't be read.}
\end{mpFunctionsExtract}




\subsection{Example}
Example:

\lstset{language={C++}}
\begin{lstlisting}
MatrixXd A(3,3);
A << 4,-1,2, -1,6,0, 2,0,5;
cout << "The matrix A is" << endl << A << endl;
LLT<MatrixXd> lltOfA(A);
// compute the Cholesky decomposition of AMatrixXd L = lltOfA.matrixL();
// retrieve factor L  in the decomposition
// The previous two lines can also be written as "L = A.llt().matrixL()"
cout << "The Cholesky factor L is" << endl << L << endl;
cout << "To check this, let us compute L * L.transpose()" << endl;
cout << L * L.transpose() << endl;
cout << "This should equal the matrix A" << endl;
\end{lstlisting}

\begin{verbatim}
Output:
The matrix A is
4 -1  2
-1  6  0
2  0  5
The Cholesky factor L is
2     0     0
-0.5   2.4     0
1 0.209  1.99
To check this, let us compute L * L.transpose()
4 -1  2
-1  6  0
2  0  5
This should equal the matrix A
\end{verbatim}



\newpage
\section{Cholesky Decomposition with Pivoting}
\label{Cholesky Decomposition with Pivoting}

\subsection{Decomposition}

\begin{mpFunctionsExtract}
	\mpFunctionFour
	{DecompCholeskyLDLT? mpNumList? the Cholesky decomposition with pivoting of $A = LL^* = U^*U$.}
	{A? mpNum[,]? the real matrix of which we are computing the $LL^T$ Cholesky decomposition.}
	{B? mpNum[,]? A vector or matrix of real numbers.}
	{UpLo? Integer? the triangular part that will be used for the decompositon: Lower (default) or Upper. The other triangular part won't be read.}
	{Output? String? A string specifying the output options.}
\end{mpFunctionsExtract}

\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionFour
	{cplxDecompCholeskyLDLT? mpNumList? the Cholesky decomposition with pivoting of $A = LL^* = U^*U$.}
	{A? mpNum[,]? the complex matrix of which we are computing the $LL^T$ Cholesky decomposition.}
	{B? mpNum[,]? A vector or complex of real numbers.}
	{UpLo? Integer? the triangular part that will be used for the decompositon: Lower (default) or Upper. The other triangular part won't be read.}
	{Output? String? A string specifying the output options.}
\end{mpFunctionsExtract}


\vspace{0.3cm}
Perform a robust Cholesky decomposition of a positive semidefinite or negative semidefinite matrix  such that $A=P^T LDL^{*} P$, where $P$ is a permutation matrix, $L$ is lower triangular with a unit diagonal and $D$ is a diagonal matrix.

The decomposition uses pivoting to ensure stability, so that $L$ will have zeros in the bottom right rank$(A) - n$ submatrix. Avoiding the square root on $D$ also stabilizes the computation.

Remember that Cholesky decompositions are not rank-revealing. Also, do not use a Cholesky decomposition to determine whether a system of equations has a solution.


\vspace{0.3cm}
LDLT< MatrixType, \_UpLo > \& \textbf{compute}  ( const MatrixType \&  a)   

Compute / recompute the LDLT decomposition $A = L D L^* = U^* D U$ of matrix 


\vspace{0.3cm}
ComputationInfo \textbf{info}  ( )  const 

Reports whether previous computation was successful. 

Returns Success if computation was succesful, NumericalIssue if the matrix.appears to be negative. 


\vspace{0.3cm}
bool \textbf{isNegative}  ( void  )  const 

Returns true if the matrix is negative (semidefinite) 


\vspace{0.3cm}
bool \textbf{isPositive}  ( )  const 

Returns true if the matrix is positive (semidefinite) 


\vspace{0.3cm}
Traits::MatrixL \textbf{matrixL}  ( )  const 

Returns a view of the lower triangular matrix L 


\vspace{0.3cm}
const MatrixType\& \textbf{matrixLDLT}  ( )  const 

Returns the internal LDLT decomposition matrix
TODO: document the storage layout 


\vspace{0.3cm}
Traits::MatrixU \textbf{matrixU}  ( )  const 

Returns a view of the upper triangular matrix U 


\vspace{0.3cm}
LDLT<MatrixType,\_UpLo>\& \textbf{rankUpdate}  ( const MatrixBase< Derived > \&  w,  const typename NumTraits< typename MatrixType::Scalar >::Real \&  sigma)   

Update the LDLT decomposition: given $A = L D L^T$, efficiently compute the decomposition of $A + \sigma w w^T$. 

Parameters: w a vector to be incorporated into the decomposition.  
sigma a scalar, +1 for updates and -1 for "downdates," which correspond to removing previously-added column vectors. Optional; default value is +1.  


\vspace{0.3cm}
MatrixType \textbf{reconstructedMatrix}  ( )  const 

Returns the matrix represented by the decomposition, i.e., it returns the product: $P^T L D L^* P$. This function is provided for debug purpose. 

\vspace{0.3cm}
void \textbf{setZero}  ( )   

Clear any existing decomposition 

\vspace{0.3cm}
const internal::solve\_retval<LDLT, Rhs> \textbf{solve}  ( const MatrixBase< Rhs > \&  b)  const 

Returns a solution $x$ of $A x = b$ using the current decomposition of $A$.

This function also supports in-place solves using the syntax x = decompositionObject.solve(x) .

This method just tries to find as good a solution as possible. If you want to check whether a solution exists or if it is accurate, just call this function to get a result and then compute the error of this result, or use MatrixBase::isApprox() directly, for instance like this:

bool a\_solution\_exists = (A*result).isApprox(b, precision); 

This method avoids dividing by zero, so that the non-existence of a solution doesn't by itself mean that you'll get inf or nan values.

More precisely, this method solves  $A x = b$  using the decomposition $A = P^T L D L^* P$ by solving the systems $P^T y_1 = b, L Y_2 = y_1, D y_3 = y_2$, $L^* y_4 = y_3$ and $P x = y_4$ in succession. If the matrix $A$ is singular, then $D$ will also be singular (all the other matrices are invertible). In that case, the least-square solution of $D y_3 = y_2$ is computed. This does not mean that this function computes the least-square solution of $A x = b$ if $A$ is singular.


\vspace{0.3cm}
const TranspositionType\& \textbf{transpositionsP}  ( )  const 

Returns the permutation matrix P as a transposition sequence. 

\vspace{0.3cm}
Diagonal<const MatrixType> \textbf{vectorD}  ( )  const 

Returns the coefficients of the diagonal matrix D 



\subsection{Linear Solver}

\begin{mpFunctionsExtract}
	\mpFunctionThree
	{SolveCholeskyLDLT? mpNum[]? the solution $x$ of $A x = b$ , based on a Cholesky decomposition with pivoting.}
	{A? mpNum[,]? A symmetric positive definite real matrix.}
	{B? mpNum[,]? A real vector or matrix.}
	{UpLo? Integer? the triangular part that will be used for the decompositon: Lower (default) or Upper. The other triangular part won't be read.}
\end{mpFunctionsExtract}


\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionThree
	{cplxSolveCholeskyLDLT? mpNum[]? the solution $x$ of $A x = b$ , based on a Cholesky decomposition with pivoting.}
	{A? mpNum[,]? A symmetric positive definite complex matrix.}
	{B? mpNum[,]? A complex vector or matrix.}
	{UpLo? Integer? the triangular part that will be used for the decompositon: Lower (default) or Upper. The other triangular part won't be read.}
\end{mpFunctionsExtract}




\subsection{Matrix Inversion}

\begin{mpFunctionsExtract}
	\mpFunctionTwo
	{InvertCholeskyLDLT? mpNum[]? $A^{-1}$, the inverse of $A$, based on a Cholesky decomposition with pivoting.}
	{A? mpNum[,]? A symmetric positive definite real matrix.}
	{UpLo? Integer? the triangular part that will be used for the decompositon: Lower (default) or Upper. The other triangular part won't be read.}
\end{mpFunctionsExtract}

\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionTwo
	{cplxInvertCholeskyLDLT? mpNum[]? $A^{-1}$, the inverse of $A$, based on a Cholesky decomposition with pivoting.}
	{A? mpNum[,]? A symmetric positive definite complex matrix.}
	{UpLo? Integer? the triangular part that will be used for the decompositon: Lower (default) or Upper. The other triangular part won't be read.}
\end{mpFunctionsExtract}




\subsection{Determinant}

\begin{mpFunctionsExtract}
	\mpFunctionTwo
	{DetCholeskyLDLT? mpNum? $|A|$, the determinant of $A$, based on a Cholesky decomposition.}
	{A? mpNum[,]? A symmetric positive definite real matrix.}
	{UpLo? Integer? the triangular part that will be used for the decompositon: Lower (default) or Upper. The other triangular part won't be read.}
\end{mpFunctionsExtract}

\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionTwo
	{cplxDetCholeskyLDLT? mpNum? $|A|$, the determinant of $A$, based on a Cholesky decomposition.}
	{A? mpNum[,]? A symmetric positive definite complex matrix.}
	{UpLo? Integer? the triangular part that will be used for the decompositon: Lower (default) or Upper. The other triangular part won't be read.}
\end{mpFunctionsExtract}




\subsection{Example}
Example:

\lstset{language={C++}}
\begin{lstlisting}
MatrixXd A(3,3);
A << 4,-1,2, -1,6,0, 2,0,5;
cout << "The matrix A is" << endl << A << endl;
LLT<MatrixXd> lltOfA(A);
// compute the Cholesky decomposition of AMatrixXd L = lltOfA.matrixL();
// retrieve factor L  in the decomposition
// The previous two lines can also be written as "L = A.llt().matrixL()"
cout << "The Cholesky factor L is" << endl << L << endl;
cout << "To check this, let us compute L * L.transpose()" << endl;
cout << L * L.transpose() << endl;
cout << "This should equal the matrix A" << endl;
\end{lstlisting}

\begin{verbatim}
Output:
The matrix A is
4 -1  2
-1  6  0
2  0  5
The Cholesky factor L is
2     0     0
-0.5   2.4     0
1 0.209  1.99
To check this, let us compute L * L.transpose()
4 -1  2
-1  6  0
2  0  5
This should equal the matrix A
\end{verbatim}



\newpage
\section{LU Decomposition with partial Pivoting}
\label{LU Decomposition with partial Pivoting}

\subsection{Decomposition}

\begin{mpFunctionsExtract}
	\mpFunctionThree
	{DecompPartialPivLU? mpNumList? the LU decomposition with partial pivoting of $A = PLU$.}
	{A? mpNum[,]? the square real matrix of which we are computing the $LU$ decomposition.}
	{B? mpNum[,]? A vector or matrix of real numbers.}
	{Output? String? A string specifying the output options.}
\end{mpFunctionsExtract}

\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionThree
	{cplxDecompPartialPivLU? mpNumList? the LU decomposition with partial pivoting of $A = PLU$.}
	{A? mpNum[,]? the square complex matrix of which we are computing the $LU$ decomposition.}
	{B? mpNum[,]? A vector or complex of real numbers.}
	{Output? String? A string specifying the output options.}
\end{mpFunctionsExtract}



\vspace{0.3cm}

This class represents a LU decomposition of a square invertible matrix, with partial pivoting: the matrix $A$ is decomposed as $A = PLU$ where $L$ is unit-lower-triangular, $U$ is upper-triangular, and $P$ is a permutation matrix.

Typically, partial pivoting LU decomposition is only considered numerically stable for square invertible matrices. Thus \textsc{LAPACK}'s \textsf{dgesv} and \textsf{dgesvx} require the matrix to be square and invertible. The present class does the same. It will assert that the matrix is square, but it won't (actually it can't) check that the matrix is invertible: it is your task to check that you only use this decomposition on invertible matrices.

The guaranteed safe alternative, working for all matrices, is the full pivoting LU decomposition, provided by class FullPivLU.

This is not a rank-revealing LU decomposition. Many features are intentionally absent from this class, such as rank computation. If you need these features, use class FullPivLU.

This LU decomposition is suitable to invert invertible matrices. It is what MatrixBase::inverse() uses in the general case. On the other hand, it is not suitable to determine whether a given matrix is invertible.

The data of the LU decomposition can be directly accessed through the methods matrixLU(), permutationP().



Returns the determinant of the matrix of which *this is the LU decomposition. It has only linear complexity (that is, O(n) where n is the dimension of the square matrix) as the LU decomposition has already been computed.

Warning: a determinant can be very big or small, so for matrices of large enough dimension, there is a risk of overflow/underflow.See Also
MatrixBase::determinant() 

\vspace{0.3cm}
const internal::solve\_retval<PartialPivLU,typename MatrixType::IdentityReturnType> \textbf{inverse}  ( )  const 


Returns the inverse of the matrix of which *this is the LU decomposition.

Warning: The matrix being decomposed here is assumed to be invertible. If you need to check for invertibility, use class FullPivLU instead.


\vspace{0.3cm}
const MatrixType\& \textbf{matrixLU}  ( )  const 

Returns the LU decomposition matrix: the upper-triangular part is U, the unit-lower-triangular part is L (at least for square matrices; in the non-square case, special care is needed, see the documentation of class FullPivLU).


\vspace{0.3cm}
const PermutationType\& \textbf{permutationP}  ( )  const 

Returns the permutation matrix P. 


\vspace{0.3cm}
MatrixType \textbf{reconstructedMatrix}  ( )  const 

Returns the matrix represented by the decomposition, i.e., it returns the product: $P^{-1} L U$. This function is provided for debug purpose. 


\vspace{0.3cm}
const internal::solve\_retval<PartialPivLU, Rhs> \textbf{solve}  ( const MatrixBase< Rhs > \&  b)  const 

This method returns the solution $x$ to the equation $Ax=b$, where $A$ is the matrix of which *this is the LU decomposition.

Parameters: $b$ the right-hand-side of the equation to solve. Can be a vector or a matrix, the only requirement in order for the equation to make sense is that b.rows()==A.rows(), where $A$ is the matrix of which *this is the LU decomposition. 

Returns the solution.





\subsection{Linear Solver}

\begin{mpFunctionsExtract}
	\mpFunctionTwo
	{SolvePartialPivLU? mpNum[]? the solution $x$ of $A x = b$ , based on a LU decomposition with partial pivoting.}
	{A? mpNum[,]? A square real matrix.}
	{B? mpNum[,]? A real vector or matrix.}
\end{mpFunctionsExtract}


\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionTwo
	{cplxSolvePartialPivLU? mpNum[]? the solution $x$ of $A x = b$ , based on a LU decomposition with partial pivoting.}
	{A? mpNum[,]? A square complex matrix.}
	{B? mpNum[,]? A complex vector or matrix.}
\end{mpFunctionsExtract}





\subsection{Matrix Inversion}

\begin{mpFunctionsExtract}
	\mpFunctionOne
	{InvertPartialPivLU? mpNum[]? $A^{-1}$, the inverse of $A$, based on a LU decomposition with partial pivoting.}
	{A? mpNum[,]? A square real matrix.}
\end{mpFunctionsExtract}

\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionOne
	{cplxInvertPartialPivLU? mpNum[]? $A^{-1}$, the inverse of $A$, based on a LU decomposition with partial pivoting.}
	{A? mpNum[,]? A square complex matrix.}
\end{mpFunctionsExtract}




\subsection{Determinant}

\begin{mpFunctionsExtract}
	\mpFunctionOne
	{DetPartialPivLU? mpNum? $|A|$, the determinant of $A$, based on a LU decomposition with partial pivoting.}
	{A? mpNum[,]? A square real matrix.}
\end{mpFunctionsExtract}

\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionOne
	{cplxDetPartialPivLU? mpNum? $|A|$, the determinant of $A$, based on a LU decomposition with partial pivoting.}
	{A? mpNum[,]? A square complex matrix.}
\end{mpFunctionsExtract}




\subsection{Example}
Example:

\lstset{language={C++}}
\begin{lstlisting}
MatrixXd A = MatrixXd::Random(3,3);
MatrixXd B = MatrixXd::Random(3,2);
cout << "Here is the invertible matrix A:" << endl << A << endl;
cout << "Here is the matrix B:" << endl << B << endl;
MatrixXd X = A.lu().solve(B);
cout << "Here is the (unique) solution X to the equation AX=B:" 
<< endl << X << endl;
cout << "Relative error: " << (A*X-B).norm() / B.norm() << endl;
\end{lstlisting}

\begin{verbatim}
Output:
Here is the invertible matrix A:
0.68  0.597  -0.33
-0.211  0.823  0.536
0.566 -0.605 -0.444
Here is the matrix B:
0.108   -0.27
-0.0452  0.0268
0.258   0.904
Here is the (unique) solution X to the equation AX=B:
0.609   2.68
-0.231  -1.57
0.51   3.51
Relative error: 3.28e-16
\end{verbatim}


\newpage
\section{LU Decomposition with full Pivoting}
\label{LU Decomposition with full Pivoting}



\subsection{Decomposition}

\begin{mpFunctionsExtract}
	\mpFunctionThree
	{DecompFullPivLU? mpNumList? the LU decomposition with full pivoting of $A = PLUQ$.}
	{A? mpNum[,]? the square real matrix of which we are computing the $LU$ decomposition.}
	{B? mpNum[,]? A vector or matrix of real numbers.}
	{Output? String? A string specifying the output options.}
\end{mpFunctionsExtract}

\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionThree
	{cplxDecompFullPivLU? mpNumList? the LU decomposition with full pivoting of $A = PLUQ$.}
	{A? mpNum[,]? the square complex matrix of which we are computing the $LU$ decomposition.}
	{B? mpNum[,]? A vector or complex of real numbers.}
	{Output? String? A string specifying the output options.}
\end{mpFunctionsExtract}


\vspace{0.3cm}

This class represents a LU decomposition of any matrix, with complete pivoting: the matrix $A$ is decomposed as $A = PLUQ$ where $L$ is unit-lower-triangular, $U$ is upper-triangular, and $P$ and $Q$ are permutation matrices. This is a rank-revealing LU decomposition. The eigenvalues (diagonal coefficients) of $U$ are sorted in such a way that any zeros are at the end.

This decomposition provides the generic approach to solving systems of linear equations, computing the rank, invertibility, inverse, kernel, and determinant.
This LU decomposition is very stable and well tested with large matrices. However there are use cases where the SVD decomposition is inherently more stable and/or flexible. For example, when computing the kernel of a matrix, working with the SVD allows to select the smallest singular values of the matrix, something that the LU decomposition doesn't see.

The data of the LU decomposition can be directly accessed through the methods matrixLU(), permutationP(), permutationQ().


Computes the LU decomposition of the given matrix.

Parameters: matrix the matrix of which to compute the LU decomposition. It is required to be nonzero. 

Returns: a reference to *this 
Referenced by FullPivLU< MatrixType >::FullPivLU().


\vspace{0.3cm}
internal::traits< MatrixType >::Scalar \textbf{determinant}  ( )  const 

Returns the determinant of the matrix of which *this is the LU decomposition. It has only linear complexity (that is, O(n) where n is the dimension of the square matrix) as the LU decomposition has already been computed.

For fixed-size matrices of size up to 4, MatrixBase::determinant() offers optimized paths.

Warning: a determinant can be very big or small, so for matrices of large enough dimension, there is a risk of overflow/underflow.


\vspace{0.3cm}
Index \textbf{dimensionOfKernel}  ( )  const 

Returns the dimension of the kernel of the matrix of which *this is the LU decomposition.
Note: This method has to determine which pivots should be considered nonzero. For that, it uses the threshold value that you can control by calling setThreshold(const RealScalar\&). References FullPivLU< MatrixType >::rank().

\vspace{0.3cm}
const internal::image\_retval<FullPivLU> \textbf{image}  ( const MatrixType \&  originalMatrix)  const 

Returns the image of the matrix, also called its column-space. The columns of the returned matrix will form a basis of the kernel.

Parameters: originalMatrix the original matrix, of which *this is the LU decomposition. The reason why it is needed to pass it here, is that this allows a large optimization, as otherwise this method would need to reconstruct it from the LU decomposition. 

Note: If the image has dimension zero, then the returned matrix is a column-vector filled with zeros.
This method has to determine which pivots should be considered nonzero. For that, it uses the threshold value that you can control by calling setThreshold(const RealScalar\&).


Example:

\lstset{language={C++}}
\begin{lstlisting}
Matrix3d m;m << 1,1,0,     1,3,2,     0,1,1;
cout << "Here is the matrix m:" << endl << m << endl;
cout << "Notice that the middle column is the sum of the two others, so the "
<< "columns are linearly dependent." << endl;
cout << "Here is a matrix whose columns have the same span but are linearly independent:"     
<< endl << m.fullPivLu().image(m) << endl;
\end{lstlisting}

\begin{verbatim}
Output:
Here is the matrix m:
1 1 0
1 3 2
0 1 1
Notice that the middle column is the sum of the two others, so the columns are linearly dependent.
Here is a matrix whose columns have the same span but are linearly independent:
1 1
3 1
1 0
\end{verbatim}


\vspace{0.3cm}
const internal::solve\_retval<FullPivLU,typename MatrixType::IdentityReturnType> \textbf{inverse}  ( )  const 

Returns the inverse of the matrix of which *this is the LU decomposition.
Note: If this matrix is not invertible, the returned matrix has undefined coefficients. Use isInvertible() to first determine whether this matrix is invertible.


\vspace{0.3cm}
bool \textbf{isInjective}  ( )  const 

Returns true if the matrix of which *this is the LU decomposition represents an injective linear map, i.e. has trivial kernel; false otherwise.
Note: This method has to determine which pivots should be considered nonzero. For that, it uses the threshold value that you can control by calling setThreshold(const RealScalar\&). References FullPivLU< MatrixType >::rank().


\vspace{0.3cm}
bool \textbf{isInvertible}  ( )  const 

Returns true if the matrix of which *this is the LU decomposition is invertible.
Note: This method has to determine which pivots should be considered nonzero. For that, it uses the threshold value that you can control by calling setThreshold(const RealScalar\&). References FullPivLU< MatrixType >::isInjective().


\vspace{0.3cm}
bool \textbf{isSurjective}  ( )  const 

Returns true if the matrix of which *this is the LU decomposition represents a surjective linear map; false otherwise.
Note: This method has to determine which pivots should be considered nonzero. For that, it uses the threshold value that you can control by calling setThreshold(const RealScalar\&). References FullPivLU< MatrixType >::rank().


\vspace{0.3cm}
const internal::kernel\_retval<FullPivLU> \textbf{kernel}  ( )  const 

Returns the kernel of the matrix, also called its null-space. The columns of the returned matrix will form a basis of the kernel.
Note: If the kernel has dimension zero, then the returned matrix is a column-vector filled with zeros.
This method has to determine which pivots should be considered nonzero. For that, it uses the threshold value that you can control by calling setThreshold(const RealScalar\&).


Example:

\lstset{language={C++}}
\begin{lstlisting}
MatrixXf m = MatrixXf::Random(3,5);
cout << "Here is the matrix m:" << endl << m << endl;
MatrixXf ker = m.fullPivLu().kernel();
cout << "Here is a matrix whose columns form a basis of the kernel of m:"     
<< endl << ker << endl;cout << "By definition of the kernel, m*ker is zero:"     
<< endl << m*ker << endl;
\end{lstlisting}

\begin{verbatim}		
Output:
Here is the matrix m:
0.68   0.597   -0.33   0.108   -0.27
-0.211   0.823   0.536 -0.0452  0.0268
0.566  -0.605  -0.444   0.258   0.904
Here is a matrix whose columns form a basis of the kernel of m:
-0.219   0.763
0.00335  -0.447
0       1
1       0
-0.145  -0.285
By definition of the kernel, m*ker is zero:
-1.12e-08  1.49e-08
-1.4e-09 -4.05e-08
1.49e-08 -2.98e-08
\end{verbatim}


\vspace{0.3cm}
const MatrixType\& \textbf{matrixLU}  ( )  const 

Returns the LU decomposition matrix: the upper-triangular part is U, the unit-lower-triangular part is L (at least for square matrices; in the non-square case, special care is needed, see the documentation of class FullPivLU).


\vspace{0.3cm}
RealScalar \textbf{maxPivot}  ( )  const 

Returns the absolute value of the biggest pivot, i.e. the biggest diagonal coefficient of U. 


\vspace{0.3cm}
Index \textbf{nonzeroPivots}  ( )  const 

Returns the number of nonzero pivots in the LU decomposition. Here nonzero is meant in the exact sense, not in a fuzzy sense. So that notion isn't really intrinsically interesting, but it is still useful when implementing algorithms.


\vspace{0.3cm}
const PermutationPType\& \textbf{permutationP}  ( )  const 
inline  

Returns the permutation matrix P


\vspace{0.3cm}
const PermutationQType\& \textbf{permutationQ}  ( )  const 

Returns the permutation matrix Q


\vspace{0.3cm}
Index \textbf{rank}  ( )  const 

Returns the rank of the matrix of which *this is the LU decomposition.
Note: This method has to determine which pivots should be considered nonzero. For that, it uses the threshold value that you can control by calling setThreshold(const RealScalar\&). References FullPivLU< MatrixType >::threshold().


\vspace{0.3cm}
MatrixType \textbf{reconstructedMatrix}  ( )  const 

Returns the matrix represented by the decomposition, i.e., it returns the product: $P^{-1} L U Q^{-1}$. This function is provided for debug purpose. 


\vspace{0.3cm}
FullPivLU\& \textbf{setThreshold}  ( const RealScalar \&  threshold)   

Allows to prescribe a threshold to be used by certain methods, such as rank(), who need to determine when pivots are to be considered nonzero. This is not used for the LU decomposition itself.
When it needs to get the threshold value, Eigen calls threshold(). By default, this uses a formula to automatically determine a reasonable threshold. Once you have called the present method setThreshold(const RealScalar\&), your value is used instead.

Parameters: threshold The new value to use as the threshold. 

A pivot will be considered nonzero if its absolute value is strictly greater than  where maxpivot is the biggest pivot.
If you want to come back to the default behavior, call setThreshold(Default\_t) 
References FullPivLU< MatrixType >::threshold().


\vspace{0.3cm}
FullPivLU\& \textbf{setThreshold}  ( Default\_t  )   

Allows to come back to the default behavior, letting Eigen use its default formula for determining the threshold.
You should pass the special object Eigen::Default as parameter here. 
lu.setThreshold(Eigen::Default); See the documentation of setThreshold(const RealScalar\&). 


\vspace{0.3cm}
const internal::solve\_retval<FullPivLU, Rhs> \textbf{solve}  ( const MatrixBase< Rhs > \&  b)  const 

Returns a solution $x$ to the equation $Ax=b$, where $A$ is the matrix of which *this is the LU decomposition.
Parameters: $b$ the right-hand-side of the equation to solve. Can be a vector or a matrix, the only requirement in order for the equation to make sense is that b.rows()==A.rows(), where A is the matrix of which *this is the LU decomposition. 

Returns a solution. This method just tries to find as good a solution as possible. If you want to check whether a solution exists or if it is accurate, just call this function to get a result and then compute the error of this result, or use MatrixBase::isApprox() directly, for instance like this:

\begin{verbatim}
bool a\_solution\_exists = (A*result).isApprox(b, precision); 
\end{verbatim}

This method avoids dividing by zero, so that the non-existence of a solution doesn't by itself mean that you'll get inf or nan values.
If there exists more than one solution, this method will arbitrarily choose one. If you need a complete analysis of the space of solutions, take the one solution obtained by this method and add to it elements of the kernel, as determined by kernel().



Example:
\lstset{language={C++}}
\begin{lstlisting}
Matrix<float,2,3> m = Matrix<float,2,3>::Random();
Matrix2f y = Matrix2f::Random();
cout << "Here is the matrix m:" << endl << m << endl;
cout << "Here is the matrix y:" << endl << y << endl;
Matrix<float,3,2> x = m.fullPivLu().solve(y);
if((m*x).isApprox(y))
{  cout << "Here is a solution x to the equation mx=y:" << endl << x << endl;}
else  cout << "The equation mx=y does not have any solution." << endl;
\end{lstlisting}

\begin{verbatim}
Output:
Here is the matrix m:
0.68  0.566  0.823
-0.211  0.597 -0.605
Here is the matrix y:
-0.33 -0.444
0.536  0.108
Here is a solution x to the equation mx=y:
0      0
0.291 -0.216
-0.6 -0.391

\end{verbatim}

\vspace{0.3cm}
RealScalar \textbf{threshold}  ( )  const 

Returns the threshold that will be used by certain methods such as rank().
See the documentation of \textbf{setThreshold}(const RealScalar\&). 



\subsection{Linear Solver}


\begin{mpFunctionsExtract}
	\mpFunctionTwo
	{SolveFullPivLU? mpNum[]? the solution $x$ of $A x = b$ , based on a LU decomposition with full pivoting.}
	{A? mpNum[,]? A square real matrix.}
	{B? mpNum[,]? A real vector or matrix.}
\end{mpFunctionsExtract}


\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionTwo
	{cplxSolveFullPivLU? mpNum[]? the solution $x$ of $A x = b$ , based on a LU decomposition with full pivoting.}
	{A? mpNum[,]? A square complex matrix.}
	{B? mpNum[,]? A complex vector or matrix.}
\end{mpFunctionsExtract}




\subsection{Matrix Inversion}


\begin{mpFunctionsExtract}
	\mpFunctionOne
	{InvertFullPivLU? mpNum[]? $A^{-1}$, the inverse of $A$, based on a LU decomposition with full pivoting.}
	{A? mpNum[,]? A square real matrix.}
\end{mpFunctionsExtract}

\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionOne
	{cplxInvertFullPivLU? mpNum[]? $A^{-1}$, the inverse of $A$, based on a LU decomposition with full pivoting.}
	{A? mpNum[,]? A square complex matrix.}
\end{mpFunctionsExtract}




\subsection{Determinant}


\begin{mpFunctionsExtract}
	\mpFunctionOne
	{DetFullPivLU? mpNum? $|A|$, the determinant of $A$, based on a LU decomposition with full pivoting.}
	{A? mpNum[,]? A square real matrix.}
\end{mpFunctionsExtract}

\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionOne
	{cplxDetFullPivLU? mpNum? $|A|$, the determinant of $A$, based on a LU decomposition with full pivoting.}
	{A? mpNum[,]? A square complex matrix.}
\end{mpFunctionsExtract}





\newpage
\section{QR Decomposition without Pivoting}
\label{QR Decomposition without Pivoting}




\subsection{Decomposition}


\begin{mpFunctionsExtract}
	\mpFunctionThree
	{DecompQR? mpNumList? the QR decomposition without pivoting of $A = QR$.}
	{A? mpNum[,]? the square real matrix of which we are computing the $LU$ decomposition.}
	{B? mpNum[,]? A vector or matrix of real numbers.}
	{Output? String? A string specifying the output options.}
\end{mpFunctionsExtract}

\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionThree
	{cplxDecompQR? mpNumList? the QR decomposition without pivoting of $A = QR$.}
	{A? mpNum[,]? the square complex matrix of which we are computing the $LU$ decomposition.}
	{B? mpNum[,]? A vector or complex of real numbers.}
	{Output? String? A string specifying the output options.}
\end{mpFunctionsExtract}


\vspace{0.3cm}

This class performs a QR decomposition of a matrix $A$ into matrices $Q$ and $R$ such that
\begin{equation}
A=QR
\end{equation} 
by using Householder transformations. Here, $Q$ a unitary matrix and $R$ an upper triangular matrix. The result is stored in a compact way compatible with LAPACK.
Note that no pivoting is performed. This is not a rank-revealing decomposition. If you want that feature, use \textsf{FullPivHouseholderQR} or \textsf{ColPivHouseholderQR} instead.

This Householder QR decomposition is faster, but less numerically stable and less feature-rich than \textsf{FullPivHouseholderQR} or \textsf{ColPivHouseholderQR}.


Member Function DocumentationMatrixType::RealScalar \textbf{absDeterminant}  ( )  const 

Returns the absolute value of the determinant of the matrix of which *this is the QR decomposition. It has only linear complexity (that is, O(n) where n is the dimension of the square matrix) as the QR decomposition has already been computed.
Note: This is only for square matrices.
Warning: a determinant can be very big or small, so for matrices of large enough dimension, there is a risk of overflow/underflow. One way to work around that is to use logAbsDeterminant() instead. See Also logAbsDeterminant(), MatrixBase::determinant() 


\vspace{0.3cm}
HouseholderQR< MatrixType > \& \textbf{compute}  ( const MatrixType \&  matrix)   

Performs the QR factorization of the given matrix matrix. The result of the factorization is stored into *this, and a reference to *this is returned.
See Also: class HouseholderQR, HouseholderQR(const MatrixType\&) 


\vspace{0.3cm}
const HCoeffsType\& \textbf{hCoeffs}  ( )  const 

Returns a const reference to the vector of Householder coefficients used to represent the factor Q.
For advanced uses only. 


\vspace{0.3cm}
HouseholderSequenceType \textbf{householderQ}  ( void  )  const 

This method returns an expression of the unitary matrix Q as a sequence of Householder transformations.
The returned expression can directly be used to perform matrix products. It can also be assigned to a dense Matrix object. Here is an example showing how to recover the full or thin matrix Q, as well as how to perform matrix products using operator*:

Example:
\lstset{language={C++}}
\begin{lstlisting}
MatrixXf A(MatrixXf::Random(5,3)), thinQ(MatrixXf::Identity(5,3)), 
Q;A.setRandom();
HouseholderQR<MatrixXf> qr(A);
Q = qr.householderQ();
thinQ = qr.householderQ() * thinQ;
std::cout << "The complete unitary matrix Q is:\n" << Q << "\n\n";
std::cout << "The thin matrix Q is:\n" << thinQ << "\n\n";
\end{lstlisting}

\begin{verbatim}
Output:
The complete unitary matrix Q is:
-0.676   0.0793    0.713  -0.0788   -0.147
-0.221   -0.322    -0.37   -0.366   -0.759
-0.353   -0.345   -0.214    0.841  -0.0518
0.582   -0.462    0.555    0.176   -0.329
-0.174   -0.747 -0.00907   -0.348    0.539

The thin matrix Q is:
-0.676   0.0793    0.713
-0.221   -0.322    -0.37
-0.353   -0.345   -0.214
0.582   -0.462    0.555
-0.174   -0.747 -0.00907
\end{verbatim}


\vspace{0.3cm}
MatrixType::RealScalar \textbf{logAbsDeterminant}  ( )  const 

Returns the natural log of the absolute value of the determinant of the matrix of which *this is the QR decomposition. It has only linear complexity (that is, O(n) where n is the dimension of the square matrix) as the QR decomposition has already been computed.
Note: This is only for square matrices.
This method is useful to work around the risk of overflow/underflow that's inherent to determinant computation.See Also
absDeterminant(), MatrixBase::determinant() 


\vspace{0.3cm}
const MatrixType\& \textbf{matrixQR}  ( )  const 

Returns a reference to the matrix where the Householder QR decomposition is stored in a LAPACK-compatible way. 


const internal::solve\_retval<HouseholderQR, Rhs> \textbf{solve}  ( const MatrixBase< Rhs > \&  b)  const 

This method finds a solution $x$ to the equation $Ax=b$, where $A$ is the matrix of which *this is the QR decomposition, if any exists.
Parameters: b the right-hand-side of the equation to solve. 

Returns a solution.
Note: The case where b is a matrix is not yet implemented. Also, this code is space inefficient.This method just tries to find as good a solution as possible. If you want to check whether a solution exists or if it is accurate, just call this function to get a result and then compute the error of this result, or use MatrixBase::isApprox() directly, for instance like this:

\begin{verbatim}
bool a\_solution\_exists = (A*result).isApprox(b, precision); 
\end{verbatim}

This method avoids dividing by zero, so that the non-existence of a solution doesn't by itself mean that you'll get inf or nan values.
If there exists more than one solution, this method will arbitrarily choose one.


Example:
\lstset{language={C++}}
\begin{lstlisting}
typedef Matrix<float,3,3> Matrix3x3;
Matrix3x3 m = Matrix3x3::Random();
Matrix3f y = Matrix3f::Random();
cout << "Here is the matrix m:" << endl << m << endl;
cout << "Here is the matrix y:" << endl << y << endl;
Matrix3f x;
x = m.householderQr().solve(y);
assert(y.isApprox(m*x));
cout << "Here is a solution x to the equation mx=y:" << endl << x << endl;
\end{lstlisting}

\begin{verbatim}
Output:
Here is the matrix m:
0.68  0.597  -0.33
-0.211  0.823  0.536
0.566 -0.605 -0.444
Here is the matrix y:
0.108   -0.27   0.832
-0.0452  0.0268   0.271
0.258   0.904   0.435
Here is a solution x to the equation mx=y:
0.609   2.68   1.67
-0.231  -1.57 0.0713
0.51   3.51   1.05
\end{verbatim}




\subsection{Linear Solver}


\begin{mpFunctionsExtract}
	\mpFunctionTwo
	{SolveQR? mpNum[]? the solution $x$ of $A x = b$ , based on a QR decomposition without pivoting.}
	{A? mpNum[,]? A square real matrix.}
	{B? mpNum[,]? A real vector or matrix.}
\end{mpFunctionsExtract}


\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionTwo
	{cplxSolveQR? mpNum[]? the solution $x$ of $A x = b$ , based on a QR decomposition without pivoting.}
	{A? mpNum[,]? A square complex matrix.}
	{B? mpNum[,]? A complex vector or matrix.}
\end{mpFunctionsExtract}





\subsection{Matrix Inversion}

\begin{mpFunctionsExtract}
	\mpFunctionOne
	{InvertQR? mpNum[]? $A^{-1}$, the inverse of $A$, based on a QR decomposition without pivoting.}
	{A? mpNum[,]? A square real matrix.}
\end{mpFunctionsExtract}

\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionOne
	{cplxInvertQR? mpNum[]? $A^{-1}$, the inverse of $A$, based on a QR decomposition without pivoting.}
	{A? mpNum[,]? A square complex matrix.}
\end{mpFunctionsExtract}





\subsection{Determinant}

\begin{mpFunctionsExtract}
	\mpFunctionOne
	{DetQR? mpNum? $|A|$, the determinant of $A$, based on a QR decomposition without pivoting.}
	{A? mpNum[,]? A square real matrix.}
\end{mpFunctionsExtract}

\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionOne
	{cplxDetQR? mpNum? $|A|$, the determinant of $A$, based on a QR decomposition without pivoting.}
	{A? mpNum[,]? A square complex matrix.}
\end{mpFunctionsExtract}



\subsection{Example}
Example:
\begin{verbatim}
Example
\end{verbatim}





\newpage
\section{QR Decomposition with column Pivoting}
\label{QR Decomposition with column Pivoting}

\subsection{Decomposition}

\begin{mpFunctionsExtract}
	\mpFunctionThree
	{DecompColPivQR? mpNumList? the QR decomposition with column-pivoting of $A = QR$.}
	{A? mpNum[,]? the square real matrix of which we are computing the $LU$ decomposition.}
	{B? mpNum[,]? A vector or matrix of real numbers.}
	{Output? String? A string specifying the output options.}
\end{mpFunctionsExtract}

\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionThree
	{cplxDecompColPivQR? mpNumList? the QR decomposition with column-pivoting of $A = QR$.}
	{A? mpNum[,]? the square complex matrix of which we are computing the $LU$ decomposition.}
	{B? mpNum[,]? A vector or complex of real numbers.}
	{Output? String? A string specifying the output options.}
\end{mpFunctionsExtract}


\vspace{0.3cm}

This class performs a rank-revealing QR decomposition of a matrix $A$ into matrices $P$, $Q$ and $R$ such that 
\begin{equation}
AP=QR
\end{equation} 

by using Householder transformations. Here, $P$ is a permutation matrix, $Q$ a unitary matrix and $R$ an upper triangular matrix.

This decomposition performs column pivoting in order to be rank-revealing and improve numerical stability. 

It is slower than \textsf{HouseholderQR}, and faster than \textsf{FullPivHouseholderQR}.


Member Function DocumentationMatrixType::RealScalar \textbf{absDeterminant}  ( )  const 

Returns the absolute value of the determinant of the matrix of which *this is the QR decomposition. It has only linear complexity (that is, O(n) where n is the dimension of the square matrix) as the QR decomposition has already been computed.
Note: This is only for square matrices. 
Warning: a determinant can be very big or small, so for matrices of large enough dimension, there is a risk of overflow/underflow. One way to work around that is to use logAbsDeterminant() instead.
See Also logAbsDeterminant(), MatrixBase::determinant() 


\vspace{0.3cm}
const PermutationType\& \textbf{colsPermutation}  ( )  const 

Returns a const reference to the column permutation matrix 


\vspace{0.3cm}
ColPivHouseholderQR< MatrixType > \& \textbf{compute}  ( const MatrixType \&  matrix)   

Performs the QR factorization of the given matrix matrix. The result of the factorization is stored into *this, and a reference to *this is returned.
See Also class ColPivHouseholderQR, ColPivHouseholderQR(const MatrixType\&) 


\vspace{0.3cm}
Index \textbf{dimensionOfKernel}  ( )  const 

Returns the dimension of the kernel of the matrix of which *this is the QR decomposition.
Note: This method has to determine which pivots should be considered nonzero. For that, it uses the threshold value that you can control by calling setThreshold(const RealScalar\&). References ColPivHouseholderQR< MatrixType >::rank().


\vspace{0.3cm}
const HCoeffsType\& \textbf{hCoeffs}  ( )  const 

Returns a const reference to the vector of Householder coefficients used to represent the factor Q.
For advanced uses only. 


\vspace{0.3cm}
ColPivHouseholderQR< MatrixType >::HouseholderSequenceType \textbf{householderQ}  ( void  )  const 

Returns the matrix Q as a sequence of householder transformations 


\vspace{0.3cm}
ComputationInfo \textbf{info}  ( )  const 

Reports whether the QR factorization was succesful. 
Note: This function always returns Success. It is provided for compatibility with other factorization routines. 


\vspace{0.3cm}
const internal::solve\_retval<ColPivHouseholderQR, typename MatrixType::IdentityReturnType> \textbf{inverse}  ( )  const 

Returns the inverse of the matrix of which *this is the QR decomposition.
Note: If this matrix is not invertible, the returned matrix has undefined coefficients. Use isInvertible() to first determine whether this matrix is invertible. 

\vspace{0.3cm}
bool \textbf{isInjective}  ( )  const 

Returns true if the matrix of which *this is the QR decomposition represents an injective linear map, i.e. has trivial kernel; false otherwise.
Note: This method has to determine which pivots should be considered nonzero. For that, it uses the threshold value that you can control by calling setThreshold(const RealScalar\&). References ColPivHouseholderQR< MatrixType >::rank().


\vspace{0.3cm}
bool \textbf{isInvertible}  ( )  const 

Returns true if the matrix of which *this is the QR decomposition is invertible.
Note: This method has to determine which pivots should be considered nonzero. For that, it uses the threshold value that you can control by calling setThreshold(const RealScalar\&). References ColPivHouseholderQR< MatrixType >::isInjective(), and ColPivHouseholderQR< MatrixType >::isSurjective().


\vspace{0.3cm}
bool \textbf{isSurjective}  ( )  const 

Returns true if the matrix of which *this is the QR decomposition represents a surjective linear map; false otherwise.
Note: This method has to determine which pivots should be considered nonzero. For that, it uses the threshold value that you can control by calling setThreshold(const RealScalar\&). References ColPivHouseholderQR< MatrixType >::rank().


\vspace{0.3cm}
MatrixType::RealScalar \textbf{logAbsDeterminant}  ( )  const 

Returns the natural log of the absolute value of the determinant of the matrix of which *this is the QR decomposition. It has only linear complexity (that is, O(n) where n is the dimension of the square matrix) as the QR decomposition has already been computed.
Note: This is only for square matrices.
This method is useful to work around the risk of overflow/underflow that's inherent to determinant computation. See Also
absDeterminant(), MatrixBase::determinant() 


\vspace{0.3cm}
const MatrixType\& \textbf{matrixQR}  ( )  const 

Returns a reference to the matrix where the Householder QR decomposition is stored 


\vspace{0.3cm}
const MatrixType\& \textbf{matrixR}  ( )  const 

Returns a reference to the matrix where the result Householder QR is stored 
Warning: The strict lower part of this matrix contains internal values. Only the upper triangular part should be referenced. To get it, use 
matrixR().template triangularView<Upper>() For rank-deficient matrices, use matrixR().topLeftCorner(rank(), rank()).template triangularView<Upper>() RealScalar 


\vspace{0.3cm}
\textbf{maxPivot}  ( )  const 

Returns the absolute value of the biggest pivot, i.e. the biggest diagonal coefficient of R. 

\vspace{0.3cm}
Index \textbf{nonzeroPivots}  ( )  const 

Returns the number of nonzero pivots in the QR decomposition. Here nonzero is meant in the exact sense, not in a fuzzy sense. So that notion isn't really intrinsically interesting, but it is still useful when implementing algorithms.
See Also rank() .


\vspace{0.3cm}
Index \textbf{rank}  ( )  const 

Returns the rank of the matrix of which *this is the QR decomposition.
Note: This method has to determine which pivots should be considered nonzero. For that, it uses the threshold value that you can control by calling setThreshold(const RealScalar\&). References ColPivHouseholderQR< MatrixType >::threshold().


\vspace{0.3cm}
ColPivHouseholderQR\& \textbf{setThreshold}  ( const RealScalar \&  threshold)   

Allows to prescribe a threshold to be used by certain methods, such as rank(), who need to determine when pivots are to be considered nonzero. This is not used for the QR decomposition itself.
When it needs to get the threshold value, Eigen calls threshold(). By default, this uses a formula to automatically determine a reasonable threshold. Once you have called the present method setThreshold(const RealScalar\&), your value is used instead.
Parameters: threshold The new value to use as the threshold. 

A pivot will be considered nonzero if its absolute value is strictly greater than  where maxpivot is the biggest pivot.
If you want to come back to the default behavior, call setThreshold(Default\_t) 
References ColPivHouseholderQR< MatrixType >::threshold().

\vspace{0.3cm}
ColPivHouseholderQR\& \textbf{setThreshold}  ( Default\_t  )   

Allows to come back to the default behavior, letting Eigen use its default formula for determining the threshold.
You should pass the special object Eigen::Default as parameter here. 
qr.setThreshold(Eigen::Default); See the documentation of setThreshold(const RealScalar\&). 


\vspace{0.3cm}
const internal::solve\_retval<ColPivHouseholderQR, Rhs> \textbf{solve}  ( const MatrixBase< Rhs > \&  b)  const 

This method finds a solution x to the equation Ax=b, where A is the matrix of which *this is the QR decomposition, if any exists.
Parameters: b the right-hand-side of the equation to solve. 

Returns a solution.
Note: The case where b is a matrix is not yet implemented. Also, this code is space inefficient.This method just tries to find as good a solution as possible. If you want to check whether a solution exists or if it is accurate, just call this function to get a result and then compute the error of this result, or use MatrixBase::isApprox() directly, for instance like this:
\begin{verbatim}
bool a\_solution\_exists = (A*result).isApprox(b, precision); 
\end{verbatim}
This method avoids dividing by zero, so that the non-existence of a solution doesn't by itself mean that you'll get inf or nan values.
If there exists more than one solution, this method will arbitrarily choose one.


Example:
\lstset{language={C++}}
\begin{lstlisting}
Matrix3f m = Matrix3f::Random();
Matrix3f y = Matrix3f::Random();
cout << "Here is the matrix m:" << endl << m << endl;
cout << "Here is the matrix y:" << endl << y << endl;Matrix3f x;
x = m.colPivHouseholderQr().solve(y);
assert(y.isApprox(m*x));
cout << "Here is a solution x to the equation mx=y:" << endl << x << endl;
\end{lstlisting}

\begin{verbatim}
Output:
Here is the matrix m:
0.68  0.597  -0.33
-0.211  0.823  0.536
0.566 -0.605 -0.444
Here is the matrix y:
0.108   -0.27   0.832
-0.0452  0.0268   0.271
0.258   0.904   0.435
Here is a solution x to the equation mx=y:
0.609   2.68   1.67
-0.231  -1.57 0.0713
0.51   3.51   1.05
\end{verbatim}


RealScalar threshold  ( )  const 
inline  

Returns the threshold that will be used by certain methods such as rank().
See the documentation of setThreshold(const RealScalar\&). 
Referenced by ColPivHouseholderQR< MatrixType >::rank(), and ColPivHouseholderQR< MatrixType >::setThreshold().





\subsection{Linear Solver}


\begin{mpFunctionsExtract}
	\mpFunctionTwo
	{SolveColPivQR? mpNum[]? the solution $x$ of $A x = b$ , based on a QR decomposition with column-pivoting.}
	{A? mpNum[,]? A square real matrix.}
	{B? mpNum[,]? A real vector or matrix.}
\end{mpFunctionsExtract}


\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionTwo
	{cplxSolveColPivQR? mpNum[]? the solution $x$ of $A x = b$ , based on a QR decomposition with column-pivoting.}
	{A? mpNum[,]? A square complex matrix.}
	{B? mpNum[,]? A complex vector or matrix.}
\end{mpFunctionsExtract}





\subsection{Matrix Inversion}

\begin{mpFunctionsExtract}
	\mpFunctionOne
	{InvertColPivQR? mpNum[]? $A^{-1}$, the inverse of $A$, based on a QR decomposition with column-pivoting.}
	{A? mpNum[,]? A square real matrix.}
\end{mpFunctionsExtract}

\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionOne
	{cplxInvertColPivQR? mpNum[]? $A^{-1}$, the inverse of $A$, based on a QR decomposition with column-pivoting.}
	{A? mpNum[,]? A square complex matrix.}
\end{mpFunctionsExtract}




\subsection{Determinant}

\begin{mpFunctionsExtract}
	\mpFunctionOne
	{DetColPivQR? mpNum? $|A|$, the determinant of $A$, based on a QR decomposition with column-pivoting.}
	{A? mpNum[,]? A square real matrix.}
\end{mpFunctionsExtract}

\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionOne
	{cplxDetColPivQR? mpNum? $|A|$, the determinant of $A$, based on a QR decomposition with column-pivoting.}
	{A? mpNum[,]? A square complex matrix.}
\end{mpFunctionsExtract}




\subsection{Example}
Example:
\begin{verbatim}
Example
\end{verbatim}




\newpage
\section{QR Decomposition with full Pivoting}
\label{QR Decomposition with full Pivoting}

\subsection{Decomposition}

\begin{mpFunctionsExtract}
	\mpFunctionThree
	{DecompFullPivQR? mpNumList? the QR decomposition with full pivoting of $AP = QR$.}
	{A? mpNum[,]? the square real matrix of which we are computing the $LU$ decomposition.}
	{B? mpNum[,]? A vector or matrix of real numbers.}
	{Output? String? A string specifying the output options.}
\end{mpFunctionsExtract}

\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionThree
	{cplxDecompFullPivQR? mpNumList? the QR decomposition with full pivoting of $AP = QR$.}
	{A? mpNum[,]? the square complex matrix of which we are computing the $LU$ decomposition.}
	{B? mpNum[,]? A vector or complex of real numbers.}
	{Output? String? A string specifying the output options.}
\end{mpFunctionsExtract}



\vspace{0.3cm}

This class performs a rank-revealing QR decomposition of a matrix $A$ into matrices $P$, $Q$ and $R$ such that 
\begin{equation}
AP=QR
\end{equation} 

by using Householder transformations. Here, $P$ is a permutation matrix, $Q$ a unitary matrix and $R$ an upper triangular matrix.

This decomposition performs a very prudent full pivoting in order to be rank-revealing and achieve optimal numerical stability. The trade-off is that it is slower than \textsf{HouseholderQR} and \textsf{ColPivHouseholderQR}.


Member Function DocumentationMatrixType::RealScalar \textbf{absDeterminant}  ( )  const 

Returns the absolute value of the determinant of the matrix of which *this is the QR decomposition. It has only linear complexity (that is, O(n) where n is the dimension of the square matrix) as the QR decomposition has already been computed.
Note: This is only for square matrices.
Warning: a determinant can be very big or small, so for matrices of large enough dimension, there is a risk of overflow/underflow. One way to work around that is to use logAbsDeterminant() instead.
See Also
logAbsDeterminant(), MatrixBase::determinant() 


\vspace{0.3cm}
const PermutationType\& \textbf{colsPermutation}  ( )  const 

Returns a const reference to the column permutation matrix 


\vspace{0.3cm}
FullPivHouseholderQR< MatrixType > \& \textbf{compute}  ( const MatrixType \&  matrix)   

Performs the QR factorization of the given matrix matrix. The result of the factorization is stored into *this, and a reference to *this is returned.
See Also
class FullPivHouseholderQR, FullPivHouseholderQR(const MatrixType\&) 


\vspace{0.3cm}
Index \textbf{dimensionOfKernel}  ( )  const 

Returns the dimension of the kernel of the matrix of which *this is the QR decomposition.
Note: This method has to determine which pivots should be considered nonzero. For that, it uses the threshold value that you can control by calling setThreshold(const RealScalar\&). References FullPivHouseholderQR< MatrixType >::rank().


\vspace{0.3cm}
const HCoeffsType\& \textbf{hCoeffs}  ( )  const 

Returns a const reference to the vector of Householder coefficients used to represent the factor Q.
For advanced uses only. 


\vspace{0.3cm}
const internal::solve\_retval<FullPivHouseholderQR, typename MatrixType::IdentityReturnType> \textbf{inverse}  ( )  const 

Returns the inverse of the matrix of which *this is the QR decomposition.
Note: If this matrix is not invertible, the returned matrix has undefined coefficients. Use isInvertible() to first determine whether this matrix is invertible. 


\vspace{0.3cm}
bool \textbf{isInjective}  ( )  const 

Returns true if the matrix of which *this is the QR decomposition represents an injective linear map, i.e. has trivial kernel; false otherwise.
Note: This method has to determine which pivots should be considered nonzero. For that, it uses the threshold value that you can control by calling setThreshold(const RealScalar\&). References FullPivHouseholderQR< MatrixType >::rank().


\vspace{0.3cm}
bool \textbf{isInvertible}  ( )  const 

Returns true if the matrix of which *this is the QR decomposition is invertible.
Note: This method has to determine which pivots should be considered nonzero. For that, it uses the threshold value that you can control by calling setThreshold(const RealScalar\&). References FullPivHouseholderQR< MatrixType >::isInjective(), and FullPivHouseholderQR< MatrixType >::isSurjective().


\vspace{0.3cm}
bool \textbf{isSurjective}  ( )  const 

Returns true if the matrix of which *this is the QR decomposition represents a surjective linear map; false otherwise.
Note: This method has to determine which pivots should be considered nonzero. For that, it uses the threshold value that you can control by calling setThreshold(const RealScalar\&). References FullPivHouseholderQR< MatrixType >::rank().
Referenced by FullPivHouseholderQR< MatrixType >::isInvertible().


\vspace{0.3cm}
MatrixType::RealScalar \textbf{logAbsDeterminant}  ( )  const 

Returns the natural log of the absolute value of the determinant of the matrix of which *this is the QR decomposition. It has only linear complexity (that is, O(n) where n is the dimension of the square matrix) as the QR decomposition has already been computed.
Note: This is only for square matrices.
This method is useful to work around the risk of overflow/underflow that's inherent to determinant computation.See Also
absDeterminant(), MatrixBase::determinant() 


\vspace{0.3cm}
FullPivHouseholderQR< MatrixType >::MatrixQReturnType \textbf{matrixQ}  ( void  )  const 

Returns Expression object representing the matrix Q 


\vspace{0.3cm}
const MatrixType\& \textbf{matrixQR}  ( )  const 

Returns a reference to the matrix where the Householder QR decomposition is stored 


\vspace{0.3cm}
RealScalar \textbf{maxPivot}  ( )  const 

Returns the absolute value of the biggest pivot, i.e. the biggest diagonal coefficient of U. 


\vspace{0.3cm}
Index \textbf{nonzeroPivots}  ( )  const 

Returns the number of nonzero pivots in the QR decomposition. Here nonzero is meant in the exact sense, not in a fuzzy sense. So that notion isn't really intrinsically interesting, but it is still useful when implementing algorithms.
See Also rank().


\vspace{0.3cm}
Index \textbf{rank}  ( )  const 

Returns the rank of the matrix of which *this is the QR decomposition.
Note: This method has to determine which pivots should be considered nonzero. For that, it uses the threshold value that you can control by calling setThreshold(const RealScalar\&). References FullPivHouseholderQR< MatrixType >::threshold().


\vspace{0.3cm}
const IntDiagSizeVectorType\& \textbf{rowsTranspositions}  ( )  const 

Returns a const reference to the vector of indices representing the rows transpositions 


\vspace{0.3cm}
FullPivHouseholderQR\& \textbf{setThreshold}  ( const RealScalar \&  threshold)   

Allows to prescribe a threshold to be used by certain methods, such as rank(), who need to determine when pivots are to be considered nonzero. This is not used for the QR decomposition itself.
When it needs to get the threshold value, Eigen calls threshold(). By default, this uses a formula to automatically determine a reasonable threshold. Once you have called the present method setThreshold(const RealScalar\&), your value is used instead.
Parameters
threshold The new value to use as the threshold. 

A pivot will be considered nonzero if its absolute value is strictly greater than  where maxpivot is the biggest pivot.
If you want to come back to the default behavior, call setThreshold(Default\_t) 
References FullPivHouseholderQR< MatrixType >::threshold().


\vspace{0.3cm}
FullPivHouseholderQR\& \textbf{setThreshold}  ( Default\_t  )   

Allows to come back to the default behavior, letting Eigen use its default formula for determining the threshold.
You should pass the special object Eigen::Default as parameter here. 
qr.setThreshold(Eigen::Default); See the documentation of setThreshold(const RealScalar\&). 


\vspace{0.3cm}
const internal::solve\_retval<FullPivHouseholderQR, Rhs> \textbf{solve}  ( const MatrixBase< Rhs > \&  b)  const 

This method finds a solution x to the equation Ax=b, where A is the matrix of which *this is the QR decomposition.
Parameters: b the right-hand-side of the equation to solve. 

Returns the exact or least-square solution if the rank is greater or equal to the number of columns of A, and an arbitrary solution otherwise.
Note: The case where b is a matrix is not yet implemented. Also, this code is space inefficient.This method just tries to find as good a solution as possible. If you want to check whether a solution exists or if it is accurate, just call this function to get a result and then compute the error of this result, or use MatrixBase::isApprox() directly, for instance like this:

\begin{verbatim}
bool a\_solution\_exists = (A*result).isApprox(b, precision); 
\end{verbatim}

This method avoids dividing by zero, so that the non-existence of a solution doesn't by itself mean that you'll get inf or nan values.
If there exists more than one solution, this method will arbitrarily choose one.


Example:
\lstset{language={C++}}
\begin{lstlisting}
Matrix3f m = Matrix3f::Random();
Matrix3f y = Matrix3f::Random();
cout << "Here is the matrix m:" << endl << m << endl;
cout << "Here is the matrix y:" << endl << y << endl;
Matrix3f x;x = m.fullPivHouseholderQr().solve(y);
assert(y.isApprox(m*x));
cout << "Here is a solution x to the equation mx=y:" << endl << x << endl;
\end{lstlisting}

\begin{verbatim}
Output:
Here is the matrix m:
0.68  0.597  -0.33
-0.211  0.823  0.536
0.566 -0.605 -0.444
Here is the matrix y:
0.108   -0.27   0.832
-0.0452  0.0268   0.271
0.258   0.904   0.435
Here is a solution x to the equation mx=y:
0.609   2.68   1.67
-0.231  -1.57 0.0713
0.51   3.51   1.05
\end{verbatim}


\vspace{0.3cm}
RealScalar \textbf{threshold}  ( )  const 

Returns the threshold that will be used by certain methods such as rank().
See the documentation of setThreshold(const RealScalar\&). 
Referenced by FullPivHouseholderQR< MatrixType >::rank(), and FullPivHouseholderQR< MatrixType >::setThreshold().



\subsection{Linear Solver}

\begin{mpFunctionsExtract}
	\mpFunctionTwo
	{SolveFullPivQR? mpNum[]? the solution $x$ of $A x = b$ , based on a QR decomposition with full pivoting.}
	{A? mpNum[,]? A square real matrix.}
	{B? mpNum[,]? A real vector or matrix.}
\end{mpFunctionsExtract}


\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionTwo
	{cplxSolveFullPivQR? mpNum[]? the solution $x$ of $A x = b$ , based on a QR decomposition with full pivoting.}
	{A? mpNum[,]? A square complex matrix.}
	{B? mpNum[,]? A complex vector or matrix.}
\end{mpFunctionsExtract}





\subsection{Matrix Inversion}

\begin{mpFunctionsExtract}
	\mpFunctionOne
	{InvertFullPivQR? mpNum[]? $A^{-1}$, the inverse of $A$, based on a QR decomposition with full pivoting.}
	{A? mpNum[,]? A square real matrix.}
\end{mpFunctionsExtract}

\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionOne
	{cplxInvertFullPivQR? mpNum[]? $A^{-1}$, the inverse of $A$, based on a QR decomposition with full pivoting.}
	{A? mpNum[,]? A square complex matrix.}
\end{mpFunctionsExtract}





\subsection{Determinant}

\begin{mpFunctionsExtract}
	\mpFunctionOne
	{DetFullPivQR? mpNum? $|A|$, the determinant of $A$, based on a QR decomposition with full pivoting.}
	{A? mpNum[,]? A square real matrix.}
\end{mpFunctionsExtract}

\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionOne
	{cplxDetFullPivQR? mpNum? $|A|$, the determinant of $A$, based on a QR decomposition with full pivoting.}
	{A? mpNum[,]? A square complex matrix.}
\end{mpFunctionsExtract}




\subsection{Example}
Example:
\begin{verbatim}
Example
\end{verbatim}





\newpage
\section{Singular Value Decomposition}
\label{Singular Value Decomposition}


Two-sided Jacobi SVD decomposition of a rectangular matrix. 

Parameters

MatrixType the type of the matrix of which we are computing the SVD decomposition  

QRPreconditioner this optional parameter allows to specify the type of QR decomposition that will be used internally for the R-SVD step for non-square matrices. See discussion of possible values below. 

SVD decomposition consists in decomposing any $n$-by-$p$ matrix $A$ as a product 
\begin{equation}
A=USV^*
\end{equation} 
where $U$ is a $n$-by-$n$ unitary, $V$ is a $p$-by-$p$ unitary, and $S$ is a $n$-by-$p$ real positive matrix which is zero outside of its main diagonal; the diagonal entries of $S$ are known as the singular values of $A$ and the columns of $U$ and $V$ are known as the left and right singular vectors of $A$ respectively.
Singular values are always sorted in decreasing order.

This JacobiSVD decomposition computes only the singular values by default. If you want $U$ or $V$, you need to ask for them explicitly.

You can ask for only thin $U$ or $V$ to be computed, meaning the following. In case of a rectangular $n$-by-$p$ matrix, letting $m$ be the smaller value among $n$ and $p$, there are only $m$ singular vectors; the remaining columns of $U$ and $V$ do not correspond to actual singular vectors. Asking for thin $U$ or $V$ means asking for only their $m$ first columns to be formed. So $U$ is then a $n$-by-$m$ matrix, and $V$ is then a $p$-by-$m$ matrix. Notice that thin $U$ and $V$ are all you need for (least squares) solving.

This JacobiSVD class is a two-sided Jacobi R-SVD decomposition, ensuring optimal reliability and accuracy. The downside is that it's slower than bidiagonalizing SVD algorithms for large square matrices; however its complexity is still  where n is the smaller dimension and p is the greater dimension, meaning that it is still of the same order of complexity as the faster bidiagonalizing R-SVD algorithms. In particular, like any R-SVD, it takes advantage of non-squareness in that its complexity is only linear in the greater dimension.

If the input matrix has inf or nan coefficients, the result of the computation is undefined, but the computation is guaranteed to terminate in finite (and reasonable) time.

The possible values for QRPreconditioner are: 
\begin{itemize}
	\item ColPivHouseholderQRPreconditioner is the default. In practice it's very safe. It uses column-pivoting QR. 
	\item FullPivHouseholderQRPreconditioner, is the safest and slowest. It uses full-pivoting QR. Contrary to other QRs, it doesn't allow computing thin unitaries. 
	\item HouseholderQRPreconditioner is the fastest, and less safe and accurate than the pivoting variants. It uses non-pivoting QR. This is very similar in safety and accuracy to the bidiagonalization process used by bidiagonalizing SVD algorithms (since bidiagonalization is inherently non-pivoting). However the resulting SVD is still more reliable than bidiagonalizing SVDs because the Jacobi-based iterarive process is more reliable than the optimized bidiagonal SVD iterations. 
	\item NoQRPreconditioner allows not to use a QR preconditioner at all. This is useful if you know that you will only be computing JacobiSVD decompositions of square matrices. Non-square matrices require a QR preconditioner. Using this option will result in faster compilation and smaller executable code. It won't significantly speed up computation, since JacobiSVD is always checking if QR preconditioning is needed before applying it anyway.
\end{itemize}




\subsection{Decomposition}

\begin{mpFunctionsExtract}
	\mpFunctionFour
	{DecompJacobiSVD? mpNumList? the Cholesky decomposition $A = LL^* = U^*U$ of a matrix.}
	{A? mpNum[,]? the real matrix of which we are computing the $LL^T$ Cholesky decomposition.}
	{B? mpNum[,]? A vector or matrix of real numbers.}
	{computationOptions? Integer? An optional parameter allowing to specify if you want full or thin U or V unitaries to be computed.}
	{Output? String? A string specifying the output options.}
\end{mpFunctionsExtract}

\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionFour
	{cplxDecompJacobiSVD? mpNumList? the Cholesky decomposition $A = LL^* = U^*U$ of a matrix.}
	{A? mpNum[,]? the complex matrix of which we are computing the $LL^T$ Cholesky decomposition.}
	{B? mpNum[,]? A vector or complex of real numbers.}
	{computationOptions? Integer? An optional parameter allowing to specify if you want full or thin U or V unitaries to be computed.}
	{Output? String? A string specifying the output options.}
\end{mpFunctionsExtract}


\vspace{0.3cm}
Member Function DocumentationJacobiSVD< MatrixType, QRPreconditioner > \& \textbf{compute}  ( const MatrixType \&  matrix,  unsigned int  computationOptions  )   

Method performing the decomposition of given matrix using custom options. 
Parameters: matrix the matrix to decompose  

computationOptions: optional parameter allowing to specify if you want full or thin U or V unitaries to be computed. By default, none is computed. This is a bit-field, the possible bits are ComputeFullU, ComputeThinU, ComputeFullV, ComputeThinV. 

Thin unitaries are only available if your matrix type has a Dynamic number of columns (for example MatrixXf). They also are not available with the (non-default) FullPivHouseholderQR preconditioner. 
References JacobiRotation< Scalar >::transpose().

\vspace{0.3cm}
JacobiSVD\& \textbf{compute}  ( const MatrixType \&  matrix)   

Method performing the decomposition of given matrix using current options. 
Parameters: matrix the matrix to decompose 

This method uses the current computationOptions, as already passed to the constructor or to compute(const MatrixType\&, unsigned int). 


\vspace{0.3cm}
bool \textbf{computeU}  ( )  const 

Returns true if U (full or thin) is asked for in this SVD decomposition 


\vspace{0.3cm}
bool \textbf{computeV}  ( )  const 

Returns true if V (full or thin) is asked for in this SVD decomposition 


\vspace{0.3cm}
const MatrixUType\& \textbf{matrixU}  ( )  const 

Returns the U matrix. For the SVD decomposition of a n-by-p matrix, letting m be the minimum of n and p, the U matrix is n-by-n if you asked for ComputeFullU, and is n-by-m if you asked for ComputeThinU.

The m first columns of U are the left singular vectors of the matrix being decomposed.

This method asserts that you asked for U to be computed. 


\vspace{0.3cm}
const MatrixVType\& \textbf{matrixV}  ( )  const 

Returns the V matrix.
For the SVD decomposition of a n-by-p matrix, letting m be the minimum of n and p, the V matrix is p-by-p if you asked for ComputeFullV, and is p-by-m if you asked for ComputeThinV.
The m first columns of V are the right singular vectors of the matrix being decomposed.
This method asserts that you asked for V to be computed. 


\vspace{0.3cm}
Index \textbf{nonzeroSingularValues}  ( )  const 

Returns the number of singular values that are not exactly 0 


\vspace{0.3cm}
const SingularValuesType\& \textbf{singularValues}  ( )  const 

Returns the vector of singular values.
For the SVD decomposition of a n-by-p matrix, letting m be the minimum of n and p, the returned vector has size m. Singular values are always sorted in decreasing order. 


\vspace{0.3cm}
const internal::solve\_retval<JacobiSVD, Rhs> \textbf{solve}  ( const MatrixBase< Rhs > \&  b)  const 

Returns a (least squares) solution of  using the current SVD decomposition of A.
Parameters: b the right-hand-side of the equation to solve. 

Note: Solving requires both U and V to be computed. Thin U and V are enough, there is no need for full U or V.
SVD solving is implicitly least-squares. Thus, this method serves both purposes of exact solving and least-squares solving. In other words, the returned solution is guaranteed to minimize the Euclidean norm $||Ax-b||$. 




\subsection{Linear Solver}

\begin{mpFunctionsExtract}
	\mpFunctionTwo
	{SolveJacobiSVD? mpNum[]? the solution $x$ of $A x = b$ , based on a singular value decomposition.}
	{A? mpNum[,]? A symmetric positive definite real matrix.}
	{B? mpNum[,]? A real vector or matrix.}
\end{mpFunctionsExtract}

\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionTwo
	{cplxSolveJacobiSVD? mpNum[]? the solution $x$ of $A x = b$ , based on a singular value decomposition.}
	{A? mpNum[,]? A symmetric positive definite complex matrix.}
	{B? mpNum[,]? A complex vector or matrix.}
\end{mpFunctionsExtract}






\subsection{Matrix Inversion}

\begin{mpFunctionsExtract}
	\mpFunctionOne
	{InvertJacobiSVD? mpNum[]? $A^{-1}$, the inverse of $A$, based on a singular value decomposition.}
	{A? mpNum[,]? A square real matrix.}
\end{mpFunctionsExtract}

\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionOne
	{cplxInvertJacobiSVD? mpNum[]? $A^{-1}$, the inverse of $A$, based on a singular value decomposition.}
	{A? mpNum[,]? A square complex matrix.}
\end{mpFunctionsExtract}




\subsection{Determinant}

\begin{mpFunctionsExtract}
	\mpFunctionOne
	{DetJacobiSVD? mpNum? $|A|$, the determinant of $A$, based on a singular value decomposition.}
	{A? mpNum[,]? A square real matrix.}
\end{mpFunctionsExtract}

\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionOne
	{cplxDetJacobiSVD? mpNum? $|A|$, the determinant of $A$, based on a singular value decomposition.}
	{A? mpNum[,]? A square complex matrix.}
\end{mpFunctionsExtract}





\subsection{Example}
Example:

Here's an example demonstrating basic usage: 

\lstset{language={C++}}
\begin{lstlisting}
MatrixXf m = MatrixXf::Random(3,2);
cout << "Here is the matrix m:" << endl << m << endl;
JacobiSVD<MatrixXf> svd(m, ComputeThinU | ComputeThinV);
cout << "Its singular values are:" << endl << svd.singularValues() << endl;
cout << "Its left singular vectors are the columns of the thin U matrix:" << endl << svd.matrixU() << endl;
cout << "Its right singular vectors are the columns of the thin V matrix:" << endl << svd.matrixV() << endl;
Vector3f rhs(1, 0, 0);
cout << "Now consider this rhs vector:" << endl << rhs << endl;
cout << "A least-squares solution of m*x = rhs is:" << endl << svd.solve(rhs) << endl;
\end{lstlisting}

\begin{verbatim}
Output:
Here is the matrix m:
0.68  0.597
-0.211  0.823
0.566 -0.605
Its singular values are:
1.19
0.899
Its left singular vectors are the columns of the thin U matrix:
0.388   0.866
0.712 -0.0634
-0.586   0.496
Its right singular vectors are the columns of the thin V matrix:
-0.183  0.983
0.983  0.183
Now consider this rhs vector:
1
0
0
A least-squares solution of m*x = rhs is:
0.888
0.496
\end{verbatim}




\newpage
\section{Householder Transformations}
\label{Householder Transformations}

Reference

Detailed Description

This module provides Householder transformations.

%\vspace{0.3cm}
%class   HouseholderSequence< VectorsType, CoeffsType, Side > 
%Sequence of Householder reflections acting on subspaces with decreasing size. More...

\vspace{0.3cm}  
HouseholderSequence

Convenience function for constructing a Householder sequence of Householder reflections acting on subspaces with decreasing size. 

Returns A HouseholderSequence constructed from the specified arguments. 


\vspace{0.3cm}
HouseholderSequence(OnTheRight)   

Convenience function for constructing a Householder sequence. 

Returns A HouseholderSequence constructed from the specified arguments.

This function differs from householderSequence() in that the template argument OnTheSide of the constructed HouseholderSequence is set to OnTheRight, instead of the default OnTheLeft. 




\subsection{Overview}

This class represents a product sequence of Householder reflections where the first Householder reflection acts on the whole space, the second Householder reflection leaves the one-dimensional subspace spanned by the first unit vector invariant, the third Householder reflection leaves the two-dimensional subspace spanned by the first two unit vectors invariant, and so on up to the last reflection which leaves all but one dimensions invariant and acts only on the last dimension. Such sequences of Householder reflections are used in several algorithms to zero out certain parts of a matrix. 


\vspace{0.3cm}
Indeed, the methods HessenbergDecomposition::matrixQ(), Tridiagonalization::matrixQ(), HouseholderQR::householderQ(), and ColPivHouseholderQR::householderQ() all return a HouseholderSequence.


\vspace{0.3cm}
More precisely, the Householder sequence represents an $n \times n$ matrix $H$ of the form $H = \prod_{i=0}^{n-1} H_i$ where the i-th Householder reflection is $H_i = I - h_i v_i v_i^*$. The i-th Householder coefficient $H_i$ is a scalar and the i-th Householder vector $v_i$ is a vector of the form 
\begin{equation}
v_i = [\underbrace{0,\ldots,0}_{i-1 \text{ zeros}}, 1, \underbrace{*,\ldots,*}_{n-i \text{ arbitrary entries}}].
\end{equation}
The last $n-i$ entries of $v_i$ are called the essential part of the Householder vector.

Typical usages are listed below, where H is a HouseholderSequence: 


\lstset{language={C++}}
\begin{lstlisting}
A.applyOnTheRight(H);             // A = A * H
A.applyOnTheLeft(H);              // A = H * A
A.applyOnTheRight(H.adjoint());   // A = A * H^*
A.applyOnTheLeft(H.adjoint());    // A = H^* * A
MatrixXd Q = H;                   // conversion to a dense matrix
\end{lstlisting}

In addition to the adjoint, you can also apply the inverse (=adjoint), the transpose, and the conjugate operators.


\subsection{Constructor}
Parameters:

[in] v Matrix containing the essential parts of the Householder vectors  

[in] h Vector containing the Householder coefficients 

Constructs the Householder sequence with coefficients given by h and vectors given by v. The i-th Householder coefficient $h_i$ is given by h(i) and the essential part of the i-th Householder vector $v_i$ is given by v(k,i) with k $>$ i (the subdiagonal part of the i-th column). If v has fewer columns than rows, then the Householder sequence contains as many Householder reflections as there are columns.


Example:
\lstset{language={C++}}
\begin{lstlisting}
Matrix3d v = Matrix3d::Random();
cout << "The matrix v is:" << endl;
cout << v << endl;
Vector3d v0(1, v(1,0), v(2,0));
cout << "The first Householder vector is: v\_0 = " << v0.transpose() << endl;
Vector3d v1(0, 1, v(2,1));
cout << "The second Householder vector is: v\_1 = " << v1.transpose()  << endl;
Vector3d v2(0, 0, 1);
cout << "The third Householder vector is: v\_2 = " << v2.transpose() << endl;
Vector3d h = Vector3d::Random();
cout << "The Householder coefficients are: h = " << h.transpose() << endl;
Matrix3d H0 = Matrix3d::Identity() - h(0) * v0 * v0.adjoint();
cout << "The first Householder reflection is represented by H\_0 = " << endl;
cout << H0 << endl;Matrix3d H1 = Matrix3d::Identity() - h(1) * v1 * v1.adjoint();
cout << "The second Householder reflection is represented by H\_1 = " << endl;cout << H1 << endl;
Matrix3d H2 = Matrix3d::Identity() - h(2) * v2 * v2.adjoint();
cout << "The third Householder reflection is represented by H\_2 = " << endl;
cout << H2 << endl;cout << "Their product is H\_0 H\_1 H\_2 = " << endl;
cout << H0 * H1 * H2 << endl;HouseholderSequence<Matrix3d, Vector3d> hhSeq(v, h);
Matrix3d hhSeqAsMatrix(hhSeq);
cout << "If we construct a HouseholderSequence from v and h" << endl;
cout << "and convert it to a matrix, we get:" << endl;cout << hhSeqAsMatrix << endl;
\end{lstlisting}

\begin{verbatim}
Output:
The matrix v is:
0.68  0.597  -0.33
-0.211  0.823  0.536
0.566 -0.605 -0.444
The first Householder vector is: v\_0 =      1 -0.211  0.566
The second Householder vector is: v\_1 =      0      1 -0.605
The third Householder vector is: v\_2 = 0 0 1
The Householder coefficients are: h =   0.108 -0.0452   0.258
The first Householder reflection is represented by H\_0 = 
0.892  0.0228 -0.0611
0.0228   0.995  0.0129
-0.0611  0.0129   0.965
The second Householder reflection is represented by H\_1 = 
1       0       0
0    1.05 -0.0273
0 -0.0273    1.02
The third Householder reflection is represented by H\_2 = 
1     0     0
0     1     0
0     0 0.742
Their product is H\_0 H\_1 H\_2 = 
0.892  0.0255 -0.0466
0.0228    1.04 -0.0105
-0.0611 -0.0129   0.728
If we construct a HouseholderSequence from v and h
and convert it to a matrix, we get:
0.892  0.0255 -0.0466
0.0228    1.04 -0.0105
-0.0611 -0.0129   0.728
\end{verbatim}

\subsection{Member Function Documentation} 

Index \textbf{cols}  ( void  )  const 

Number of columns of transformation viewed as a matrix. 

Returns Number of columns. 
This equals the dimension of the space that the transformation acts on. 


\vspace{0.3cm}
const \textbf{HouseholderSequence}

Returns a reference to the derived object 


\vspace{0.3cm}
const EssentialVectorType \textbf{essentialVector}  ( Index  k)  const 

Essential part of a Householder vector. 

Parameters: [in] k Index of Householder reflection  

Returns: Vector containing non-trivial entries of k-th Householder vector

This function returns the essential part of the Householder vector $v_i$. This is a vector of length $n-i$ containing the last $n-i$ entries of the vector 
\begin{equation}
v_i = [\underbrace{0,\ldots,0}_{i-1 \text{ zeros}}, 1, \underbrace{*,\ldots,*}_{n-i \text{ arbitrary entries}}].
\end{equation}
The index $i$ equals k + shift(), corresponding to the k-th column of the matrix v passed to the constructor.


\vspace{0.3cm}
\textbf{Matrix\_type\_times\_scalar\_type}

Computes the product of a Householder sequence with a matrix. 

Parameters: [in] other Matrix being multiplied.  

Returns Expression object representing the product.
This function computes $HM$ where $H$ is the Householder sequence represented by *this and  $M$ is the matrix other. 


\vspace{0.3cm}
Index \textbf{rows}  ( void  )  const 

Number of rows of transformation viewed as a matrix. 

Returns Number of rows

This equals the dimension of the space that the transformation acts on. 


\vspace{0.3cm}
HouseholderSequence\& \textbf{setLength}  ( Index  length)   

Sets the length of the Householder sequence. 

Parameters: [in] length New value for the length. 

By default, the length $n$ of the Householder sequence $H=H_0 H_1 \ldots H_{n-1}$ is set to the number of columns of the matrix v passed to the constructor, or the number of rows if that is smaller. After this function is called, the length equals length.


\vspace{0.3cm}
HouseholderSequence\& \textbf{setShift}  ( Index  shift)   

Sets the shift of the Householder sequence. 

Parameters: [in] shift New value for the shift. 

By default, a HouseholderSequence object represents  $H=H_0 H_1 \ldots H_{n-1}$  and the i-th column of the matrix v passed to the constructor corresponds to the i-th Householder reflection. After this function is called, the object represents  $H=H_{\text{shift}} H_{\text{shift + 1}} \ldots H_{n-1}$  and the i-th column of v corresponds to the (shift+i)-th Householder reflection.


\vspace{0.3cm}
HouseholderSequence\& \textbf{setTrans}  ( bool  trans)   

Sets the transpose flag. 

Parameters: [in] trans New value of the transpose flag. 

By default, the transpose flag is not set. If the transpose flag is set, then this object represents $H^T = H_{n-1}^T \ldots H_1^T H_0^T$ instead of  $H=H_0 H_1 \ldots H_{n-1}$.


\vspace{0.3cm}
Index \textbf{size}  ( )  const 

Returns the number of coefficients, which is rows()*cols(). 







\chapter{Eigensystems, (based on Eigen)}
\label{LinearAlgebra} % So I can \ref{altrings3} later.
%\lipsum[2-3]

Book reference: \cite{Golub1996}



\section{Symmetric/Hermitian Eigensystems}
\label{Real Symmetric Eigensystem}


A matrix $A$ is selfadjoint if it equals its adjoint. For real matrices, this means that the matrix is symmetric: it equals its transpose. This class computes the eigenvalues and eigenvectors of a selfadjoint matrix. These are the scalars $\lambda$ and vectors $v$  such that $A v=\lambda v$. The eigenvalues of a selfadjoint matrix are always real. If $D$ is a diagonal matrix with the eigenvalues on the diagonal, and $V$ is a matrix with the eigenvectors as its columns, then $A=V D V^{-1}$ (for selfadjoint matrices, the matrix $V$ is always invertible). This is called the eigendecomposition.

The algorithm exploits the fact that the matrix is selfadjoint, making it faster and more accurate than the general purpose eigenvalue algorithms implemented in EigenSolver and ComplexEigenSolver.

Only the lower triangular part of the input matrix is referenced.

Call the function compute() to compute the eigenvalues and eigenvectors of a given matrix. Alternatively, you can use the SelfAdjointEigenSolver(const MatrixType, int) constructor which computes the eigenvalues and eigenvectors at construction time. Once the eigenvalue and eigenvectors are computed, they can be retrieved with the eigenvalues() and eigenvectors() functions.

The documentation for SelfAdjointEigenSolver(const MatrixType, int) contains an example of the typical use of this class.

To solve the generalized eigenvalue problem $A v = \lambda B v$ and the likes, see the class \textsf{GeneralizedSelfAdjointEigenSolver}.


\subsection{Real Symmetric Matrices}

\begin{mpFunctionsExtract}
	\mpFunctionOne
	{EigenSymm? mpNum? the eigenvalues of a real symmetric matrix.}
	{A? mpNum[,]? the real matrix of which we are computing the eigenvalues.}
\end{mpFunctionsExtract}

\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionOne
	{EigenSymmv? mpNum? the eigenvalues and eigenvectors of a real symmetric matrix.}
	{A? mpNum[,]? the real matrix of which we are computing the eigenvalues.}
\end{mpFunctionsExtract}



%
%\begin{tabular}{p{481pt}}
%\toprule
%\noindent \textsf{Function \textbf{EigenSymm}($\boldsymbol{a}\ As\ mpNum$, $\boldsymbol{b}\ As\ mpNum$) As mpNum}\index{Multiprecision Functions!EigenSymm} \\
%\noindent \textsf{Function \textbf{EigenSymmv}($\boldsymbol{a}\ As\ mpNum$, $\boldsymbol{b}\ As\ mpNum$) As mpNum}\index{Multiprecision Functions!EigenSymmv} \\
%\bottomrule
%\end{tabular}

\vspace{0.3cm}
Member Function DocumentationSelfAdjointEigenSolver< MatrixType > \& \textbf{compute}  ( const MatrixType \&  matrix,  int  options = ComputeEigenvectors)   

Computes eigendecomposition of given matrix. 
Parameters: 
[in] matrix Selfadjoint matrix whose eigendecomposition is to be computed. Only the lower triangular part of the matrix is referenced.  
[in] options Can be ComputeEigenvectors (default) or EigenvaluesOnly.  

Returns Reference to *this.

This function computes the eigenvalues of matrix. The eigenvalues() function can be used to retrieve them. If options equals ComputeEigenvectors, then the eigenvectors are also computed and can be retrieved by calling eigenvectors().

This implementation uses a symmetric QR algorithm. The matrix is first reduced to tridiagonal form using the Tridiagonalization class. The tridiagonal matrix is then brought to diagonal form with implicit symmetric QR steps with Wilkinson shift. Details can be found in Section 8.3 of \cite{Golub1996}.

The cost of the computation is about $9n^3$ if the eigenvectors are required and $4n^3/3$ if they are not required.

This method reuses the memory in the SelfAdjointEigenSolver object that was allocated when the object was constructed, if the size of the matrix does not change.


Example:

\lstset{language={C++}}
\begin{lstlisting}
SelfAdjointEigenSolver<MatrixXf> es(4);
MatrixXf X = MatrixXf::Random(4,4);
MatrixXf A = X + X.transpose();
es.compute(A);
cout << "The eigenvalues of A are: " << es.eigenvalues().transpose() << endl;
es.compute(A + MatrixXf::Identity(4,4)); // re-use es to compute eigenvalues of A+I
cout << "The eigenvalues of A+I are: " << es.eigenvalues().transpose() << endl;
\end{lstlisting}

\begin{verbatim}
Output:
The eigenvalues of A are:  -1.58 -0.473   1.32   2.46
The eigenvalues of A+I are: -0.581  0.527   2.32   3.46
\end{verbatim}


See Also
SelfAdjointEigenSolver(const MatrixType\&, int) 
References Eigen::ComputeEigenvectors, Eigen::NoConvergence, and Eigen::Success.
Referenced by SelfAdjointEigenSolver< \_MatrixType >::SelfAdjointEigenSolver().



\vspace{0.3cm}
SelfAdjointEigenSolver< MatrixType > \& \textbf{computeDirect}  ( const MatrixType \&  matrix,   int  options = ComputeEigenvectors)   

Computes eigendecomposition of given matrix using a direct algorithm. 
This is a variant of compute(const MatrixType\&, int options) which directly solves the underlying polynomial equation.
Currently only 3x3 matrices for which the sizes are known at compile time are supported (e.g., Matrix3d).
This method is usually significantly faster than the QR algorithm but it might also be less accurate. It is also worth noting that for 3x3 matrices it involves trigonometric operations which are not necessarily available for all scalar types.
See Also
compute(const MatrixType\&, int options) 


\vspace{0.3cm}
const RealVectorType\& \textbf{eigenvalues}  ( )  const 

Returns the eigenvalues of given matrix. 
Returns A const reference to the column vector containing the eigenvalues.
Precondition: The eigenvalues have been computed before.
The eigenvalues are repeated according to their algebraic multiplicity, so there are as many eigenvalues as rows in the matrix. The eigenvalues are sorted in increasing order.


Example:
\lstset{language={C++}}
\begin{lstlisting}
MatrixXd ones = MatrixXd::Ones(3,3);
SelfAdjointEigenSolver<MatrixXd> es(ones);
cout << "The eigenvalues of the 3x3 matrix of ones are:"      
<< endl << es.eigenvalues() << endl;
\end{lstlisting}

\begin{verbatim}
Output:
The eigenvalues of the 3x3 matrix of ones are:
-3.09e-16
0
3
\end{verbatim}

See Also eigenvectors(), MatrixBase::eigenvalues() 


\vspace{0.3cm}
const MatrixType\& \textbf{eigenvectors}  ( )  const 

Returns the eigenvectors of given matrix. 
Returns A const reference to the matrix whose columns are the eigenvectors.
Precondition: The eigenvectors have been computed before.

Column $k$ of the returned matrix is an eigenvector corresponding to eigenvalue number $k$ as returned by eigenvalues(). The eigenvectors are normalized to have (Euclidean) norm equal to one. If this object was used to solve the eigenproblem for the selfadjoint matrix $A$, then the matrix returned by this function is the matrix $V$ in the eigendecomposition $A=V D V^{-1}$.


Example:
\lstset{language={C++}}
\begin{lstlisting}
MatrixXd ones = MatrixXd::Ones(3,3);
SelfAdjointEigenSolver<MatrixXd> es(ones);
cout << "The first eigenvector of the 3x3 matrix of ones is:"      
<< endl << es.eigenvectors().col(1) << endl;
\end{lstlisting}

\begin{verbatim}
Output:
The first eigenvector of the 3x3 matrix of ones is:
0
-0.707
0.707
\end{verbatim}
See Also eigenvalues() 


\vspace{0.3cm}
ComputationInfo info  ( )  const 

Reports whether previous computation was successful. 
Returns: Success if computation was succesful, NoConvergence otherwise. 


\vspace{0.3cm}
MatrixType \textbf{operatorInverseSqrt}  ( )  const 


\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionOne
	{MatSymmInverseSqrt? mpNum? the inverse matrix square root of a real symmetric matrix.}
	{A? mpNum[,]? the real matrix of which we are computing the eigenvalues.}
\end{mpFunctionsExtract}



Computes the inverse square root of the matrix. 
Returns the inverse positive-definite square root of the matrix
Precondition: The eigenvalues and eigenvectors of a positive-definite matrix have been computed before.
This function uses the eigendecomposition $A=V D V^{-1}$ to compute the inverse square root as $V D^{-1/2} V^{-1}$. This is cheaper than first computing the square root with operatorSqrt() and then its inverse with MatrixBase::inverse().


Example:
\lstset{language={C++}}
\begin{lstlisting}
MatrixXd X = MatrixXd::Random(4,4);
MatrixXd A = X * X.transpose();
cout << "Here is a random positive-definite matrix, A:" << endl << A << endl << endl;
SelfAdjointEigenSolver<MatrixXd> es(A);
cout << "The inverse square root of A is: " << endl;
cout << es.operatorInverseSqrt() << endl;
cout << "We can also compute it with operatorSqrt() and inverse(). That yields: " << endl;
cout << es.operatorSqrt().inverse() << endl;
\end{lstlisting}

\begin{verbatim}
Output:
Here is a random positive-definite matrix, A:
1.41 -0.697 -0.111  0.508
-0.697  0.423 0.0991   -0.4
-0.111 0.0991   1.25  0.902
0.508   -0.4  0.902    1.4

The inverse square root of A is: 
1.88   2.78 -0.546  0.605
2.78   8.61   -2.3   2.74
-0.546   -2.3   1.92  -1.36
0.605   2.74  -1.36   2.18
We can also compute it with operatorSqrt() and inverse(). That yields: 
1.88   2.78 -0.546  0.605
2.78   8.61   -2.3   2.74
-0.546   -2.3   1.92  -1.36
0.605   2.74  -1.36   2.18
\end{verbatim}
See Also operatorSqrt(), MatrixBase::inverse(), MatrixFunctions Module 


\vspace{0.3cm}
MatrixType \textbf{operatorSqrt}  ( )  const 



\begin{mpFunctionsExtract}
	\mpFunctionOne
	{MatSymmSqrt? mpNum? the matrix square root of a real symmetric matrix.}
	{A? mpNum[,]? the real matrix of which we are computing the eigenvalues.}
\end{mpFunctionsExtract}


Computes the positive-definite square root of the matrix. 
Returns the positive-definite square root of the matrix
Precondition: The eigenvalues and eigenvectors of a positive-definite matrix have been computed before.
The square root of a positive-definite matrix $A$ is the positive-definite matrix whose square equals $A$. This function uses the eigendecomposition $A = V D V^{-1}$ to compute the square root as $A^{1/2} = V D^{1/2} V^{-1}$.


Example:
\lstset{language={C++}}
\begin{lstlisting}
MatrixXd X = MatrixXd::Random(4,4);
MatrixXd A = X * X.transpose();
cout << "Here is a random positive-definite matrix, A:" << endl << A << endl << endl;
SelfAdjointEigenSolver<MatrixXd> es(A);
MatrixXd sqrtA = es.operatorSqrt();
cout << "The square root of A is: " << endl << sqrtA << endl;
cout << "If we square this, we get: " << endl << sqrtA*sqrtA << endl;
\end{lstlisting}

\begin{verbatim}
Output:
Here is a random positive-definite matrix, A:
1.41 -0.697 -0.111  0.508
-0.697  0.423 0.0991   -0.4
-0.111 0.0991   1.25  0.902
0.508   -0.4  0.902    1.4

The square root of A is: 
1.09  -0.432 -0.0685     0.2
-0.432   0.379   0.141  -0.269
-0.0685   0.141       1   0.468
0.2  -0.269   0.468    1.04
If we square this, we get: 
1.41 -0.697 -0.111  0.508
-0.697  0.423 0.0991   -0.4
-0.111 0.0991   1.25  0.902
0.508   -0.4  0.902    1.4
\end{verbatim}
See Also: operatorInverseSqrt(), MatrixFunctions Module 

\vspace{0.3cm}
Member Data Documentationconst int \textbf{m\_maxIterations} 

Maximum number of iterations. The algorithm terminates if it does not converge within m\_maxIterations * n iterations, where n denotes the size of the matrix. This value is currently set to 30 (copied from LAPACK). 



%\newpage
%\section{Complex Hermitian Eigensystem}
%\label{Complex Hermitian Eigensystem}



\subsection{Complex Hermitian Matrices}


\begin{mpFunctionsExtract}
	\mpFunctionOne
	{cplxEigenHerm? mpNum? the eigenvalues of a complex hermitian matrix.}
	{A? mpNum[,]? the complex hermitian  matrix of which we are computing the eigenvalues.}
\end{mpFunctionsExtract}

\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionOne
	{cplxEigenHermv? mpNum? the eigenvalues and eigenvectors of a complex hermitian matrix.}
	{A? mpNum[,]? the complex hermitian  matrix of which we are computing the eigenvalues.}
\end{mpFunctionsExtract}


%\begin{tabular}{p{481pt}}
%\toprule
%\noindent \textsf{Function \textbf{EigenHerm}($\boldsymbol{a}\ As\ mpNum$, $\boldsymbol{b}\ As\ mpNum$) As mpNum}\index{Multiprecision Functions!EigenHerm} \\
%\noindent \textsf{Function \textbf{EigenHermv}($\boldsymbol{a}\ As\ mpNum$, $\boldsymbol{b}\ As\ mpNum$) As mpNum}\index{Multiprecision Functions!EigenHermv} \\
%\bottomrule
%\end{tabular}
%
%\vspace{0.3cm}
%\lipsum[1]

\subsubsection{Example}

Example:
\lstset{language={C++}}
\begin{lstlisting}
MatrixXd X = MatrixXd::Random(5,5);
MatrixXd A = X + X.transpose();
cout << "Here is a random symmetric 5x5 matrix, A:" << endl << A << endl << endl;
SelfAdjointEigenSolver<MatrixXd> es(A);
cout << "The eigenvalues of A are:" << endl << es.eigenvalues() << endl;
cout << "The matrix of eigenvectors, V, is:" << endl << es.eigenvectors() << endl << endl;
double lambda = es.eigenvalues()[0];
cout << "Consider the first eigenvalue, lambda = " << lambda << endl;
VectorXd v = es.eigenvectors().col(0);
cout << "If v is the corresponding eigenvector, then lambda * v = " << endl << lambda * v << endl;
cout << "... and A * v = " << endl << A * v << endl << endl;
MatrixXd D = es.eigenvalues().asDiagonal();
MatrixXd V = es.eigenvectors();
cout << "Finally, V * D * V^(-1) = " << endl << V * D * V.inverse() << endl;
\end{lstlisting}

\begin{verbatim}
Output:
Here is a random symmetric 5x5 matrix, A:
1.36 -0.816  0.521   1.43 -0.144
-0.816 -0.659  0.794 -0.173 -0.406
0.521  0.794 -0.541  0.461  0.179
1.43 -0.173  0.461  -1.43  0.822
-0.144 -0.406  0.179  0.822  -1.37

The eigenvalues of A are:
-2.65
-1.77
-0.745
0.227
2.29
The matrix of eigenvectors, V, is:
0.326 -0.0984  -0.347  0.0109   0.874
0.207  -0.642  -0.228  -0.662  -0.232
-0.0495   0.629   0.164   -0.74   0.164
-0.721  -0.397   0.402  -0.115   0.385
0.573  -0.156   0.799  0.0256  0.0858

Consider the first eigenvalue, lambda = -2.65
If v is the corresponding eigenvector, then lambda * v = 
-0.865
-0.55
0.131
1.91
-1.52
... and A * v = 
-0.865
-0.55
0.131
1.91
-1.52

Finally, V * D * V^(-1) = 
1.36 -0.816  0.521   1.43 -0.144
-0.816 -0.659  0.794 -0.173 -0.406
0.521  0.794 -0.541  0.461  0.179
1.43 -0.173  0.461  -1.43  0.822
-0.144 -0.406  0.179  0.822  -1.37
\end{verbatim}



\newpage
\section{General (Nonsymmetric) Eigensystems}
\label{Real General (Nonsymmetric) Eigensystem}

Computes eigenvalues and eigenvectors of general matrices. 

This is defined in the Eigenvalues module.

MatrixType the type of the matrix of which we are computing the eigendecomposition; this is expected to be an instantiation of the Matrix class template. Currently, only real matrices are supported. 

The eigenvalues and eigenvectors of a matrix $A$ are scalars $\lambda$ and vectors $v$ such that $A v = \lambda v$. If $D$ is a diagonal matrix with the eigenvalues on the diagonal, and $V$ is a matrix with the eigenvectors as its columns, then $A V = V D$ . The matrix $V$ is almost always invertible, in which case we have $A = V D V^{-1}$. This is called the eigendecomposition.
The eigenvalues and eigenvectors of a matrix may be complex, even when the matrix is real. However, we can choose real matrices $V$  and $D$  satisfying $A V = V D$, just like the eigendecomposition, if the matrix $D$ is not required to be diagonal, but if it is allowed to have blocks of the form 
\begin{equation} 
\begin{pmatrix}
u & v \\
-v & u \\
\end{pmatrix}
\end{equation} 

(where $u$ and $v$ are real numbers) on the diagonal. These blocks correspond to complex eigenvalue pairs $u \pm iv$. We call this variant of the eigendecomposition the pseudo-eigendecomposition.

Call the function compute() to compute the eigenvalues and eigenvectors of a given matrix. Alternatively, you can use the EigenSolver(const MatrixType, bool) constructor which computes the eigenvalues and eigenvectors at construction time. Once the eigenvalue and eigenvectors are computed, they can be retrieved with the eigenvalues() and eigenvectors() functions. The pseudoEigenvalueMatrix() and pseudoEigenvectors() methods allow the construction of the pseudo-eigendecomposition.

The documentation for EigenSolver(const MatrixType, bool) contains an example of the typical use of this class.

See Also

MatrixBase::eigenvalues(), class ComplexEigenSolver, class SelfAdjointEigenSolver 


\subsection{Real Nonsymmetric Matrices}


\begin{mpFunctionsExtract}
	\mpFunctionOne
	{EigenNonsymm? mpNum[]? the eigenvalues of a real general (non-symmetric) matrix.}
	{A? mpNum[,]? the real general (non-symmetric) matrix of which we are computing the eigenvalues.}
\end{mpFunctionsExtract}

\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionOne
	{EigenNonsymmv? mpNumList[2]? the eigenvalues and eigenvectors of a real general (non-symmetric) matrix.}
	{A? mpNum[,]? the real general (non-symmetric) matrix of which we are computing the eigenvalues.}
\end{mpFunctionsExtract}



\begin{mpFunctionsExtract}
	\mpFunctionOne
	{PseudoEigenNonsymm? mpNum[]? the pseudoeigenvalues of a real general (non-symmetric) matrix.}
	{A? mpNum[,]? the real general (non-symmetric) matrix of which we are computing the pseudoeigenvalues.}
\end{mpFunctionsExtract}

\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionOne
	{PseudoEigenNonsymmv? mpNumList[2]? the pseudoeigenvalues and pseudoeigenvectors of a real general (non-symmetric) matrix.}
	{A? mpNum[,]? the real general (non-symmetric) matrix of which we are computing the pseudoeigenvalues and pseudoeigenvectors.}
\end{mpFunctionsExtract}


%
%\begin{tabular}{p{481pt}}
%\toprule
%\noindent \textsf{Function \textbf{cplxEigenNonsymm}($\boldsymbol{a}\ As\ mpNum$, $\boldsymbol{b}\ As\ mpNum$) As mpNum}\index{Multiprecision Functions!cplxEigenNonsymm} \\
%\bottomrule
%\end{tabular}

\vspace{0.3cm}
Member Function DocumentationEigenSolver< MatrixType > \& \textbf{compute}  ( const MatrixType \&  matrix, bool  computeEigenvectors = true)   

Computes eigendecomposition of given matrix. 
Parameters: 

[in] matrix Square matrix whose eigendecomposition is to be computed.  

[in] computeEigenvectors If true, both the eigenvectors and the eigenvalues are computed; if false, only the eigenvalues are computed.  

Returns: Reference to *this 

This function computes the eigenvalues of the real matrix matrix. The eigenvalues() function can be used to retrieve them. If computeEigenvectors is true, then the eigenvectors are also computed and can be retrieved by calling eigenvectors().

The matrix is first reduced to real Schur form using the RealSchur class. The Schur decomposition is then used to compute the eigenvalues and eigenvectors.
The cost of the computation is dominated by the cost of the Schur decomposition, which is very approximately $25n^3$ (where $n$  is the size of the matrix) if computeEigenvectors is true, and $10n^3$ if computeEigenvectors is false.
This method reuses of the allocated data in the EigenSolver object.


Example:
\lstset{language={C++}}
\begin{lstlisting}
EigenSolver<MatrixXf> es;
MatrixXf A = MatrixXf::Random(4,4);
es.compute(A, /* computeEigenvectors = */ false);
cout << "The eigenvalues of A are: " << es.eigenvalues().transpose() << endl;
es.compute(A + MatrixXf::Identity(4,4), false); // re-use es to compute eigenvalues of A+I
cout << "The eigenvalues of A+I are: " << es.eigenvalues().transpose() << endl;
\end{lstlisting}

\begin{verbatim}
Output:
The eigenvalues of A are:    (0.755,0.528)   (0.755,-0.528)  (-0.323,0.0965) (-0.323,-0.0965)
The eigenvalues of A+I are:    (1.75,0.528)   (1.75,-0.528)  (0.677,0.0965) (0.677,-0.0965)
\end{verbatim}



\vspace{0.3cm}
const EigenvalueType\& \textbf{eigenvalues}  ( )  const 

Returns the eigenvalues of given matrix. 
Returns: A const reference to the column vector containing the eigenvalues.
Precondition: Either the constructor EigenSolver(const MatrixType\&,bool) or the member function compute(const MatrixType\&, bool) has been called before.
The eigenvalues are repeated according to their algebraic multiplicity, so there are as many eigenvalues as rows in the matrix. The eigenvalues are not sorted in any particular order.


Example:
\lstset{language={C++}}
\begin{lstlisting}
MatrixXd ones = MatrixXd::Ones(3,3);
EigenSolver<MatrixXd> es(ones, false);
cout << "The eigenvalues of the 3x3 matrix of ones are:"      << endl << es.eigenvalues() << endl;
\end{lstlisting}

\begin{verbatim}
Output:
The eigenvalues of the 3x3 matrix of ones are:
(-5.31e-17,0)
(3,0)
(0,0)

\end{verbatim}



\vspace{0.3cm}
EigenSolver< MatrixType >::EigenvectorsType \textbf{eigenvectors}  ( )  const 

Returns the eigenvectors of given matrix. 
Returns Matrix whose columns are the (possibly complex) eigenvectors.
Precondition: Either the constructor EigenSolver(const MatrixType\&,bool) or the member function compute(const MatrixType\&, bool) has been called before, and computeEigenvectors was set to true (the default).
Column $k$ of the returned matrix is an eigenvector corresponding to eigenvalue number $k$ as returned by eigenvalues(). The eigenvectors are normalized to have (Euclidean) norm equal to one. The matrix returned by this function is the matrix $V$ in the eigendecomposition $A = V D V^{-1}$, if it exists.


Example:
\lstset{language={C++}}
\begin{lstlisting}
MatrixXd ones = MatrixXd::Ones(3,3);
EigenSolver<MatrixXd> es(ones);
cout << "The first eigenvector of the 3x3 matrix of ones is:"      
<< endl << es.eigenvectors().col(1) << endl;
\end{lstlisting}

\begin{verbatim}		
Output:
The first eigenvector of the 3x3 matrix of ones is:
(0.577,0)
(0.577,0)
(0.577,0)
\end{verbatim}



\vspace{0.3cm}
MatrixType \textbf{pseudoEigenvalueMatrix}  ( )  const 

Returns the block-diagonal matrix in the pseudo-eigendecomposition. 
Returns A block-diagonal matrix.

Precondition: Either the constructor EigenSolver(const MatrixType\&,bool) or the member function compute(const MatrixType\&, bool) has been called before.
The matrix $D$ returned by this function is real and block-diagonal. The blocks on the diagonal are either 1-by-1 or 2-by-2 blocks of the form $\begin{pmatrix}
u & v \\
-v & u \\
\end{pmatrix}
$. These blocks are not sorted in any particular order. The matrix $D$ and the matrix $V$ returned by pseudoEigenvectors() satisfy $A V = V D$.

See Also pseudoEigenvectors() for an example, eigenvalues() 


\vspace{0.3cm}
const MatrixType\& \textbf{pseudoEigenvectors}  ( )  const 

Returns the pseudo-eigenvectors of given matrix. 
Returns Const reference to matrix whose columns are the pseudo-eigenvectors.

Precondition: Either the constructor EigenSolver(const MatrixType\&,bool) or the member function compute(const MatrixType\&, bool) has been called before, and computeEigenvectors was set to true (the default).
The real matrix $V$ returned by this function and the block-diagonal matrix $D$ returned by pseudoEigenvalueMatrix() satisfy $A V = V D$.


Example:
\lstset{language={C++}}
\begin{lstlisting}
MatrixXd A = MatrixXd::Random(6,6);
cout << "Here is a random 6x6 matrix, A:" << endl << A << endl << endl;
EigenSolver<MatrixXd> es(A);
MatrixXd D = es.pseudoEigenvalueMatrix();
MatrixXd V = es.pseudoEigenvectors();
cout << "The pseudo-eigenvalue matrix D is:" << endl << D << endl;
cout << "The pseudo-eigenvector matrix V is:" << endl << V << endl;
cout << "Finally, V * D * V^(-1) = " << endl << V * D * V.inverse() << endl;
\end{lstlisting}

\begin{verbatim}
Output:
Here is a random 6x6 matrix, A:
0.68   -0.33   -0.27  -0.717  -0.687  0.0259
-0.211   0.536  0.0268   0.214  -0.198   0.678
0.566  -0.444   0.904  -0.967   -0.74   0.225
0.597   0.108   0.832  -0.514  -0.782  -0.408
0.823 -0.0452   0.271  -0.726   0.998   0.275
-0.605   0.258   0.435   0.608  -0.563  0.0486

The pseudo-eigenvalue matrix D is:
0.049   1.06      0      0      0      0
-1.06  0.049      0      0      0      0
0      0  0.967      0      0      0
0      0      0  0.353      0      0
0      0      0      0  0.618  0.129
0      0      0      0 -0.129  0.618
The pseudo-eigenvector matrix V is:
-0.571   -0.888   -0.066    -1.13     17.2    -3.54
0.263   -0.204   -0.869     0.21     9.73     10.7
-0.827   -0.352    0.209   0.0871    -9.75    -4.17
-1.15   0.0535  -0.0857   -0.971     9.36    -4.53
-0.485    0.258    0.436    0.337    -9.74    -2.21
0.206    0.353   -0.426 -0.00873   -0.942     2.98
Finally, V * D * V^(-1) = 
0.68   -0.33   -0.27  -0.717  -0.687  0.0259
-0.211   0.536  0.0268   0.214  -0.198   0.678
0.566  -0.444   0.904  -0.967   -0.74   0.225
0.597   0.108   0.832  -0.514  -0.782  -0.408
0.823 -0.0452   0.271  -0.726   0.998   0.275
-0.605   0.258   0.435   0.608  -0.563  0.0486
\end{verbatim}




\subsubsection{Example}


Example:
\lstset{language={C++}}
\begin{lstlisting}
MatrixXd A = MatrixXd::Random(6,6);
cout << "Here is a random 6x6 matrix, A:" << endl << A << endl << endl;
EigenSolver<MatrixXd> es(A);
cout << "The eigenvalues of A are:" << endl << es.eigenvalues() << endl;
cout << "The matrix of eigenvectors, V, is:" << endl << es.eigenvectors() << endl << endl;
complex<double> lambda = es.eigenvalues()[0];
cout << "Consider the first eigenvalue, lambda = " << lambda << endl;
VectorXcd v = es.eigenvectors().col(0);
cout << "If v is the corresponding eigenvector, then lambda * v = " << endl << lambda * v << endl;
cout << "... and A * v = " << endl << A.cast<complex<double> >() * v << endl << endl;
MatrixXcd D = es.eigenvalues().asDiagonal();MatrixXcd V = es.eigenvectors();
cout << "Finally, V * D * V^(-1) = " << endl << V * D * V.inverse() << endl;
\end{lstlisting}

\begin{verbatim}
Output:
Here is a random 6x6 matrix, A:
0.68   -0.33   -0.27  -0.717  -0.687  0.0259
-0.211   0.536  0.0268   0.214  -0.198   0.678
0.566  -0.444   0.904  -0.967   -0.74   0.225
0.597   0.108   0.832  -0.514  -0.782  -0.408
0.823 -0.0452   0.271  -0.726   0.998   0.275
-0.605   0.258   0.435   0.608  -0.563  0.0486

The eigenvalues of A are:
(0.049,1.06)
(0.049,-1.06)
(0.967,0)
(0.353,0)
(0.618,0.129)
(0.618,-0.129)
The matrix of eigenvectors, V, is:
(-0.292,-0.454)   (-0.292,0.454)      (-0.0607,0)       (-0.733,0)    (0.59,-0.122)     (0.59,0.122)
(0.134,-0.104)    (0.134,0.104)       (-0.799,0)        (0.136,0)    (0.335,0.368)   (0.335,-0.368)
(-0.422,-0.18)    (-0.422,0.18)        (0.192,0)       (0.0563,0)  (-0.335,-0.143)   (-0.335,0.143)
(-0.589,0.0274) (-0.589,-0.0274)      (-0.0788,0)       (-0.627,0)   (0.322,-0.156)    (0.322,0.156)
(-0.248,0.132)  (-0.248,-0.132)        (0.401,0)        (0.218,0)  (-0.335,-0.076)   (-0.335,0.076)
(0.105,0.18)    (0.105,-0.18)       (-0.392,0)     (-0.00564,0)  (-0.0324,0.103) (-0.0324,-0.103)

Consider the first eigenvalue, lambda = (0.049,1.06)
If v is the corresponding eigenvector, then lambda * v = 
(0.466,-0.331)
(0.117,0.137)
(0.17,-0.456)
(-0.0578,-0.622)
(-0.152,-0.256)
(-0.186,0.12)
... and A * v = 
(0.466,-0.331)
(0.117,0.137)
(0.17,-0.456)
(-0.0578,-0.622)
(-0.152,-0.256)
(-0.186,0.12)

Finally, V * D * V^(-1) = 
(0.68,1.9e-16)    (-0.33,4.82e-17)   (-0.27,-2.37e-16)    (-0.717,1.6e-16)   (-0.687,-2.2e-16)   (0.0259,2.72e-16)
(-0.211,2.22e-16)    (0.536,4.16e-17)  (0.0268,-2.98e-16)           (0.214,0)   (-0.198,6.66e-16)    (0.678,6.66e-16)
(0.566,1.22e-15)   (-0.444,1.11e-16)   (0.904,-4.61e-16)  (-0.967,-3.61e-16)    (-0.74,7.22e-16)    (0.225,8.88e-16)
(0.597,1.6e-15)    (0.108,1.84e-16)    (0.832,-5.6e-16)  (-0.514,-4.44e-16)   (-0.782,1.28e-15)   (-0.408,9.44e-16)
(0.823,-8.33e-16) (-0.0452,-2.71e-16)    (0.271,5.53e-16)   (-0.726,7.77e-16)   (0.998,-2.33e-15)   (0.275,-1.67e-15)
(-0.605,1.03e-15)    (0.258,1.91e-16)    (0.435,-4.6e-16)   (0.608,-6.38e-16)   (-0.563,1.69e-15)   (0.0486,1.25e-15)
\end{verbatim}





%
%\newpage
%\section{Complex General (Nonsymmetric) Eigensystem}
%\label{Complex General (Nonsymmetric) Eigensystem}

Computes eigenvalues and eigenvectors of general complex matrices. 

This is defined in the Eigenvalues module.

MatrixType the type of the matrix of which we are computing the eigendecomposition; this is expected to be an instantiation of the Matrix class template. 

The eigenvalues and eigenvectors of a matrix $A$ are scalars $\lambda$ and vectors $v$ such that $A v = \lambda v$. If $D$ is a diagonal matrix with the eigenvalues on the diagonal, and $V$ is a matrix with the eigenvectors as its columns, then $A V = V D$. The matrix $V$ is almost always invertible, in which case we have $A=V D V^{-1}$. This is called the eigendecomposition.
The main function in this class is compute(), which computes the eigenvalues and eigenvectors of a given function. The documentation for that function contains an example showing the main features of the class.

See Also

class EigenSolver, class SelfAdjointEigenSolver 


\subsection{Complex Nonsymmetric Matrices}



\begin{mpFunctionsExtract}
	\mpFunctionOne
	{cplxEigenNonsymm? mpNum[]? the eigenvalues of a complex general (non-symmetric) matrix.}
	{A? mpNum[,]? the complex general (non-symmetric) matrix of which we are computing the eigenvalues.}
\end{mpFunctionsExtract}

\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionOne
	{cplxEigenNonsymmv? mpNumList[2]? the eigenvalues and eigenvectors of a complex general (non-symmetric) matrix.}
	{A? mpNum[,]? the complex general (non-symmetric) matrix of which we are computing the eigenvalues.}
\end{mpFunctionsExtract}

%
%\begin{tabular}{p{481pt}}
%\toprule
%\noindent \textsf{Function \textbf{cplxEigenNonsymmv}($\boldsymbol{a}\ As\ mpNum$, $\boldsymbol{b}\ As\ mpNum$) As mpNum}\index{Multiprecision Functions!cplxEigenNonsymmv} \\
%\bottomrule
%\end{tabular}

\vspace{0.3cm}
Member Function DocumentationComplexEigenSolver< MatrixType > \& compute  ( const MatrixType \&  matrix, bool  computeEigenvectors = true)   

Computes eigendecomposition of given matrix. 
Parameters: [in] matrix Square matrix whose eigendecomposition is to be computed.  
[in] computeEigenvectors If true, both the eigenvectors and the eigenvalues are computed; if false, only the eigenvalues are computed.  

Returns: Reference to *this 

This function computes the eigenvalues of the complex matrix matrix. The eigenvalues() function can be used to retrieve them. If computeEigenvectors is true, then the eigenvectors are also computed and can be retrieved by calling eigenvectors().
The matrix is first reduced to Schur form using the ComplexSchur class. The Schur decomposition is then used to compute the eigenvalues and eigenvectors.
The cost of the computation is dominated by the cost of the Schur decomposition, which is $O(n^3)$ where $n$ is the size of the matrix.


Example:
\lstset{language={C++}}
\begin{lstlisting}
MatrixXcf A = MatrixXcf::Random(4,4);
cout << "Here is a random 4x4 matrix, A:" << endl << A << endl << endl;
ComplexEigenSolver<MatrixXcf> ces;
ces.compute(A);
cout << "The eigenvalues of A are:" << endl << ces.eigenvalues() << endl;
cout << "The matrix of eigenvectors, V, is:" << endl << ces.eigenvectors() << endl << endl;
complex<float> lambda = ces.eigenvalues()[0];
cout << "Consider the first eigenvalue, lambda = " << lambda << endl;
VectorXcf v = ces.eigenvectors().col(0);
cout << "If v is the corresponding eigenvector, then lambda * v = " << endl << lambda * v << endl;
cout << "... and A * v = " << endl << A * v << endl << endl;
cout << "Finally, V * D * V^(-1) = " 
<< endl     
<< ces.eigenvectors() * ces.eigenvalues().asDiagonal() * ces.eigenvectors().inverse() 
<< endl;
\end{lstlisting}

\begin{verbatim}
Output:
Here is a random 4x4 matrix, A:
(-0.211,0.68)  (0.108,-0.444)   (0.435,0.271) (-0.198,-0.687)
(0.597,0.566) (0.258,-0.0452)  (0.214,-0.717)  (-0.782,-0.74)
(-0.605,0.823)  (0.0268,-0.27) (-0.514,-0.967)  (-0.563,0.998)
(0.536,-0.33)   (0.832,0.904)  (0.608,-0.726)  (0.678,0.0259)

The eigenvalues of A are:
(0.137,0.505)
(-0.758,1.22)
(1.52,-0.402)
(-0.691,-1.63)
The matrix of eigenvectors, V, is:
(-0.246,-0.106)     (0.418,0.263)   (0.0417,-0.296)    (-0.122,0.271)
(-0.205,-0.629)    (0.466,-0.457)    (0.244,-0.456)      (0.247,0.23)
(-0.432,-0.0359) (-0.0651,-0.0146)    (-0.191,0.334)   (0.859,-0.0877)
(-0.301,0.46)    (-0.41,-0.397)     (0.623,0.328)    (-0.116,0.195)

Consider the first eigenvalue, lambda = (0.137,0.505)
If v is the corresponding eigenvector, then lambda * v = 
(0.0197,-0.139)
(0.29,-0.19)
(-0.0412,-0.223)
(-0.274,-0.0891)
... and A * v = 
(0.0197,-0.139)
(0.29,-0.19)
(-0.0412,-0.223)
(-0.274,-0.0891)

Finally, V * D * V^(-1) = 
(-0.211,0.68)  (0.108,-0.444)   (0.435,0.271) (-0.198,-0.687)
(0.597,0.566) (0.258,-0.0452)  (0.214,-0.717)  (-0.782,-0.74)
(-0.605,0.823)  (0.0268,-0.27) (-0.514,-0.967)  (-0.563,0.998)
(0.536,-0.33)   (0.832,0.904)  (0.608,-0.726)  (0.678,0.0259)
\end{verbatim}	

References Eigen::Success.
Referenced by ComplexEigenSolver< \_MatrixType >::ComplexEigenSolver().



\vspace{0.3cm}
const EigenvalueType\& \textbf{eigenvalues}  ( )  const 

Returns the eigenvalues of given matrix. 
Returns A const reference to the column vector containing the eigenvalues.

Precondition: Either the constructor ComplexEigenSolver(const MatrixType\& matrix, bool) or the member function compute(const MatrixType\& matrix, bool) has been called before to compute the eigendecomposition of a matrix.
This function returns a column vector containing the eigenvalues. Eigenvalues are repeated according to their algebraic multiplicity, so there are as many eigenvalues as rows in the matrix. The eigenvalues are not sorted in any particular order.


Example:
\lstset{language={C++}}
\begin{lstlisting}
MatrixXcf ones = MatrixXcf::Ones(3,3);
ComplexEigenSolver<MatrixXcf> ces(ones, /* computeEigenvectors = */ false);
cout << "The eigenvalues of the 3x3 matrix of ones are:"
<< endl << ces.eigenvalues() << endl;
\end{lstlisting}		

\begin{verbatim}		
Output:
The eigenvalues of the 3x3 matrix of ones are:
(0,-0)
(0,0)
(3,0)
\end{verbatim}



\vspace{0.3cm}
const EigenvectorType\& \textbf{eigenvectors}  ( )  const 

Returns the eigenvectors of given matrix. 
Returns A const reference to the matrix whose columns are the eigenvectors.

Precondition: Either the constructor ComplexEigenSolver(const MatrixType\& matrix, bool) or the member function compute(const MatrixType\& matrix, bool) has been called before to compute the eigendecomposition of a matrix, and computeEigenvectors was set to true (the default).
This function returns a matrix whose columns are the eigenvectors. Column $k$ is an eigenvector corresponding to eigenvalue number $k$ as returned by eigenvalues(). The eigenvectors are normalized to have (Euclidean) norm equal to one. The matrix returned by this function is the matrix $V$ in the eigendecomposition $A=VD V^{-1}$, if it exists.


Example:
\lstset{language={C++}}
\begin{lstlisting}
MatrixXcf ones = MatrixXcf::Ones(3,3);
ComplexEigenSolver<MatrixXcf> ces(ones);
cout << "The first eigenvector of the 3x3 matrix of ones is:"
<< endl << ces.eigenvectors().col(1) << endl;
\end{lstlisting}

\begin{verbatim}		
Output:
The first eigenvector of the 3x3 matrix of ones is:
(0.154,0)
(-0.772,0)
(0.617,0)
\end{verbatim}



\vspace{0.3cm}
ComputationInfo \textbf{info}  ( )  const 

Reports whether previous computation was successful. 
Returns : Success if computation was succesful, NoConvergence otherwise. 
References ComplexSchur< \_MatrixType >::info().






\newpage
\section{Generalized Eigensystems}
\label{Real Generalized Symmetric Eigensystem}

Computes eigenvalues and eigenvectors of the generalized selfadjoint eigen problem. 
This is defined in the Eigenvalues module.

MatrixType the type of the matrix of which we are computing the eigendecomposition; this is expected to be an instantiation of the Matrix class template. 

This class solves the generalized eigenvalue problem $A v = \lambda B v$. In this case, the matrix $A$ should be selfadjoint and the matrix $B$  should be positive definite.

Only the lower triangular part of the input matrix is referenced.

Call the function compute() to compute the eigenvalues and eigenvectors of a given matrix. Alternatively, you can use the GeneralizedSelfAdjointEigenSolver(const MatrixType, const MatrixType, int) constructor which computes the eigenvalues and eigenvectors at construction time. Once the eigenvalue and eigenvectors are computed, they can be retrieved with the eigenvalues() and eigenvectors() functions.

\vspace{0.3cm}
GeneralizedSelfAdjointEigenSolver  ( const MatrixType \&  matA,    const MatrixType \&  matB,    int  options = ComputeEigenvectors|Ax\_lBx )   

Constructor; computes generalized eigendecomposition of given matrix pencil. 
Parameters:

[in] matA Selfadjoint matrix in matrix pencil. Only the lower triangular part of the matrix is referenced.  

[in] matB Positive-definite matrix in matrix pencil. Only the lower triangular part of the matrix is referenced.  

[in] options A or-ed set of flags {ComputeEigenvectors,EigenvaluesOnly} | {Ax\_lBx,ABx\_lx,BAx\_lx}. Default is ComputeEigenvectors|Ax\_lBx. 

This constructor calls compute(const MatrixType\&, const MatrixType\&, int) to compute the eigenvalues and (if requested) the eigenvectors of the generalized eigenproblem $Ax = \lambda B x$ with matA the selfadjoint matrix $A$ and matB the positive definite matrix $B$. Each eigenvector $x$ satisfies the property $x^8 B x =1$. The eigenvectors are computed if options contains ComputeEigenvectors.
In addition, the two following variants can be solved via options: 

•ABx\_lx: $A B x = \lambda x$

•BAx\_lx: $B A x = \lambda x$.



Example:
\lstset{language={C++}}
\begin{lstlisting}
MatrixXd X = MatrixXd::Random(5,5);
MatrixXd A = X + X.transpose();
cout << "Here is a random symmetric matrix, A:" << endl << A << endl;
X = MatrixXd::Random(5,5);
MatrixXd B = X * X.transpose();
cout << "and a random postive-definite matrix, B:" << endl << B << endl << endl;
GeneralizedSelfAdjointEigenSolver<MatrixXd> es(A,B);
cout << "The eigenvalues of the pencil (A,B) are:" << endl << es.eigenvalues() << endl;
cout << "The matrix of eigenvectors, V, is:" << endl << es.eigenvectors() << endl << endl;
double lambda = es.eigenvalues()[0];
cout << "Consider the first eigenvalue, lambda = " << lambda << endl;
VectorXd v = es.eigenvectors().col(0);
cout << "If v is the corresponding eigenvector, then A * v = " 
<< endl << A * v << endl;
cout << "... and lambda * B * v = " << endl << lambda * B * v << endl << endl;
\end{lstlisting}

\begin{verbatim}
Output:
Here is a random symmetric matrix, A:
1.36 -0.816  0.521   1.43 -0.144
-0.816 -0.659  0.794 -0.173 -0.406
0.521  0.794 -0.541  0.461  0.179
1.43 -0.173  0.461  -1.43  0.822
-0.144 -0.406  0.179  0.822  -1.37
and a random postive-definite matrix, B:
0.132  0.0109 -0.0512  0.0674  -0.143
0.0109    1.68    1.13   -1.12   0.916
-0.0512    1.13     2.3   -2.14    1.86
0.0674   -1.12   -2.14    2.69   -2.01
-0.143   0.916    1.86   -2.01    1.68

The eigenvalues of the pencil (A,B) are:
-227
-3.9
-0.837
0.101
54.2
The matrix of eigenvectors, V, is:
-14.2    1.03 -0.0766  0.0273   -8.36
-0.0546   0.115  -0.729  -0.478   0.374
9.23  -0.624  0.0165  -0.499    3.01
-7.88    -1.3  -0.225  -0.109   -3.85
-20.8  -0.805   0.567  0.0828   -8.73

Consider the first eigenvalue, lambda = -227
If v is the corresponding eigenvector, then A * v = 
-22.8
28.8
-19.8
-21.9
25.9
... and lambda * B * v = 
-22.8
28.8
-19.8
-21.9
25.9
\end{verbatim}



\subsection{Real Generalized Symmetric-Definite Eigensystems}


\begin{mpFunctionsExtract}
	\mpFunctionTwo
	{EigenGensymm? mpNum? the eigenvalues of a real Generalized Symmetric-Definite Eigensystem.}
	{A? mpNum[,]? Selfadjoint matrix in matrix pencil. Only the lower triangular part of the matrix is referenced.}
	{B? mpNum[,]? Positive-definite matrix in matrix pencil. Only the lower triangular part of the matrix is referenced.}
\end{mpFunctionsExtract}

\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionTwo
	{EigenGensymmv? mpNum? the eigenvalues and eigenvectors of a real Generalized Symmetric-Definite Eigensystem.}
	{A? mpNum[,]? Selfadjoint matrix in matrix pencil. Only the lower triangular part of the matrix is referenced.}
	{B? mpNum[,]? Positive-definite matrix in matrix pencil. Only the lower triangular part of the matrix is referenced.}
\end{mpFunctionsExtract}


%
%\begin{tabular}{p{481pt}}
%\toprule
%\noindent \textsf{Function \textbf{EigenGensymm}($\boldsymbol{a}\ As\ mpNum$, $\boldsymbol{b}\ As\ mpNum$) As mpNum}\index{Multiprecision Functions!EigenGensymm} \\
%\noindent \textsf{Function \textbf{EigenGensymmv}($\boldsymbol{a}\ As\ mpNum$, $\boldsymbol{b}\ As\ mpNum$) As mpNum}\index{Multiprecision Functions!EigenGensymmv} \\
%\bottomrule
%\end{tabular}

\vspace{0.3cm}
Member Function Documentation

GeneralizedSelfAdjointEigenSolver< MatrixType > \& compute  ( const MatrixType \&  matA, const MatrixType \&  matB,  int  options = ComputeEigenvectors|Ax\_lBx )   

Computes generalized eigendecomposition of given matrix pencil. 
Parameters
[in] matA Selfadjoint matrix in matrix pencil. Only the lower triangular part of the matrix is referenced.  
[in] matB Positive-definite matrix in matrix pencil. Only the lower triangular part of the matrix is referenced.  
[in] options A or-ed set of flags {ComputeEigenvectors,EigenvaluesOnly} | {Ax\_lBx,ABx\_lx,BAx\_lx}. Default is ComputeEigenvectors|Ax\_lBx. 

Returns
Reference to *this 
According to options, this function computes eigenvalues and (if requested) the eigenvectors of one of the following three generalized eigenproblems:

•Ax\_lBx: $Ax=\lambda Bx$

•ABx\_lx: $A B x = \lambda x$

•BAx\_lx:  $B A x = \lambda x$ 

with matA the selfadjoint matrix $A$ and matB the positive definite matrix $B$. In addition, each eigenvector  satisfies the property $x^* B x=1$.The eigenvalues() function can be used to retrieve the eigenvalues. If options contains ComputeEigenvectors, then the eigenvectors are also computed and can be retrieved by calling eigenvectors().

The implementation uses LLT to compute the Cholesky decomposition $B=L L^*$ and computes the classical eigendecomposition of the selfadjoint matrix $L^{-1} A (L^*)^{-1}$ if options contains Ax\_lBx and of $L^* A L$ otherwise. This solves the generalized eigenproblem, because any solution of the generalized eigenproblem $A x = \lambda B x$ corresponds to a solution $L^{-1} A(L^*)^{-1} (L^* x) = \lambda(L^* x)$ of the eigenproblem for $L^{-1}A(L^*)^{-1}$. Similar statements can be made for the two other variants.



Example:
\lstset{language={C++}}
\begin{lstlisting}
MatrixXd X = MatrixXd::Random(5,5);
MatrixXd A = X * X.transpose();
X = MatrixXd::Random(5,5);
MatrixXd B = X * X.transpose();
GeneralizedSelfAdjointEigenSolver<MatrixXd> es(A,B,EigenvaluesOnly);
cout << "The eigenvalues of the pencil (A,B) are:" << endl << es.eigenvalues() << endl;
es.compute(B,A,false);
cout << "The eigenvalues of the pencil (B,A) are:" << endl << es.eigenvalues() << endl;
\end{lstlisting}

\begin{verbatim}
Output:
The eigenvalues of the pencil (A,B) are:
0.0289
0.299
2.11
8.64
2.08e+03
The eigenvalues of the pencil (B,A) are:
0.000481
0.116
0.473
3.34
34.6
\end{verbatim}


SelfAdjointEigenSolver< MatrixType > \& compute  ( const MatrixType \&  matrix,   int  options = ComputeEigenvectors )   

Computes eigendecomposition of given matrix. 
Parameters
[in] matrix Selfadjoint matrix whose eigendecomposition is to be computed. Only the lower triangular part of the matrix is referenced.  
[in] options Can be ComputeEigenvectors (default) or EigenvaluesOnly.  

Returns Reference to *this 

This function computes the eigenvalues of matrix. The eigenvalues() function can be used to retrieve them. If options equals ComputeEigenvectors, then the eigenvectors are also computed and can be retrieved by calling eigenvectors().

This implementation uses a symmetric QR algorithm. The matrix is first reduced to tridiagonal form using the Tridiagonalization class. The tridiagonal matrix is then brought to diagonal form with implicit symmetric QR steps with Wilkinson shift. Details can be found in Section 8.3 of  \cite{Golub1996}.
The cost of the computation is about $9n^3$ if the eigenvectors are required and $4n^3/3$ if they are not required.

This method reuses the memory in the SelfAdjointEigenSolver object that was allocated when the object was constructed, if the size of the matrix does not change.


Example:
\lstset{language={C++}}
\begin{lstlisting}
SelfAdjointEigenSolver<MatrixXf> es(4);
MatrixXf X = MatrixXf::Random(4,4);
MatrixXf A = X + X.transpose();es.compute(A);
cout << "The eigenvalues of A are: " << es.eigenvalues().transpose() << endl;
es.compute(A + MatrixXf::Identity(4,4)); // re-use es to compute eigenvalues of A+I
cout << "The eigenvalues of A+I are: " << es.eigenvalues().transpose() << endl;
\end{lstlisting}

\begin{verbatim}
Output:
The eigenvalues of A are:  -1.58 -0.473   1.32   2.46
The eigenvalues of A+I are: -0.581  0.527   2.32   3.46
\end{verbatim}



\vspace{0.3cm}
SelfAdjointEigenSolver< MatrixType > \& \textbf{computeDirect}  ( const MatrixType \&  matrix,   int  options = ComputeEigenvectors  )   

Computes eigendecomposition of given matrix using a direct algorithm. 
This is a variant of compute(const MatrixType\&, int options) which directly solves the underlying polynomial equation.
Currently only 3x3 matrices for which the sizes are known at compile time are supported (e.g., Matrix3d).
This method is usually significantly faster than the QR algorithm but it might also be less accurate. It is also worth noting that for 3x3 matrices it involves trigonometric operations which are not necessarily available for all scalar types.
See Also
compute(const MatrixType\&, int options) 


\vspace{0.3cm}
const RealVectorType\& \textbf{eigenvalues}  ( )  const 

Returns the eigenvalues of given matrix. 
Returns A const reference to the column vector containing the eigenvalues.
Precondition
The eigenvalues have been computed before.
The eigenvalues are repeated according to their algebraic multiplicity, so there are as many eigenvalues as rows in the matrix. The eigenvalues are sorted in increasing order.


Example:
\lstset{language={C++}}
\begin{lstlisting}
MatrixXd ones = MatrixXd::Ones(3,3);
SelfAdjointEigenSolver<MatrixXd> es(ones);
cout << "The eigenvalues of the 3x3 matrix of ones are:" 
<< endl << es.eigenvalues() << endl;
\end{lstlisting}

\begin{verbatim}
Output:
The eigenvalues of the 3x3 matrix of ones are:
-3.09e-16
0
3
\end{verbatim}


\vspace{0.3cm}
const MatrixType\& eigenvectors  ( )  const 
inlineinherited  

Returns the eigenvectors of given matrix. 
Returns
A const reference to the matrix whose columns are the eigenvectors.
Precondition
The eigenvectors have been computed before.
Column $k$ of the returned matrix is an eigenvector corresponding to eigenvalue number $k$ as returned by eigenvalues(). The eigenvectors are normalized to have (Euclidean) norm equal to one. If this object was used to solve the eigenproblem for the selfadjoint matrix $A$, then the matrix returned by this function is the matrix $V$ in the eigendecomposition $A=V D V^{-1}$.


Example:
\lstset{language={C++}}
\begin{lstlisting}
MatrixXd ones = MatrixXd::Ones(3,3);
SelfAdjointEigenSolver<MatrixXd> es(ones);
cout << "The first eigenvector of the 3x3 matrix of ones is:"
<< endl << es.eigenvectors().col(1) << endl;
\end{lstlisting}

\begin{verbatim}		
Output:
The first eigenvector of the 3x3 matrix of ones is:
0
-0.707
0.707
\end{verbatim}


\vspace{0.3cm}
ComputationInfo \textbf{info}  ( )  const 

Reports whether previous computation was successful. 
Returns
Success if computation was succesful, NoConvergence otherwise. 



\vspace{0.3cm}
Member Data Documentationconst int m\_maxIterations 
staticinherited  

Maximum number of iterations. 
The algorithm terminates if it does not converge within m\_maxIterations * n iterations, where n denotes the size of the matrix. This value is currently set to 30 (copied from LAPACK). 



%\newpage
%\section{Complex Generalized Hermitian Eigensystem}
%\label{Complex Generalized Hermitian Eigensystem}



\subsection{Complex Hermitian Generalized Symmetric-Definite Eigensystems}


\begin{mpFunctionsExtract}
	\mpFunctionTwo
	{cplxEigenGenherm? mpNum? the eigenvalues of a Complex Hermitian Generalized Symmetric-Definite Eigensystem.}
	{A? mpNum[,]? Selfadjoint matrix in matrix pencil. Only the lower triangular part of the matrix is referenced.}
	{B? mpNum[,]? Positive-definite matrix in matrix pencil. Only the lower triangular part of the matrix is referenced.}
\end{mpFunctionsExtract}

\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionTwo
	{cplxEigenGenhermv? mpNum? the eigenvalues and eigenvectors of a Complex Hermitian Generalized Symmetric-Definite Eigensystem.}
	{A? mpNum[,]? Selfadjoint matrix in matrix pencil. Only the lower triangular part of the matrix is referenced.}
	{B? mpNum[,]? Positive-definite matrix in matrix pencil. Only the lower triangular part of the matrix is referenced.}
\end{mpFunctionsExtract}

%
%\begin{tabular}{p{481pt}}
%\toprule
%\noindent \textsf{Function \textbf{EigenGenherm}($\boldsymbol{a}\ As\ mpNum$, $\boldsymbol{b}\ As\ mpNum$) As mpNum}\index{Multiprecision Functions!EigenGenherm} \\
%\noindent \textsf{Function \textbf{EigenGenhermv}($\boldsymbol{a}\ As\ mpNum$, $\boldsymbol{b}\ As\ mpNum$) As mpNum}\index{Multiprecision Functions!EigenGenhermv} \\
%\bottomrule
%\end{tabular}

\vspace{0.3cm}
GeneralizedSelfAdjointEigenSolver  ( const MatrixType \&  matA,    const MatrixType \&  matB,    int  options = ComputeEigenvectors|Ax\_lBx )   

Constructor; computes generalized eigendecomposition of given matrix pencil. 
Parameters:

[in] matA Selfadjoint matrix in matrix pencil. Only the lower triangular part of the matrix is referenced.  

[in] matB Positive-definite matrix in matrix pencil. Only the lower triangular part of the matrix is referenced.  

[in] options A or-ed set of flags {ComputeEigenvectors,EigenvaluesOnly} | {Ax\_lBx,ABx\_lx,BAx\_lx}. Default is ComputeEigenvectors|Ax\_lBx. 

This constructor calls compute(const MatrixType\&, const MatrixType\&, int) to compute the eigenvalues and (if requested) the eigenvectors of the generalized eigenproblem $Ax = \lambda B x$ with matA the selfadjoint matrix $A$ and matB the positive definite matrix $B$. Each eigenvector $x$ satisfies the property $x^8 B x =1$. The eigenvectors are computed if options contains ComputeEigenvectors.
In addition, the two following variants can be solved via options: 

•ABx\_lx: $A B x = \lambda x$

•BAx\_lx: $B A x = \lambda x$.




\newpage
\subsection{Real Generalized Nonsymmetric Eigensystem}



\begin{mpFunctionsExtract}
	\mpFunctionTwo
	{EigenGenNonsymm? mpNum? the eigenvalues of a real Generalized Non-Symmetric Eigensystem.}
	{A? mpNum[,]? Selfadjoint matrix in matrix pencil. Only the lower triangular part of the matrix is referenced.}
	{B? mpNum[,]? Positive-definite matrix in matrix pencil. Only the lower triangular part of the matrix is referenced.}
\end{mpFunctionsExtract}

\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionTwo
	{EigenGenNonsymmv? mpNum? the eigenvalues and eigenvectors of a real Generalized Non-Symmetric Eigensystem.}
	{A? mpNum[,]? Selfadjoint matrix in matrix pencil. Only the lower triangular part of the matrix is referenced.}
	{B? mpNum[,]? Positive-definite matrix in matrix pencil. Only the lower triangular part of the matrix is referenced.}
\end{mpFunctionsExtract}


%\label{Real Generalized Nonsymmetric Eigensystem}

Computes the generalized eigenvalues and eigenvectors of a pair of general (nonsymmetric) matrices. 

This is defined in the Eigenvalues module.

MatrixType the type of the matrices of which we are computing the eigen-decomposition; this is expected to be an instantiation of the Matrix class template. Currently, only real matrices are supported. 

The generalized eigenvalues and eigenvectors of a matrix pair $A$ and $B$ are scalars $\lambda$ and vectors $v$ such that $A v = \lambda B v$. If $D$ is a diagonal matrix with the eigenvalues on the diagonal, and $V$ is a matrix with the eigenvectors as its columns, then $A V = B V D$. The matrix $V$ is almost always invertible, in which case we have $A = B V D V^{-1}$. This is called the generalized eigen-decomposition.

The generalized eigenvalues and eigenvectors of a matrix pair may be complex, even when the matrices are real. Moreover, the generalized eigenvalue might be infinite if the matrix $B$ is singular. To workaround this difficulty, the eigenvalues are provided as a pair of complex $\alpha$ and real $\beta$ such that: $\lambda_i = \alpha_i / \beta_i$. If $\beta_i$ is (nearly) zero, then one can consider the well defined left eigenvalue $\mu = \beta_i / \alpha_i$  such that: $\mu_i A v_i = B v_i$, or even $\mu_i u_i^TA = u_i^T B$  where $u_i$  is called the left eigenvector.

Call the function compute() to compute the generalized eigenvalues and eigenvectors of a given matrix pair. 

Alternatively, you can use the GeneralizedEigenSolver(const MatrixType, const MatrixType, bool) constructor which computes the eigenvalues and eigenvectors at construction time. Once the eigenvalue and eigenvectors are computed, they can be retrieved with the eigenvalues() and eigenvectors() functions.

\subsubsection{Member Function Documentation}

ComplexVectorType \textbf{alphas}  ( )  const 

Returns A const reference to the vectors containing the alpha values

This vector permits to reconstruct the j-th eigenvalues as alphas(i)/betas(j).


\vspace{0.3cm}
VectorType \textbf{betas}  ( )  const 

Returns A const reference to the vectors containing the beta values
This vector permits to reconstruct the j-th eigenvalues as alphas(i)/betas(j).


\vspace{0.3cm}
GeneralizedEigenSolver< MatrixType > \& \textbf{compute}  ( const MatrixType \&  A,  const MatrixType \&  B,  bool  computeEigenvectors = true )   

Computes generalized eigendecomposition of given matrix. 
Parameters

[in] A Square matrix whose eigendecomposition is to be computed.  

[in] B Square matrix whose eigendecomposition is to be computed.  

[in] computeEigenvectors If true, both the eigenvectors and the eigenvalues are computed; if false, only the eigenvalues are computed.  

Returns Reference to *this 

This function computes the eigenvalues of the real matrix matrix. The eigenvalues() function can be used to retrieve them. If computeEigenvectors is true, then the eigenvectors are also computed and can be retrieved by calling eigenvectors().

The matrix is first reduced to real generalized Schur form using the RealQZ class. The generalized Schur decomposition is then used to compute the eigenvalues and eigenvectors.

The cost of the computation is dominated by the cost of the generalized Schur decomposition.

This method reuses of the allocated data in the GeneralizedEigenSolver object. 



\vspace{0.3cm}
EigenvalueType \textbf{eigenvalues}  ( )  const 

Returns an expression of the computed generalized eigenvalues. 
Returns An expression of the column vector containing the eigenvalues.

It is a shortcut for
this->alphas().cwiseQuotient(this->betas()); Not that betas might contain zeros. It is therefore not recommended to use this function, but rather directly deal with the alphas and betas vectors.

Precondition:

Either the constructor GeneralizedEigenSolver(const MatrixType\&,const MatrixType\&,bool) or the member function compute(const MatrixType\&,const MatrixType\&,bool) has been called before.

The eigenvalues are repeated according to their algebraic multiplicity, so there are as many eigenvalues as rows in the matrix. The eigenvalues are not sorted in any particular order.


\vspace{0.3cm}
GeneralizedEigenSolver\& \textbf{setMaxIterations}  ( Index  maxIters)   

Sets the maximal number of iterations allowed. 




\subsubsection{Example}

Here is an usage example of this class: 



Example:
\lstset{language={C++}}
\begin{lstlisting}
GeneralizedEigenSolver<MatrixXf> ges;
MatrixXf A = MatrixXf::Random(4,4);
MatrixXf B = MatrixXf::Random(4,4);
ges.compute(A, B);
cout << "The (complex) numerators of the generalized eigenvalues are: "
<< ges.alphas().transpose() << endl;
cout << "The (real) denominatore of the generalized eigenvalues are: "
<< ges.betas().transpose() << endl;
cout << "The (complex) generalized eigenvalues are (alphas./beta): "
<< ges.eigenvalues().transpose() << endl;
\end{lstlisting}

\begin{verbatim}
Output:
The (complex) numerators of the generalized eigenvalues are:  
(0.644,0.795) (0.644,-0.795)     (-0.398,0)      (-1.12,0)

The (real) denominatore of the generalized eigenvalues are:
1.51  1.51 -1.25 0.746

The (complex) generalized eigenvalues are (alphas./beta):  
(0.427,0.528) (0.427,-0.528)     (0.318,-0)       (-1.5,0)
\end{verbatim}






\newpage
\section{Decompositions}
\subsection{Tridiagonalization}
\label{Tridiagonalization}

Tridiagonal decomposition of a selfadjoint matrix. 

This is defined in the Eigenvalues module.

MatrixType the type of the matrix of which we are computing the tridiagonal decomposition; this is expected to be an instantiation of the Matrix class template. 

This class performs a tridiagonal decomposition of a selfadjoint matrix $A$ such that: $A = QTQ^{*}$ where $Q$  is unitary and $T$ a real symmetric tridiagonal matrix.
A tridiagonal matrix is a matrix which has nonzero elements only on the main diagonal and the first diagonal below and above it. The Hessenberg decomposition of a selfadjoint matrix is in fact a tridiagonal decomposition. This class is used in SelfAdjointEigenSolver to compute the eigenvalues and eigenvectors of a selfadjoint matrix.

Call the function compute() to compute the tridiagonal decomposition of a given matrix. Alternatively, you can use the Tridiagonalization(const MatrixType) constructor which computes the tridiagonal Schur decomposition at construction time. Once the decomposition is computed, you can use the matrixQ() and matrixT() functions to retrieve the matrices Q and T in the decomposition.

The documentation of Tridiagonalization(const MatrixType) contains an example of the typical use of this class.



Example:
\lstset{language={C++}}
\begin{lstlisting}
MatrixXd X = MatrixXd::Random(5,5);
MatrixXd A = X + X.transpose();
cout << "Here is a random symmetric 5x5 matrix:" << endl << A << endl << endl;
Tridiagonalization<MatrixXd> triOfA(A);
MatrixXd Q = triOfA.matrixQ();
cout << "The orthogonal matrix Q is:" << endl << Q << endl;
MatrixXd T = triOfA.matrixT();
cout << "The tridiagonal matrix T is:" << endl << T << endl << endl;
cout << "Q * T * Q^T = " << endl << Q * T * Q.transpose() << endl;
\end{lstlisting}

\begin{verbatim}
Output:
Here is a random symmetric 5x5 matrix:
1.36 -0.816  0.521   1.43 -0.144
-0.816 -0.659  0.794 -0.173 -0.406
0.521  0.794 -0.541  0.461  0.179
1.43 -0.173  0.461  -1.43  0.822
-0.144 -0.406  0.179  0.822  -1.37

The orthogonal matrix Q is:
1        0        0        0        0
0   -0.471    0.127   -0.671   -0.558
0    0.301   -0.195    0.437   -0.825
0    0.825   0.0459   -0.563 -0.00872
0  -0.0832   -0.971   -0.202   0.0922
The tridiagonal matrix T is:
1.36   1.73      0      0      0
1.73   -1.2 -0.966      0      0
0 -0.966  -1.28  0.214      0
0      0  0.214  -1.69  0.345
0      0      0  0.345  0.164

Q * T * Q^T = 
1.36 -0.816  0.521   1.43 -0.144
-0.816 -0.659  0.794 -0.173 -0.406
0.521  0.794 -0.541  0.461  0.179
1.43 -0.173  0.461  -1.43  0.822
-0.144 -0.406  0.179  0.822  -1.37
\end{verbatim}





\subsubsection{Member Function Documentation}
Member Function Documentation


\vspace{0.3cm}
Tridiagonalization\& \textbf{compute}  ( const MatrixType \&  matrix)   

Computes tridiagonal decomposition of given matrix. 
Parameters: 
[in] matrix Selfadjoint matrix whose tridiagonal decomposition is to be computed.  

Returns Reference to *this 

The tridiagonal decomposition is computed by bringing the columns of the matrix successively in the required form using Householder reflections. The cost is  flops, where  denotes the size of the given matrix.
This method reuses of the allocated data in the Tridiagonalization object, if the size of the matrix does not change.


Example:
\lstset{language={C++}}
\begin{lstlisting}
Tridiagonalization<MatrixXf> tri;
MatrixXf X = MatrixXf::Random(4,4);
MatrixXf A = X + X.transpose();
tri.compute(A);
cout << "The matrix T in the tridiagonal decomposition of A is: " << endl;
cout << tri.matrixT() << endl;
tri.compute(2*A); // re-use tri to compute eigenvalues of 2A
cout << "The matrix T in the tridiagonal decomposition of 2A is: " << endl;
cout << tri.matrixT() << endl;
\end{lstlisting}

\begin{verbatim}
Output:
The matrix T in the tridiagonal decomposition of A is: 
1.36 -0.704      0      0
-0.704 0.0147   1.71      0
0   1.71  0.856  0.641
0      0  0.641 -0.506
The matrix T in the tridiagonal decomposition of 2A is: 
2.72  -1.41      0      0
-1.41 0.0294   3.43      0
0   3.43   1.71   1.28
0      0   1.28  -1.01
\end{verbatim}




\vspace{0.3cm}
Tridiagonalization< MatrixType >::DiagonalReturnType \textbf{diagonal}  ( )  const 

Returns the diagonal of the tridiagonal matrix T in the decomposition. 
Returns
expression representing the diagonal of T
Precondition
Either the constructor Tridiagonalization(const MatrixType\&) or the member function compute(const MatrixType\&) has been called before to compute the tridiagonal decomposition of a matrix.


Example:
\lstset{language={C++}}
\begin{lstlisting}
MatrixXcd X = MatrixXcd::Random(4,4);
MatrixXcd A = X + X.adjoint();
cout << "Here is a random self-adjoint 4x4 matrix:" << endl << A << endl << endl;
Tridiagonalization<MatrixXcd> triOfA(A);
MatrixXd T = triOfA.matrixT();
cout << "The tridiagonal matrix T is:" << endl << T << endl << endl;
cout << "We can also extract the diagonals of T directly ..." << endl;
VectorXd diag = triOfA.diagonal();
cout << "The diagonal is:" << endl << diag << endl; 
VectorXd subdiag = triOfA.subDiagonal();
cout << "The subdiagonal is:" << endl << subdiag << endl;
\end{lstlisting}

\begin{verbatim}
Output:
Here is a random self-adjoint 4x4 matrix:
(-0.422,0)  (0.705,-1.01) (-0.17,-0.552) (0.338,-0.357)
(0.705,1.01)      (0.515,0) (0.241,-0.446)   (0.05,-1.64)
(-0.17,0.552)  (0.241,0.446)      (-1.03,0)  (0.0449,1.72)
(0.338,0.357)    (0.05,1.64) (0.0449,-1.72)       (1.36,0)

The tridiagonal matrix T is:
-0.422  -1.45      0      0
-1.45   1.01  -1.42      0
0  -1.42    1.8   -1.2
0      0   -1.2  -1.96

We can also extract the diagonals of T directly ...
The diagonal is:
-0.422
1.01
1.8
-1.96
The subdiagonal is:
-1.45
-1.42
-1.2
\end{verbatim}


See Also
matrixT(), subDiagonal() 


\vspace{0.3cm}
CoeffVectorType \textbf{householderCoefficients}  ( )  const 


Returns the Householder coefficients. 
Returns a const reference to the vector of Householder coefficients

Precondition: Either the constructor Tridiagonalization(const MatrixType\&) or the member function compute(const MatrixType\&) has been called before to compute the tridiagonal decomposition of a matrix.
The Householder coefficients allow the reconstruction of the matrix $Q$ in the tridiagonal decomposition from the packed data.


Example:
\lstset{language={C++}}
\begin{lstlisting}
Matrix4d X = Matrix4d::Random(4,4);
Matrix4d A = X + X.transpose();
cout << "Here is a random symmetric 4x4 matrix:" << endl << A << endl;
Tridiagonalization<Matrix4d> triOfA(A);
Vector3d hc = triOfA.householderCoefficients();
cout << "The vector of Householder coefficients is:" << endl << hc << endl;
\end{lstlisting}

\begin{verbatim}
Output:
Here is a random symmetric 4x4 matrix:
1.36   0.612   0.122   0.326
0.612   -1.21  -0.222   0.563
0.122  -0.222 -0.0904    1.16
0.326   0.563    1.16    1.66
The vector of Householder coefficients is:
1.87
1.24
0
\end{verbatim}

See Also
packedMatrix(), Householder module 


\vspace{0.3cm}
HouseholderSequenceType \textbf{matrixQ}  ( )  const 

Returns the unitary matrix Q in the decomposition. 

Returns object representing the matrix Q

Precondition:

Either the constructor Tridiagonalization(const MatrixType\&) or the member function compute(const MatrixType\&) has been called before to compute the tridiagonal decomposition of a matrix.

This function returns a light-weight object of template class HouseholderSequence. You can either apply it directly to a matrix or you can convert it to a matrix of type MatrixType.

See Also
Tridiagonalization(const MatrixType\&) for an example, matrixT(), class HouseholderSequence 


\vspace{0.3cm}
MatrixTReturnType \textbf{matrixT}  ( )  const 

Returns an expression of the tridiagonal matrix T in the decomposition. 

Returns expression object representing the matrix T

Precondition:

Either the constructor Tridiagonalization(const MatrixType\&) or the member function compute(const MatrixType\&) has been called before to compute the tridiagonal decomposition of a matrix.

Currently, this function can be used to extract the matrix T from internal data and copy it to a dense matrix object. In most cases, it may be sufficient to directly use the packed matrix or the vector expressions returned by diagonal() and subDiagonal() instead of creating a new dense copy matrix with this function.


\vspace{0.3cm}
const MatrixType\& \textbf{packedMatrix}  ( )  const 

Returns the internal representation of the decomposition. 

Returns a const reference to a matrix with the internal representation of the decomposition.

Precondition:

Either the constructor Tridiagonalization(const MatrixType\&) or the member function compute(const MatrixType\&) has been called before to compute the tridiagonal decomposition of a matrix.

The returned matrix contains the following information:

•the strict upper triangular part is equal to the input matrix A.

•the diagonal and lower sub-diagonal represent the real tridiagonal symmetric matrix T.

•the rest of the lower part contains the Householder vectors that, combined with Householder coefficients returned by householderCoefficients(), allows to reconstruct the matrix $Q$ as $Q = H_{N-1}\ldots H_1 H_0$. Here, the matrices $H_i$ are the Householder transformations $H_i=(I - h_i v_i v_i^T)$ where $h_i$  is the $i$th Householder coefficient and $v_i$ is the Householder vector defined by $v_i=[0,\ldots,0,1,M(i+2,i),\ldots,M(N-1,i)]^T$ with $M$ the matrix returned by this function.See LAPACK for further details on this packed storage.


Example:
\lstset{language={C++}}
\begin{lstlisting}
Matrix4d X = Matrix4d::Random(4,4);
Matrix4d A = X + X.transpose();
cout << "Here is a random symmetric 4x4 matrix:" << endl << A << endl;
Tridiagonalization<Matrix4d> triOfA(A);
Matrix4d pm = triOfA.packedMatrix();
cout << "The packed matrix M is:" << endl << pm << endl;
cout << "The diagonal and subdiagonal corresponds to the matrix T, which is:"
<< endl << triOfA.matrixT() << endl;
\end{lstlisting}

\begin{verbatim}			
Output:
Here is a random symmetric 4x4 matrix:
1.36   0.612   0.122   0.326
0.612   -1.21  -0.222   0.563
0.122  -0.222 -0.0904    1.16
0.326   0.563    1.16    1.66
The packed matrix M is:
1.36  0.612  0.122  0.326
-0.704 0.0147 -0.222  0.563
0.0925   1.71  0.856   1.16
0.248  0.785  0.641 -0.506
The diagonal and subdiagonal corresponds to the matrix T, which is:
1.36 -0.704      0      0
-0.704 0.0147   1.71      0
0   1.71  0.856  0.641
0      0  0.641 -0.506
\end{verbatim}		
See Also householderCoefficients() 


\vspace{0.3cm}
Tridiagonalization< MatrixType >::SubDiagonalReturnType \textbf{subDiagonal}  ( )  const 

Returns the subdiagonal of the tridiagonal matrix T in the decomposition. 

Returns expression representing the subdiagonal of T

Precondition:
Either the constructor Tridiagonalization(const MatrixType\&) or the member function compute(const MatrixType\&) has been called before to compute the tridiagonal decomposition of a matrix.










\newpage
\subsection{Hessenberg Decomposition}
\label{Hessenberg Decomposition}

Reduces a square matrix to Hessenberg form by an orthogonal similarity transformation. 

This is defined in the Eigenvalues module.

MatrixType the type of the matrix of which we are computing the Hessenberg decomposition 

This class performs an Hessenberg decomposition of a matrix $A$. 

In the real case, the Hessenberg decomposition consists of an orthogonal matrix $Q$ and a Hessenberg matrix $H$ such that $A = Q H Q^T$. An orthogonal matrix is a matrix whose inverse equals its transpose $(Q^{-1} = Q^T)$. A Hessenberg matrix has zeros below the subdiagonal, so it is almost upper triangular. 

The Hessenberg decomposition of a complex matrix is $A = Q H Q^*$ with $Q$ unitary (that is, $Q^{-1} = Q^*$).

Call the function compute() to compute the Hessenberg decomposition of a given matrix. Alternatively, you can use the HessenbergDecomposition(const MatrixType) constructor which computes the Hessenberg decomposition at construction time. Once the decomposition is computed, you can use the matrixH() and matrixQ() functions to construct the matrices H and Q in the decomposition.

The documentation for matrixH() contains an example of the typical use of this class.

See Also

class ComplexSchur, class Tridiagonalization, QR Module 

\subsubsection{Member Function Documentation}

HessenbergDecomposition\& compute  ( const MatrixType \&  matrix)   

Computes Hessenberg decomposition of given matrix. 
Parameters
[in] matrix Square matrix whose Hessenberg decomposition is to be computed.  

Returns Reference to *this 
The Hessenberg decomposition is computed by bringing the columns of the matrix successively in the required form using Householder reflections (see, e.g., Algorithm 7.4.2 in  \cite{Golub1996}). The cost is $10n^3/3$ flops, where $n$ denotes the size of the given matrix.
This method reuses of the allocated data in the HessenbergDecomposition object.


Example:
\lstset{language={C++}}
\begin{lstlisting}
MatrixXcf A = MatrixXcf::Random(4,4);
HessenbergDecomposition<MatrixXcf> hd(4);
hd.compute(A);
cout << "The matrix H in the decomposition of A is:" << endl << hd.matrixH() << endl;
hd.compute(2*A); // re-use hd to compute and store decomposition of 2A
cout << "The matrix H in the decomposition of 2A is:" << endl << hd.matrixH() << endl;
\end{lstlisting}

\begin{verbatim}
Output:
The matrix H in the decomposition of A is:
(-0.211,0.68)     (0.346,0.216)  (-0.688,0.00979)    (0.0451,0.584)
(-1.45,0) (-0.0574,-0.0123)    (-0.196,0.385)     (0.395,0.389)
(0,0)          (1.68,0)   (-0.397,-0.552)    (0.156,-0.241)
(0,0)             (0,0)          (1.56,0)    (0.876,-0.423)
The matrix H in the decomposition of 2A is:
(-0.422,1.36)    (0.691,0.431)   (-1.38,0.0196)    (0.0902,1.17)
(-2.91,0) (-0.115,-0.0246)    (-0.392,0.77)    (0.791,0.777)
(0,0)         (3.36,0)    (-0.795,-1.1)   (0.311,-0.482)
(0,0)            (0,0)         (3.12,0)    (1.75,-0.846)
\end{verbatim}


\vspace{0.3cm}					
const CoeffVectorType\& \textbf{householderCoefficients}  ( )  const 

Returns the Householder coefficients. 
Returns
a const reference to the vector of Householder coefficients
Precondition
Either the constructor HessenbergDecomposition(const MatrixType\&) or the member function compute(const MatrixType\&) has been called before to compute the Hessenberg decomposition of a matrix.
The Householder coefficients allow the reconstruction of the matrix  in the Hessenberg decomposition from the packed data.
See Also
packedMatrix(), Householder module 


\vspace{0.3cm}
MatrixHReturnType \textbf{matrixH}  ( )  const 

Constructs the Hessenberg matrix H in the decomposition. 
Returns
expression object representing the matrix H
Precondition
Either the constructor HessenbergDecomposition(const MatrixType\&) or the member function compute(const MatrixType\&) has been called before to compute the Hessenberg decomposition of a matrix.
The object returned by this function constructs the Hessenberg matrix H when it is assigned to a matrix or otherwise evaluated. The matrix H is constructed from the packed matrix as returned by packedMatrix(): The upper part (including the subdiagonal) of the packed matrix contains the matrix H. It may sometimes be better to directly use the packed matrix instead of constructing the matrix H.


Example:
\lstset{language={C++}}
\begin{lstlisting}
Matrix4f A = MatrixXf::Random(4,4);
cout << "Here is a random 4x4 matrix:" << endl << A << endl;
HessenbergDecomposition<MatrixXf> hessOfA(A);
MatrixXf H = hessOfA.matrixH();
cout << "The Hessenberg matrix H is:" << endl << H << endl;
MatrixXf Q = hessOfA.matrixQ();
cout << "The orthogonal matrix Q is:" << endl << Q << endl;
cout << "Q H Q^T is:" << endl << Q * H * Q.transpose() << endl;
\end{lstlisting}

\begin{verbatim}
Output:
Here is a random 4x4 matrix:
0.68   0.823  -0.444   -0.27
-0.211  -0.605   0.108  0.0268
0.566   -0.33 -0.0452   0.904
0.597   0.536   0.258   0.832
The Hessenberg matrix H is:
0.68  -0.691  -0.645   0.235
0.849   0.836  -0.419   0.794
0  -0.469  -0.547 -0.0731
0       0  -0.559  -0.107
The orthogonal matrix Q is:
1       0       0       0
0  -0.249  -0.958   0.144
0   0.667  -0.277  -0.692
0   0.703 -0.0761   0.707
Q H Q^T is:
0.68   0.823  -0.444   -0.27
-0.211  -0.605   0.108  0.0268
0.566   -0.33 -0.0452   0.904
0.597   0.536   0.258   0.832	
\end{verbatim}

See Also
matrixQ(), packedMatrix() 


\vspace{0.3cm}
HouseholderSequenceType \textbf{matrixQ}  ( )  const 

Reconstructs the orthogonal matrix Q in the decomposition. 
Returns
object representing the matrix Q
Precondition
Either the constructor HessenbergDecomposition(const MatrixType\&) or the member function compute(const MatrixType\&) has been called before to compute the Hessenberg decomposition of a matrix.
This function returns a light-weight object of template class HouseholderSequence. You can either apply it directly to a matrix or you can convert it to a matrix of type MatrixType.
See Also
matrixH() for an example, class HouseholderSequence 


\vspace{0.3cm}
const MatrixType\& \textbf{packedMatrix}  ( )  const 

Returns the internal representation of the decomposition. 
Returns a const reference to a matrix with the internal representation of the decomposition.

Precondition
Either the constructor HessenbergDecomposition(const MatrixType\&) or the member function compute(const MatrixType\&) has been called before to compute the Hessenberg decomposition of a matrix.

The returned matrix contains the following information:

•the upper part and lower sub-diagonal represent the Hessenberg matrix H

•the rest of the lower part contains the Householder vectors that, combined with Householder coefficients returned by householderCoefficients(), allows to reconstruct the matrix $Q$ as $Q = H\_{N-1}\ldots H\_1 H\_0$. Here, the matrices $H\_i$ are the Householder transformations $H\_i=(I - h\_i v\_i v\_i^T)$ where $h\_i$  is the $i$th Householder coefficient and $v\_i$ is the Householder vector defined by $v\_i=[0,\ldots,0,1,M(i+2,i),\ldots,M(N-1,i)]^T$ with $M$ the matrix returned by this function.See LAPACK for further details on this packed storage.


Example:
\lstset{language={C++}}
\begin{lstlisting}
Matrix4d A = Matrix4d::Random(4,4);
cout << "Here is a random 4x4 matrix:" << endl << A << endl;
HessenbergDecomposition<Matrix4d> hessOfA(A);
Matrix4d pm = hessOfA.packedMatrix();
cout << "The packed matrix M is:" << endl << pm << endl;
cout << "The upper Hessenberg part corresponds to the matrix H, which is:"
<< endl << hessOfA.matrixH() << endl;
Vector3d hc = hessOfA.householderCoefficients();
cout << "The vector of Householder coefficients is:" << endl << hc << endl;
\end{lstlisting}

\begin{verbatim}
Output:
Here is a random 4x4 matrix:
0.68   0.823  -0.444   -0.27
-0.211  -0.605   0.108  0.0268
0.566   -0.33 -0.0452   0.904
0.597   0.536   0.258   0.832
The packed matrix M is:
0.68  -0.691  -0.645   0.235
0.849   0.836  -0.419   0.794
-0.534  -0.469  -0.547 -0.0731
-0.563   0.344  -0.559  -0.107
The upper Hessenberg part corresponds to the matrix H, which is:
0.68  -0.691  -0.645   0.235
0.849   0.836  -0.419   0.794
0  -0.469  -0.547 -0.0731
0       0  -0.559  -0.107
The vector of Householder coefficients is:
1.25
1.79
0
\end{verbatim}

See Also householderCoefficients() 













\newpage
\subsection{Real QZ Decomposition}
\label{Real QZ Decomposition}

Performs a real QZ decomposition of a pair of square matrices. 

This is defined in the Eigenvalues module.

MatrixType the type of the matrix of which we are computing the real QZ decomposition; this is expected to be an instantiation of the Matrix class template. 

Given a real square matrices $A$ and $B$, this class computes the real QZ decomposition: $ A = Q S Z$, $B = Q T Z$  where $Q$ and $Z$ are real orthogonal matrixes, $T$ is upper-triangular matrix, and $S$ is upper quasi-triangular matrix. An orthogonal matrix is a matrix whose inverse is equal to its transpose, $U^{-1} = U^T$. A quasi-triangular matrix is a block-triangular matrix whose diagonal consists of 1-by-1 blocks and 2-by-2 blocks where further reduction is impossible due to complex eigenvalues.

The eigenvalues of the pencil $A - zB$ can be obtained from 1x1 and 2x2 blocks on the diagonals of $S$ and $T$.

Call the function compute() to compute the real QZ decomposition of a given pair of matrices. Alternatively, you can use the RealQZ(const MatrixType B, const MatrixType B, bool computeQZ) constructor which computes the real QZ decomposition at construction time. Once the decomposition is computed, you can use the matrixS(), matrixT(), matrixQ() and matrixZ() functions to retrieve the matrices S, T, Q and Z in the decomposition. If computeQZ==false, some time is saved by not computing matrices Q and Z.


Example:
\lstset{language={C++}}
\begin{lstlisting}
MatrixXf A = MatrixXf::Random(4,4);
MatrixXf B = MatrixXf::Random(4,4);
RealQZ<MatrixXf> qz(4); // preallocate space for 4x4 matrices
qz.compute(A,B);  // A = Q S Z,  B = Q T Z// print original matrices and result of decomposition
cout << "A:\n" << A << "\n" << "B:\n" << B << "\n";
cout << "S:\n" << qz.matrixS() << "\n" << "T:\n" << qz.matrixT() << "\n";
cout << "Q:\n" << qz.matrixQ() << "\n" << "Z:\n" << qz.matrixZ() << "\n";// verify precision
cout << "\nErrors:"  << "\n|A-QSZ|: " 
<< (A-qz.matrixQ()*qz.matrixS()*qz.matrixZ()).norm()  
<< ", |B-QTZ|: " << (B-qz.matrixQ()*qz.matrixT()*qz.matrixZ()).norm()  
<< "\n|QQ* - I|: " << (qz.matrixQ()*qz.matrixQ().adjoint() - MatrixXf::Identity(4,4)).norm()  
<< ", |ZZ* - I|: " << (qz.matrixZ()*qz.matrixZ().adjoint() - MatrixXf::Identity(4,4)).norm()  << "\n";
\end{lstlisting}

\begin{verbatim}
Output:
A:   
0.68   0.823  -0.444   -0.27 
-0.211  -0.605   0.108  0.0268  
0.566   -0.33 -0.0452   0.904  
0.597   0.536   0.258   0.832
B: 
0.271 -0.967 -0.687  0.998 
0.435 -0.514 -0.198 -0.563
-0.717 -0.726  -0.74 0.0259 
0.214  0.608 -0.782  0.678
S: 
0.927 -0.928   0.643 -0.227
-0.594   0.36  0.146 -0.606
0      0  -0.398 -0.164
0      0      0  -1.12
T:
1.51  0.278 -0.238  0.501
0  -1.04  0.519 -0.239
0      0  -1.25  0.438
0      0      0  0.746
Q:
0.603  0.011  0.552  0.576
-0.142  0.243  0.761 -0.585
0.092 -0.958  0.152 -0.223
0.78  0.149 -0.306 -0.526
Z:
0.284    0.26  -0.696   0.606
-0.918  -0.108   -0.38  0.0406
-0.269   0.783   0.462    0.32
-0.0674  -0.555   0.398   0.727
Errors:
|A-QSZ|: 1.13e-06, |B-QTZ|: 1.81e-06
|QQ* - I|: 1.01e-06, |ZZ* - I|: 7.02e-07
\end{verbatim}


Note
The implementation is based on the algorithm in  \cite{Golub1996}, and \cite{Moler_1973}.


\subsubsection{Member Function Documentation}

RealQZ< MatrixType > \& \textbf{compute}  ( const MatrixType \&  A,    const MatrixType \&  B,    bool  computeQZ = true   )   

Computes QZ decomposition of given matrix. 
Parameters
[in] A Matrix A.  
[in] B Matrix B.  
[in] computeQZ If false, A and Z are not computed.  

Returns Reference to *this 
References Eigen::NoConvergence, and Eigen::Success.
Referenced by RealQZ< MatrixType >::RealQZ().


\vspace{0.3cm}
ComputationInfo \textbf{info}  ( )  const 

Reports whether previous computation was successful. 
Returns Success if computation was succesful, NoConvergence otherwise. 

\vspace{0.3cm}
const MatrixType\& \textbf{matrixQ}  ( )  const 

Returns matrix Q in the QZ decomposition. 
Returns
A const reference to the matrix Q. 


\vspace{0.3cm}
const MatrixType\& \textbf{matrixS}  ( )  const 

Returns matrix S in the QZ decomposition. 
Returns
A const reference to the matrix S. 


\vspace{0.3cm}
const MatrixType\& \textbf{matrixT}  ( )  const 

Returns matrix S in the QZ decomposition. 
Returns
A const reference to the matrix S. 


\vspace{0.3cm}
const MatrixType\& \textbf{matrixZ}  ( )  const 

Returns matrix Z in the QZ decomposition. 
Returns
A const reference to the matrix Z. 

\vspace{0.3cm}
RealQZ\& \textbf{setMaxIterations}  ( Index  maxIters)   

Sets the maximal number of iterations allowed to converge to one eigenvalue or decouple the problem. 
Referenced by GeneralizedEigenSolver< \_MatrixType >::setMaxIterations().











\newpage
\subsection{Real Schur Decomposition}
\label{Real Schur Decomposition}

Performs a real Schur decomposition of a square matrix. 
This is defined in the Eigenvalues module.


MatrixType the type of the matrix of which we are computing the real Schur decomposition; this is expected to be an instantiation of the Matrix class template. 

Given a real square matrix $A$, this class computes the real Schur decomposition: $A = U T U^T$ where $U$ is a real orthogonal matrix and $T$ is a real quasi-triangular matrix. An orthogonal matrix is a matrix whose inverse is equal to its transpose, $U^{-1} = U^T$. A quasi-triangular matrix is a block-triangular matrix whose diagonal consists of 1-by-1 blocks and 2-by-2 blocks with complex eigenvalues. The eigenvalues of the blocks on the diagonal of $T$ are the same as the eigenvalues of the matrix $A$, and thus the real Schur decomposition is used in EigenSolver to compute the eigendecomposition of a matrix.

Call the function compute() to compute the real Schur decomposition of a given matrix. Alternatively, you can use the RealSchur(const MatrixType, bool) constructor which computes the real Schur decomposition at construction time. Once the decomposition is computed, you can use the matrixU() and matrixT() functions to retrieve the matrices U and T in the decomposition.

The documentation of RealSchur(const MatrixType, bool) contains an example of the typical use of this class.

See Also

class ComplexSchur, class EigenSolver, class ComplexEigenSolver 


Example:
\lstset{language={C++}}
\begin{lstlisting}
MatrixXd A = MatrixXd::Random(6,6);
cout << "Here is a random 6x6 matrix, A:" << endl << A << endl << endl;
RealSchur<MatrixXd> schur(A);
cout << "The orthogonal matrix U is:" << endl << schur.matrixU() << endl;
cout << "The quasi-triangular matrix T is:" << endl << schur.matrixT() << endl << endl;
MatrixXd U = schur.matrixU();
MatrixXd T = schur.matrixT();
cout << "U * T * U^T = " << endl << U * T * U.transpose() << endl;
\end{lstlisting}

\begin{verbatim}
Output:
Here is a random 6x6 matrix, A:
0.68   -0.33   -0.27  -0.717  -0.687  0.0259
-0.211   0.536  0.0268   0.214  -0.198   0.678
0.566  -0.444   0.904  -0.967   -0.74   0.225
0.597   0.108   0.832  -0.514  -0.782  -0.408
0.823 -0.0452   0.271  -0.726   0.998   0.275
-0.605   0.258   0.435   0.608  -0.563  0.0486

The orthogonal matrix U is:
0.348  -0.754 0.00435  -0.351  0.0145   0.432
-0.16  -0.266  -0.747   0.457  -0.366  0.0571
0.505  -0.157  0.0746   0.644   0.518  -0.177
0.703   0.324  -0.409  -0.349  -0.187  -0.275
0.296   0.372    0.24   0.324  -0.379   0.684
-0.126   0.305   -0.46  -0.161   0.647   0.485
The quasi-triangular matrix T is:
-0.2   -1.83   0.864   0.271    1.09    0.14
0.647   0.298 -0.0536   0.676  -0.288   0.023
0       0   0.967  -0.201  -0.429   0.847
0       0       0   0.353   0.602   0.694
0       0       0       0   0.572   -1.03
0       0       0       0  0.0184   0.664

U * T * U^T = 
0.68   -0.33   -0.27  -0.717  -0.687  0.0259
-0.211   0.536  0.0268   0.214  -0.198   0.678
0.566  -0.444   0.904  -0.967   -0.74   0.225
0.597   0.108   0.832  -0.514  -0.782  -0.408
0.823 -0.0452   0.271  -0.726   0.998   0.275
-0.605   0.258   0.435   0.608  -0.563  0.0486
\end{verbatim}

\subsubsection{Member Function Documentation}

Member Function DocumentationRealSchur< MatrixType > \& \textbf{compute}  ( const MatrixType \&  matrix,    bool  computeU = true   )   

Computes Schur decomposition of given matrix. 

Parameters

[in] matrix Square matrix whose Schur decomposition is to be computed.  

[in] computeU If true, both T and U are computed; if false, only T is computed.  

Returns Reference to *this 

The Schur decomposition is computed by first reducing the matrix to Hessenberg form using the class HessenbergDecomposition. The Hessenberg matrix is then reduced to triangular form by performing Francis QR iterations with implicit double shift. The cost of computing the Schur decomposition depends on the number of iterations; as a rough guide, it may be taken to be  flops if computeU is true and  flops if computeU is false.


Example:
\lstset{language={C++}}
\begin{lstlisting}
MatrixXf A = MatrixXf::Random(4,4);
RealSchur<MatrixXf> schur(4);
schur.compute(A, /* computeU = */ false);
cout << "The matrix T in the decomposition of A is:" << endl << schur.matrixT() << endl;
schur.compute(A.inverse(), /* computeU = */ false);
cout << "The matrix T in the decomposition of A^(-1) is:" << endl << schur.matrixT() << endl;
\end{lstlisting}

\begin{verbatim}
Output:
The matrix T in the decomposition of A is:
0.523 -0.698  0.148  0.742
0.475  0.986 -0.793  0.721
0      0  -0.28  -0.77
0      0 0.0145 -0.367
The matrix T in the decomposition of A^(-1) is:
-3.06 -4.57 -6.05  5.39
0.168 -2.62 -3.33  3.86
0     0 0.434  0.56
0     0 -1.06  1.35
\end{verbatim}


See Also
compute(const MatrixType\&, bool, Index) 
Referenced by RealSchur< MatrixType >::RealSchur().


\vspace{0.3cm}
RealSchur\& \textbf{computeFromHessenberg}  ( const HessMatrixType \&  matrixH,    const OrthMatrixType \&  matrixQ,    bool  computeU   )   

Computes Schur decomposition of a Hessenberg matrix $H = Z T Z^T$. 
Parameters

[in] matrixH Matrix in Hessenberg form H  

[in] matrixQ orthogonal matrix Q that transform a matrix A to H : $A = Q H Q^T$  

computeU Computes the matriX U of the Schur vectors  

Returns
Reference to *this 
This routine assumes that the matrix is already reduced in Hessenberg form matrixH using either the class HessenbergDecomposition or another mean. It computes the upper quasi-triangular matrix T of the Schur decomposition of H When computeU is true, this routine computes the matrix U such that $A = U T U^T = (QZ) T (QZ)^T = Q H Q^T$ where A is the initial matrix
NOTE Q is referenced if computeU is true; so, if the initial orthogonal matrix is not available, the user should give an identity matrix (Q.setIdentity())
See Also
compute(const MatrixType\&, bool) 


\vspace{0.3cm}
ComputationInfo \textbf{info}  ( )  const 

Reports whether previous computation was successful. 
Returns
Success if computation was succesful, NoConvergence otherwise. 


\vspace{0.3cm}
const MatrixType\& \textbf{matrixT}  ( )  const 

Returns the quasi-triangular matrix in the Schur decomposition. 
Returns
A const reference to the matrix T.
Precondition
Either the constructor RealSchur(const MatrixType\&, bool) or the member function compute(const MatrixType\&, bool) has been called before to compute the Schur decomposition of a matrix.
See Also
RealSchur(const MatrixType\&, bool) for an example 


\vspace{0.3cm}
const MatrixType\& \textbf{matrixU}  ( )  const 

Returns the orthogonal matrix in the Schur decomposition. 
Returns
A const reference to the matrix U.
Precondition
Either the constructor RealSchur(const MatrixType\&, bool) or the member function compute(const MatrixType\&, bool) has been called before to compute the Schur decomposition of a matrix, and computeU was set to true (the default value).
See Also
RealSchur(const MatrixType\&, bool) for an example 


\vspace{0.3cm}
RealSchur\& \textbf{setMaxIterations}  ( Index  maxIters)   

Sets the maximum number of iterations allowed. 
If not specified by the user, the maximum number of iterations is m\_maxIterationsPerRow times the size of the matrix. 
Referenced by EigenSolver< \_MatrixType >::setMaxIterations().


\vspace{0.3cm}
Member Data Documentationconst int \textbf{m\_maxIterationsPerRow} 

Maximum number of iterations per row. 
If not otherwise specified, the maximum number of iterations is this number times the size of the matrix. It is currently set to 40. 











\newpage
\subsection{Complex Schur Decomposition}
\label{Complex Schur Decomposition}

Performs a complex Schur decomposition of a real or complex square matrix. 

This is defined in the Eigenvalues module.

MatrixType the type of the matrix of which we are computing the Schur decomposition; this is expected to be an instantiation of the Matrix class template. 

Given a real or complex square matrix $A$, this class computes the Schur decomposition: $A = U T U^*$ where $U$ is a unitary complex matrix, and $T$ is a complex upper triangular matrix. The diagonal of the matrix $T$ corresponds to the eigenvalues of the matrix $A$.

Call the function compute() to compute the Schur decomposition of a given matrix. Alternatively, you can use the ComplexSchur(const MatrixType, bool) constructor which computes the Schur decomposition at construction time. Once the decomposition is computed, you can use the matrixU() and matrixT() functions to retrieve the matrices $U$ and $V$ in the decomposition.


See Also

class RealSchur, class EigenSolver, class ComplexEigenSolver 


\subsubsection{Member Function Documentation}

ComplexSchur< MatrixType > \& \textbf{compute}  ( const MatrixType \&  matrix,    bool  computeU = true  )   

Computes Schur decomposition of given matrix. 
Parameters
[in] matrix Square matrix whose Schur decomposition is to be computed.  
[in] computeU If true, both T and U are computed; if false, only T is computed. 

Returns Reference to *this 

The Schur decomposition is computed by first reducing the matrix to Hessenberg form using the class HessenbergDecomposition. The Hessenberg matrix is then reduced to triangular form by performing QR iterations with a single shift. The cost of computing the Schur decomposition depends on the number of iterations; as a rough guide, it may be taken on the number of iterations; as a rough guide, it may be taken to be  complex flops, or  complex flops if computeU is false.


Example:
\lstset{language={C++}}
\begin{lstlisting}
MatrixXcf A = MatrixXcf::Random(4,4);
ComplexSchur<MatrixXcf> schur(4);
schur.compute(A);
cout << "The matrix T in the decomposition of A is:" << endl << schur.matrixT() << endl;
schur.compute(A.inverse());
cout << "The matrix T in the decomposition of A^(-1) is:" << endl << schur.matrixT() << endl;
\end{lstlisting}

\begin{verbatim}
Output:
The matrix T in the decomposition of A is:
(-0.691,-1.63)  (0.763,-0.144) (-0.104,-0.836) (-0.462,-0.378)
(0,0)   (-0.758,1.22)  (-0.65,-0.772)  (-0.244,0.113)
(0,0)           (0,0)   (0.137,0.505) (0.0687,-0.404)
(0,0)           (0,0)           (0,0)   (1.52,-0.402)
The matrix T in the decomposition of A^(-1) is:
(0.501,-1.84)    (-1.01,-0.984)       (0.636,1.3)    (-0.676,0.352)
(0,0)   (-0.369,-0.593)     (0.0733,0.18) (-0.0658,-0.0263)
(0,0)             (0,0)    (-0.222,0.521)    (-0.191,0.121)
(0,0)             (0,0)             (0,0)     (0.614,0.162)
\end{verbatim}

See Also
compute(const MatrixType\&, bool, Index) 
References ComplexSchur< \_MatrixType >::computeFromHessenberg(), and Eigen::Success.
Referenced by ComplexSchur< MatrixType >::ComplexSchur().

\vspace{0.3cm}
ComplexSchur\& \textbf{computeFromHessenberg}  ( const HessMatrixType \&  matrixH,    const OrthMatrixType \&  matrixQ,   bool  computeU = true  )   

Compute Schur decomposition from a given Hessenberg matrix. 

Parameters

[in] matrixH Matrix in Hessenberg form H  

[in] matrixQ orthogonal matrix Q that transform a matrix A to H : $A = Q H Q^T$  

computeU Computes the matriX U of the Schur vectors  

Returns Reference to *this 

This routine assumes that the matrix is already reduced in Hessenberg form matrixH using either the class HessenbergDecomposition or another mean. It computes the upper quasi-triangular matrix T of the Schur decomposition of H When computeU is true, this routine computes the matrix U such that $A = U T U^T = (QZ) T (QZ)^T = Q H Q^T$ where A is the initial matrix

NOTE Q is referenced if computeU is true; so, if the initial orthogonal matrix is not available, the user should give an identity matrix (Q.setIdentity())
See Also
compute(const MatrixType\&, bool) 
Referenced by ComplexSchur< \_MatrixType >::compute().


\vspace{0.3cm}
ComputationInfo \textbf{info}  ( )  const 

Reports whether previous computation was successful. 
Returns
Success if computation was succesful, NoConvergence otherwise. 
Referenced by ComplexEigenSolver< \_MatrixType >::info().


\vspace{0.3cm}
const ComplexMatrixType\& \textbf{matrixT}  ( )  const 

Returns the triangular matrix in the Schur decomposition. 

Returns A const reference to the matrix T.

It is assumed that either the constructor ComplexSchur(const MatrixType\& matrix, bool computeU) or the member function compute(const MatrixType\& matrix, bool computeU) has been called before to compute the Schur decomposition of a matrix.

Note that this function returns a plain square matrix. If you want to reference only the upper triangular part, use: 
schur.matrixT().triangularView<Upper>() 


Example:
\lstset{language={C++}}
\begin{lstlisting}
MatrixXcf A = MatrixXcf::Random(4,4);
cout << "Here is a random 4x4 matrix, A:" 
<< endl << A << endl << endl;ComplexSchur<MatrixXcf> schurOfA(A, false); // false means do not compute U
cout << "The triangular matrix T is:" 
<< endl << schurOfA.matrixT() << endl;
\end{lstlisting}

\begin{verbatim}
Output:
Here is a random 4x4 matrix, A:
(-0.211,0.68)  (0.108,-0.444)   (0.435,0.271) (-0.198,-0.687)
(0.597,0.566) (0.258,-0.0452)  (0.214,-0.717)  (-0.782,-0.74)
(-0.605,0.823)  (0.0268,-0.27) (-0.514,-0.967)  (-0.563,0.998)
(0.536,-0.33)   (0.832,0.904)  (0.608,-0.726)  (0.678,0.0259)

The triangular matrix T is:
(-0.691,-1.63)  (0.763,-0.144) (-0.104,-0.836) (-0.462,-0.378)
(0,0)   (-0.758,1.22)  (-0.65,-0.772)  (-0.244,0.113)
(0,0)           (0,0)   (0.137,0.505) (0.0687,-0.404)
(0,0)           (0,0)           (0,0)   (1.52,-0.402)
\end{verbatim}


\vspace{0.3cm}					
const ComplexMatrixType\& \textbf{matrixU}  ( )  const 

Returns the unitary matrix in the Schur decomposition. 
Returns
A const reference to the matrix U.
It is assumed that either the constructor ComplexSchur(const MatrixType\& matrix, bool computeU) or the member function compute(const MatrixType\& matrix, bool computeU) has been called before to compute the Schur decomposition of a matrix, and that computeU was set to true (the default value).


Example:
\lstset{language={C++}}
\begin{lstlisting}
MatrixXcf A = MatrixXcf::Random(4,4);
cout << "Here is a random 4x4 matrix, A:" << endl << A << endl << endl;
ComplexSchur<MatrixXcf> schurOfA(A);
cout << "The unitary matrix U is:" << endl << schurOfA.matrixU() << endl;
\end{lstlisting}

\begin{verbatim}
Output:
Here is a random 4x4 matrix, A:
(-0.211,0.68)  (0.108,-0.444)   (0.435,0.271) (-0.198,-0.687)
(0.597,0.566) (0.258,-0.0452)  (0.214,-0.717)  (-0.782,-0.74)
(-0.605,0.823)  (0.0268,-0.27) (-0.514,-0.967)  (-0.563,0.998)
(0.536,-0.33)   (0.832,0.904)  (0.608,-0.726)  (0.678,0.0259)

The unitary matrix U is:
(-0.122,0.271)   (0.354,0.255)    (-0.7,0.321) (0.0909,-0.346)
(0.247,0.23)  (0.435,-0.395)   (0.184,-0.38)  (0.492,-0.347)
(0.859,-0.0877)  (0.00469,0.21) (-0.256,0.0163)   (0.133,0.355)
(-0.116,0.195) (-0.484,-0.432)  (-0.183,0.359)   (0.559,0.231)
\end{verbatim}


\vspace{0.3cm}
ComplexSchur\& \textbf{setMaxIterations}  ( Index  maxIters)   

Sets the maximum number of iterations allowed. 
If not specified by the user, the maximum number of iterations is m\_maxIterationsPerRow times the size of the matrix. 
Referenced by ComplexEigenSolver< \_MatrixType >::setMaxIterations().


\vspace{0.3cm}
Member Data Documentationconst int m\\textbf{maxIterationsPerRow} 

Maximum number of iterations per row. 
If not otherwise specified, the maximum number of iterations is this number times the size of the matrix. It is currently set to 30. 



\newpage
\section{Matrix Functions}
\label{Matrix Functions}

Matrix functions are defined as follows. Suppose that $f$ is an entire function (that is, a function on the complex plane that is everywhere complex differentiable). Then its Taylor series 
\begin{equation}
f(0) + f'(0)x + \frac{f''(0)}{2}x^2  + \frac{f'''(0)}{3!}x^3 + \cdots
\end{equation}
converges to $f(x)$. In this case, we can define the matrix function by the same series: 
\begin{equation}
f(M) = f(0) + f'(0)M + \frac{f''(0)}{2}M^2  + \frac{f'''(0)}{3!}M^3 + \cdots
\end{equation}




\subsection{Matrix Square Root}


\begin{mpFunctionsExtract}
	\mpFunctionOne
	{MatSqrt? mpNum? an expression representing the matrix square root of the real matrix M.}
	{M? mpNum[,]? the real matrix of which we are computing the matrix square root.}
\end{mpFunctionsExtract}

\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionOne
	{cplxMatSqrt? mpNum? an expression representing the matrix square root of the complex matrix M.}
	{M? mpNum[,]? the complex matrix of which we are computing the matrix square root.}
\end{mpFunctionsExtract}


%
%\begin{tabular}{p{481pt}}
%\toprule
%\noindent \textsf{Function \textbf{MatSqrt}(\textbf{A} As mpNum[,]) As mpNum[,]}\index{Multiprecision Functions!MatSqrt} \\
%\noindent \textsf{Function \textbf{cplxMatSqrt}((\textbf{A} As mpNum[,]) As mpNum[,]}\index{Multiprecision Functions!cplxMatSqrt} \\
%\bottomrule
%\end{tabular}

\vspace{0.3cm}
Compute the matrix square root.

Parameters

[in] M invertible matrix whose square root is to be computed.  

Returns: expression representing the matrix square root of M.

The matrix square root of $M$ is the matrix $M^{1/2}$ whose square is the original matrix; so if $S = M^{1/2}$ then $S^2=M$.

In the real case, the matrix $M$ should be invertible and it should have no eigenvalues which are real and negative (pairs of complex conjugate eigenvalues are allowed). In that case, the matrix has a square root which is also real, and this is the square root computed by this function.

The matrix square root is computed by first reducing the matrix to quasi-triangular form with the real Schur decomposition. The square root of the quasi-triangular matrix can then be computed directly. The cost is approximately $25n^3$ real flops for the real Schur decomposition and $n^3$ real flops for the remainder (though the computation time in practice is likely more than this indicates).

Details of the algorithm can be found in \cite{Higham_1987}.

If the matrix is positive-definite symmetric, then the square root is also positive-definite symmetric. In this case, it is best to use SelfAdjointEigenSolver::operatorSqrt() to compute it.


In the complex case, the matrix $M$ should be invertible; this is a restriction of the algorithm. The square root computed by this algorithm is the one whose eigenvalues have an argument in the interval $\left(-\tfrac{1}{2}\pi, \tfrac{1}{2}\pi\right]$. This is the usual branch cut.

The computation is the same as in the real case, except that the complex Schur decomposition is used to reduce the matrix to a triangular matrix. The theoretical cost is the same. Details are in \cite{Bjoerck_1983}.


Example: The following program checks that the square root of 
\begin{equation} 
\begin{pmatrix}
\cos\left(\frac{1}{3}\pi\right) & -\sin\left(\frac{1}{3}\pi\right) \\
\sin\left(\frac{1}{3}\pi\right) & \cos\left(\frac{1}{3}\pi\right) \\
\end{pmatrix}
\end{equation} 
corresponding to a rotation over 60 degrees, is a rotation over 30 degrees: 
\begin{equation} 
\begin{pmatrix}
\cos\left(\frac{1}{6}\pi\right) & -\sin\left(\frac{1}{6}\pi\right) \\
\sin\left(\frac{1}{6}\pi\right) & \cos\left(\frac{1}{6}\pi\right) \\
\end{pmatrix}
\end{equation} 




\lstset{language={C++}}
\begin{lstlisting}

#include <unsupported/Eigen/MatrixFunctions>
#include <iostream>

using namespace Eigen;

int main()
{
const double pi = std::acos(-1.0);

MatrixXd A(2,2);
A << cos(pi/3), -sin(pi/3),
sin(pi/3),  cos(pi/3);
std::cout << "The matrix A is:\n" << A << "\n\n";
std::cout << "The matrix square root of A is:\n" << A.sqrt() << "\n\n";
std::cout << "The square of the last matrix is:\n" 
<< A.sqrt() * A.sqrt() << "\n";
}
\end{lstlisting}

\begin{verbatim}
Output:
The matrix A is:
0.5 -0.866025
0.866025       0.5

The matrix square root of A is:
0.866025     -0.5
0.5 0.866025

The square of the last matrix is:
0.5 -0.866025
0.866025       0.5

\end{verbatim} 


\newpage
\subsection{Matrix Exponential}


\begin{mpFunctionsExtract}
	\mpFunctionOne
	{MatExp? mpNum? an expression representing the matrix exponential of the real matrix M.}
	{M? mpNum[,]? the real matrix of which we are computing the matrix exponential.}
\end{mpFunctionsExtract}

\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionOne
	{cplxMatExp? mpNum? an expression representing the matrix exponential of the complex matrix M.}
	{M? mpNum[,]? the complex matrix of which we are computing the matrix exponential.}
\end{mpFunctionsExtract}

%
%\begin{tabular}{p{481pt}}
%\toprule
%\noindent \textsf{Function \textbf{MatExp}(\textbf{A} As mpNum[,]) As mpNum[,]}\index{Multiprecision Functions!MatExp} \\
%\noindent \textsf{Function \textbf{cplxMatExp}((\textbf{A} As mpNum[,]) As mpNum[,]}\index{Multiprecision Functions!cplxMatExp} \\
%\bottomrule
%\end{tabular}

\vspace{0.3cm}
Compute the matrix exponential.

Parameters: [in] M matrix whose exponential is to be computed.  

Returns: expression representing the matrix exponential of M.

The matrix exponential of $M$ is defined by 
\begin{equation}
\exp(M) = \sum_{k=0}^\infty \frac{M^k}{k!}.
\end{equation} 
The matrix exponential can be used to solve linear ordinary differential equations: the solution of $y'=M y$ with the initial condition $y(0)=y_0$ is given by $y(t)=\exp(M) y_0$.
The cost of the computation is approximately $20n^3$ for matrices of size $n$. The number 20 depends weakly on the norm of the matrix.

The matrix exponential is computed using the scaling-and-squaring method combined with Pad\'e approximation. The matrix is first rescaled, then the exponential of the reduced matrix is computed approximant, and then the rescaling is undone by repeated squaring. The degree of the Pad\'e approximant is chosen such that the approximation error is less than the round-off error. However, errors may accumulate during the squaring phase.

Details of the algorithm can be found in \cite{Higham_2005}.

Example: The following program checks that 


\begin{equation} 
\exp
\begin{pmatrix}
0 & \tfrac{1}{4}\pi & 0 \\
-\tfrac{1}{4}\pi  & 0 & 0 \\
0 & 0 & 0 \\
\end{pmatrix}
=
\begin{pmatrix}
\tfrac{1}{2}\sqrt{2} & -\tfrac{1}{2}\sqrt{2} & 0 \\
\tfrac{1}{2}\sqrt{2} & \tfrac{1}{2}\sqrt{2} & 0 \\
0 & 0 & 1 \\
\end{pmatrix}
\end{equation} 


This corresponds to a rotation of $\tfrac{1}{4}\pi$ radians around the z-axis.

\lstset{language={C++}}
\begin{lstlisting}
#include <unsupported/Eigen/MatrixFunctions>
#include <iostream>

using namespace Eigen;

int main()
{
const double pi = std::acos(-1.0);
MatrixXd A(3,3);
A << 0,    -pi/4, 0,
pi/4, 0,     0,
0,    0,     0;  
std::cout << "The matrix A is:\n" << A << "\n\n";  
std::cout << "The matrix exponential of A is:\n" 
<< A.exp() << "\n\n";
}
\end{lstlisting}

\begin{verbatim}
Output:
The matrix A is:
0 -0.785398         0
0.785398         0         0
0         0         0

The matrix exponential of A is:
0.707107 -0.707107         0
0.707107  0.707107         0
0         0         1
\end{verbatim} 

Note: M has to be a matrix of real or complex.




\subsection{Matrix Logarithm}


\begin{mpFunctionsExtract}
	\mpFunctionOne
	{MatLog? mpNum? an expression representing the matrix logarithm of the real matrix M.}
	{M? mpNum[,]? the real matrix of which we are computing the matrix logarithm.}
\end{mpFunctionsExtract}

\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionOne
	{cplxMatLog? mpNum? an expression representing the matrix logarithm of the complex matrix M.}
	{M? mpNum[,]? the complex matrix of which we are computing the matrix logarithm.}
\end{mpFunctionsExtract}

%
%\begin{tabular}{p{481pt}}
%\toprule
%\noindent \textsf{Function \textbf{MatLog}(\textbf{A} As mpNum[,]) As mpNum[,]}\index{Multiprecision Functions!MatLog} \\
%\noindent \textsf{Function \textbf{cplxMatLog}((\textbf{A} As mpNum[,]) As mpNum[,]}\index{Multiprecision Functions!cplxMatLog} \\
%\bottomrule
%\end{tabular}

\vspace{0.3cm}
Compute the matrix logarithm.

Parameters: [in] M invertible matrix whose logarithm is to be computed.  

Returns: expression representing the matrix logarithm root of M.

The matrix logarithm of $M$ is a matrix $X$ such that $\exp(X)=M$ where exp denotes the matrix exponential. As for the scalar logarithm, the equation $\exp(X) = M$ may have multiple solutions; this function returns a matrix whose eigenvalues have imaginary part in the interval $(-\pi, \pi]$.

In the real case, the matrix $M$ should be invertible and it should have no eigenvalues which are real and negative (pairs of complex conjugate eigenvalues are allowed). In the complex case, it only needs to be invertible.

This function computes the matrix logarithm using the Schur-Parlett algorithm as implemented by MatrixBase::matrixFunction(). The logarithm of an atomic block is computed by MatrixLogarithmAtomic, which uses direct computation for 1-by-1 and 2-by-2 blocks and an inverse scaling-and-squaring algorithm for bigger blocks, with the square roots computed by MatrixBase::sqrt().
Details of the algorithm can be found in Section 11.6.2 of \cite{Higham_2008}.

Example: The following program checks that 


\begin{equation} 
\log
\begin{pmatrix}
\tfrac{1}{2}\sqrt{2} & -\tfrac{1}{2}\sqrt{2} & 0 \\
\tfrac{1}{2}\sqrt{2} & \tfrac{1}{2}\sqrt{2} & 0 \\
0 & 0 & 1 \\
\end{pmatrix}
=
\begin{pmatrix}
0 & \tfrac{1}{4}\pi & 0 \\
-\tfrac{1}{4}\pi  & 0 & 0 \\
0 & 0 & 0 \\
\end{pmatrix}
\end{equation} 
This corresponds to a rotation of $\tfrac{1}{4}\pi$ radians around the z-axis. This is the inverse of the example used in the documentation of exp().



\lstset{language={C++}}
\begin{lstlisting}
#include <unsupported/Eigen/MatrixFunctions>
#include <iostream>

using namespace Eigen;

int main()
{
using std::sqrt;
MatrixXd A(3,3);
A << 0.5*sqrt(2), -0.5*sqrt(2), 0,
0.5*sqrt(2),  0.5*sqrt(2), 0,
0,            0,           1;
std::cout << "The matrix A is:\n" << A << "\n\n";
std::cout << "The matrix logarithm of A is:\n" << A.log() << "\n";
}
\end{lstlisting}

\begin{verbatim}			
Output:
The matrix A is:
0.707107 -0.707107         0
0.707107  0.707107         0
0         0         1

The matrix logarithm of A is:
-1.11022e-16    -0.785398            0
0.785398 -1.11022e-16            0
0            0            0

\end{verbatim}




\newpage
\subsection{Matrix raised to arbitrary real power}


\begin{mpFunctionsExtract}
	\mpFunctionTwo
	{MatPow? mpNum? an expression representing the matrix power of the real matrix M.}
	{M? mpNum[,]? M base of the matrix power, should be a square matrix.}
	{p? mpNum? exponent of the matrix power, should be real.}
\end{mpFunctionsExtract}

\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionTwo
	{cplxMatPow? mpNum? an expression representing the matrix power of the complex matrix M.}
	{M? mpNum[,]? M base of the matrix power, should be a square matrix.}
	{p? mpNum? exponent of the matrix power, should be real.}
\end{mpFunctionsExtract}

%
%\begin{tabular}{p{481pt}}
%\toprule
%\noindent \textsf{Function \textbf{MatPow}(\textbf{A} As mpNum[,], \textbf{x} As mpNum) As mpNum[,]}\index{Multiprecision Functions!MatPow} \\
%\noindent \textsf{Function \textbf{cplxMatPow}((\textbf{A} As mpNum[,], \textbf{x} As mpNum) As mpNum[,]}\index{Multiprecision Functions!cplxMatPow} \\
%\bottomrule
%\end{tabular}

\vspace{0.3cm}


MatrixBase::pow()



Compute the matrix raised to arbitrary real power.
const MatrixPowerReturnValue<Derived> MatrixBase<Derived>::pow(RealScalar p) constParameters
[in] M base of the matrix power, should be a square matrix.  
[in] p exponent of the matrix power, should be real. 

The matrix power $M^p$ is defined as $\exp(p \log(M))$, where exp denotes the matrix exponential, and log denotes the matrix logarithm.

The matrix $M$ should meet the conditions to be an argument of matrix logarithm. If $p$ is not of the real scalar type of M, it is casted into the real scalar type of M.

This function computes the matrix power using the Schur-Pad\'e algorithm as implemented by class MatrixPower. The exponent is split into integral part and fractional part, where the fractional part is in the interval $(-1,1)$. The main diagonal and the first super-diagonal is directly computed.

Details of the algorithm can be found in \cite{Higham_2011}.

Example: The following program checks that 

\begin{equation} 
\begin{pmatrix}
\cos(1) & -\sin(1) & 0 \\
\sin(1)  & \cos(1) & 0 \\
0 & 0 & 1 \\
\end{pmatrix}
^{\tfrac{1}{4}\pi} =
\begin{pmatrix}
\tfrac{1}{2}\sqrt{2} & -\tfrac{1}{2}\sqrt{2} & 0 \\
\tfrac{1}{2}\sqrt{2} & \tfrac{1}{2}\sqrt{2} & 0 \\
0 & 0 & 1 \\
\end{pmatrix}
\end{equation} 



This corresponds to $\tfrac{1}{4}\pi$ rotations of 1 radian around the z-axis.


\lstset{language={C++}}
\begin{lstlisting}
#include <unsupported/Eigen/MatrixFunctions>
#include <iostream>

using namespace Eigen;

int main()
{
const double pi = std::acos(-1.0);
Matrix3d A; 
A << cos(1), -sin(1), 0,
sin(1),  cos(1), 0,
0 ,      0 , 1;
std::cout << "The matrix A is:\n" << A << "\n\n"
"The matrix power A^(pi/4) is:\n" << A.pow(pi/4) << std::endl;	
return 0;
}
\end{lstlisting}

\begin{verbatim}		
Output:
The matrix A is:
0.540302 -0.841471         0
0.841471  0.540302         0
0         0         1

The matrix power A^(pi/4) is:
0.707107 -0.707107         0
0.707107  0.707107         0
0         0         1
\end{verbatim}

MatrixBase::pow() is user-friendly. However, there are some circumstances under which you should use class MatrixPower directly. MatrixPower can save the result of Schur decomposition, so it's better for computing various powers for the same matrix.
Example: 


\lstset{language={C++}}
\begin{lstlisting}

#include <unsupported/Eigen/MatrixFunctions>
#include <iostream>

using namespace Eigen;

int main()
{
Matrix4cd A = Matrix4cd::Random();
MatrixPower<Matrix4cd> Apow(A); 

std::cout << "The matrix A is:\n" << A << "\n\n"               
"A^3.1 is:\n" << Apow(3.1) << "\n\n"               
"A^3.3 is:\n" << Apow(3.3) << "\n\n"               
"A^3.7 is:\n" << Apow(3.7) << "\n\n"               
"A^3.9 is:\n" << Apow(3.9) << std::endl;
return 0;
}
\end{lstlisting}

\begin{verbatim}		
Output:
The matrix A is:
(-0.211234,0.680375)   (0.10794,-0.444451)   (0.434594,0.271423) (-0.198111,-0.686642)
(0.59688,0.566198) (0.257742,-0.0452059)  (0.213938,-0.716795) (-0.782382,-0.740419)
(-0.604897,0.823295) (0.0268018,-0.270431) (-0.514226,-0.967399)  (-0.563486,0.997849)
(0.536459,-0.329554)    (0.83239,0.904459)  (0.608354,-0.725537)  (0.678224,0.0258648)

A^3.1 is:
(2.80575,-0.607662) (-1.16847,-0.00660555)    (-0.760385,1.01461)   (-0.38073,-0.106512)
(1.4041,-3.61891)     (1.00481,0.186263)   (-0.163888,0.449419)   (-0.388981,-1.22629)
(-2.07957,-1.58136)     (0.825866,2.25962)     (5.09383,0.155736)    (0.394308,-1.63034)
(-0.818997,0.671026)  (2.11069,-0.00768024)    (-1.37876,0.140165)    (2.50512,-0.854429)

A^3.3 is:
(2.83571,-0.238717) (-1.48174,-0.0615217)  (-0.0544396,1.68092) (-0.292699,-0.621726)
(2.0521,-3.58316)    (0.87894,0.400548)  (0.738072,-0.121242)   (-1.07957,-1.63492)
(-3.00106,-1.10558)     (1.52205,1.92407)    (5.29759,-1.83562)  (-0.532038,-1.50253)
(-0.491353,-0.4145)     (2.5761,0.481286)  (-1.21994,0.0367069)    (2.67112,-1.06331)

A^3.7 is:
(1.42126,0.33362)   (-1.39486,-0.560486)      (1.44968,2.47066)   (-0.324079,-1.75879)
(2.65301,-1.82427)   (0.357333,-0.192429)      (2.01017,-1.4791)    (-2.71518,-2.35892)
(-3.98544,0.964861)     (2.26033,0.554254)     (3.18211,-5.94352)    (-2.22888,0.128951)
(0.944969,-2.14683)      (3.31345,1.66075) (-0.0623743,-0.848324)        (2.3897,-1.863)

A^3.9 is:
(0.0720766,0.378685) (-0.931961,-0.978624)      (1.9855,2.34105)  (-0.530547,-2.17664)
(2.40934,-0.265286)  (0.0299975,-1.08827)    (1.98974,-2.05886)   (-3.45767,-2.50235)
(-3.71666,2.3874)        (2.054,-0.303)   (0.844348,-7.29588)    (-2.59136,1.57689)
(1.87645,-2.38798)     (3.52111,2.10508)    (0.799055,-1.6122)    (1.93452,-2.44408)
\end{verbatim}



\newpage
\subsection{Matrix General Function}


\begin{mpFunctionsExtract}
	\mpFunctionTwo
	{MatGeneralFunction? mpNum? an expression representing f applied to the real matrix M.}
	{M? mpNum[,]? argument of matrix function, should be a square matrix.}
	{f? mpFunction? f an entire function; f(x,n) should compute the n-th derivative of f at x.}
\end{mpFunctionsExtract}

\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionTwo
	{cplxMatGeneralFunction? mpNum? an expression representing f applied to the complex matrix M.}
	{M? mpNum[,]? argument of matrix function, should be a square matrix.}
	{f? mpFunction? f an entire function; f(x,n) should compute the n-th derivative of f at x.}
\end{mpFunctionsExtract}

%
%\begin{tabular}{p{481pt}}
%\toprule
%\noindent \textsf{Function \textbf{MatGeneralFunction}(\textbf{A} As mpNum[,], \textbf{f} As mpFunction) As mpNum[,]}\index{Multiprecision Functions!MatGeneralFunction} \\
%\noindent \textsf{Function \textbf{cplxMatGeneralFunction}((\textbf{A} As mpNum[,], \textbf{f} As mpFunction) As mpNum[,]}\index{Multiprecision Functions!cplxMatGeneralFunction} \\
%\bottomrule
%\end{tabular}

\vspace{0.3cm}
Compute a matrix function.

Parameters

[in] M argument of matrix function, should be a square matrix. 

[in] f an entire function; f(x,n) should compute the n-th derivative of f at x.  

Returns expression representing f applied to M.

Suppose that M is a matrix whose entries have type Scalar. Then, the second argument, f, should be a function with prototype 

ComplexScalar f(ComplexScalar, int) 

where ComplexScalar = std::complex<Scalar> if Scalar is real (e.g., float or double) and ComplexScalar = Scalar if Scalar is complex. 

The return value of f(x,n) should be $f^{(n)}(x)$, the n-th derivative of f at x.

This routine uses the algorithm described in \cite{Davies_2003}.


The actual work is done by the MatrixFunction class.
Example: The following program checks that 


\begin{equation} 
\exp
\begin{pmatrix}
0 & \tfrac{1}{4}\pi & 0 \\
-\tfrac{1}{4}\pi  & 0 & 0 \\
0 & 0 & 0 \\
\end{pmatrix}
=
\begin{pmatrix}
\tfrac{1}{2}\sqrt{2} & -\tfrac{1}{2}\sqrt{2} & 0 \\
\tfrac{1}{2}\sqrt{2} & \tfrac{1}{2}\sqrt{2} & 0 \\
0 & 0 & 1 \\
\end{pmatrix}
\end{equation} 



This corresponds to a rotation of $\tfrac{1}{4}\pi$ radians around the z-axis. This is the same example as used in the documentation of exp().


\lstset{language={C++}}
\begin{lstlisting}
#include <unsupported/Eigen/MatrixFunctions>
#include <iostream>

using namespace Eigen;

std::complex<double> expfn(std::complex<double> x, int)
{
return std::exp(x);
}

int main()
{
const double pi = std::acos(-1.0); 

MatrixXd A(3,3);
A << 0,    -pi/4, 0,
pi/4, 0,     0,
0,    0,     0;  

std::cout << "The matrix A is:\n" << A << "\n\n";  
std::cout << "The matrix exponential of A is:\n"             
<< A.matrixFunction(expfn) << "\n\n";
}
\end{lstlisting}

\begin{verbatim}
Output:
The matrix A is:
0 -0.785398         0
0.785398         0         0
0         0         0

The matrix exponential of A is:
0.707107 -0.707107         0
0.707107  0.707107         0
0         0         1

\end{verbatim}
Note that the function expfn is defined for complex numbers x, even though the matrix A is over the reals. Instead of expfn, we could also have used StdStemFunctions::exp: 

A.matrixFunction(StdStemFunctions<std::complex<double> >::exp, \&B);



\subsection{Matrix Sine}


\begin{mpFunctionsExtract}
	\mpFunctionOne
	{MatSin? mpNum? an expression representing the matrix sine of the real matrix M.}
	{M? mpNum[,]? the real matrix of which we are computing the matrix sine.}
\end{mpFunctionsExtract}

\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionOne
	{cplxMatSin? mpNum? an expression representing the matrix sine of the complex matrix M.}
	{M? mpNum[,]? the complex matrix of which we are computing the matrix sine.}
\end{mpFunctionsExtract}

%
%\begin{tabular}{p{481pt}}
%\toprule
%\noindent \textsf{Function \textbf{MatSin}(\textbf{A} As mpNum[,]) As mpNum[,]}\index{Multiprecision Functions!MatSin} \\
%\noindent \textsf{Function \textbf{cplxMatSin}((\textbf{A} As mpNum[,]) As mpNum[,]}\index{Multiprecision Functions!cplxMatSin} \\
%\bottomrule
%\end{tabular}

\vspace{0.3cm}
Compute the matrix sine.

Parameters:
[in] M a square matrix.  

Returns:
expression representing $\sin(M)$.

This function calls matrixFunction() with StdStemFunctions::sin().

Example:


\lstset{language={C++}}
\begin{lstlisting}
#include <unsupported/Eigen/MatrixFunctions>
#include <iostream>

using namespace Eigen;

int main()
{
MatrixXd A = MatrixXd::Random(3,3);
std::cout << "A = \n" << A << "\n\n";

MatrixXd sinA = A.sin();
std::cout << "sin(A) = \n" << sinA << "\n\n";

MatrixXd cosA = A.cos();
std::cout << "cos(A) = \n" << cosA << "\n\n";

// The matrix functions satisfy sin^2(A) + cos^2(A) = I,   
// like the scalar functions. 
std::cout << "sin^2(A) + cos^2(A) = \n" << sinA*sinA + cosA*cosA << "\n\n";
}
\end{lstlisting}

\begin{verbatim}
Output:
A = 
0.680375   0.59688 -0.329554
-0.211234  0.823295  0.536459
0.566198 -0.604897 -0.444451

sin(A) = 
0.679919    0.4579 -0.400612
-0.227278  0.821913    0.5358
0.570141 -0.676728 -0.462398

cos(A) = 
0.927728  -0.530361  -0.110482
0.00969246   0.889022  -0.137604
-0.132574   -0.04289    1.16475

sin^2(A) + cos^2(A) = 
1  4.44089e-16  1.94289e-16
6.38378e-16            1  5.55112e-16
0 -6.10623e-16            1

\end{verbatim}



\newpage
\subsection{Matrix Cosine}


\begin{mpFunctionsExtract}
	\mpFunctionOne
	{MatCos? mpNum? an expression representing the matrix cosine of the real matrix M.}
	{M? mpNum[,]? the real matrix of which we are computing the matrix cosine.}
\end{mpFunctionsExtract}

\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionOne
	{cplxMatCos? mpNum? an expression representing the matrix cosine of the complex matrix M.}
	{M? mpNum[,]? the complex matrix of which we are computing the matrix cosine.}
\end{mpFunctionsExtract}

%
%\begin{tabular}{p{481pt}}
%\toprule
%\noindent \textsf{Function \textbf{MatCos}(\textbf{A} As mpNum[,]) As mpNum[,]}\index{Multiprecision Functions!MatCos} \\
%\noindent \textsf{Function \textbf{cplxMatCos}((\textbf{A} As mpNum[,]) As mpNum[,]}\index{Multiprecision Functions!cplxMatCos} \\
%\bottomrule
%\end{tabular}

\vspace{0.3cm}
Compute the matrix cosine.

Parameters: [in] M a square matrix.  

Returns expression representing $\cos(M)$.

This function calls matrixFunction() with StdStemFunctions::cos().

See Also sin() for an example.



\subsection{Matrix Hyperbolic Sine}


\begin{mpFunctionsExtract}
	\mpFunctionOne
	{MatSinh? mpNum? an expression representing the matrix hyperbolic sine of the real matrix M.}
	{M? mpNum[,]? the real matrix of which we are computing the matrix hyperbolic sine.}
\end{mpFunctionsExtract}

\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionOne
	{cplxMatSinh? mpNum? an expression representing the matrix hyperbolic sine of the complex matrix M.}
	{M? mpNum[,]? the complex matrix of which we are computing the matrix hyperbolic sine.}
\end{mpFunctionsExtract}

%
%\begin{tabular}{p{481pt}}
%\toprule
%\noindent \textsf{Function \textbf{MatSinh}(\textbf{A} As mpNum[,]) As mpNum[,]}\index{Multiprecision Functions!MatSinh} \\
%\noindent \textsf{Function \textbf{cplxMatSinh}((\textbf{A} As mpNum[,]) As mpNum[,]}\index{Multiprecision Functions!cplxMatSinh} \\
%\bottomrule
%\end{tabular}

\vspace{0.3cm}
Compute the matrix hyperbolic sine.

Parameters:
[in] M a square matrix.  

Returns:
expression representing $\sinh(M)$.

This function calls matrixFunction() with StdStemFunctions::sinh().

Example:

\lstset{language={C++}}
\begin{lstlisting}
#include <unsupported/Eigen/MatrixFunctions>
#include <iostream>

using namespace Eigen;

int main()
{
MatrixXf A = MatrixXf::Random(3,3);
std::cout << "A = \n" << A << "\n\n";

MatrixXf sinhA = A.sinh();
std::cout << "sinh(A) = \n" << sinhA << "\n\n";

MatrixXf coshA = A.cosh();
std::cout << "cosh(A) = \n" << coshA << "\n\n";

// The matrix functions satisfy cosh^2(A) - sinh^2(A) = I,
// like the scalar functions.
std::cout << "cosh^2(A) - sinh^2(A) = \n" << coshA*coshA - sinhA*sinhA 
<< "\n\n";
}
\end{lstlisting}

\begin{verbatim}
Output:
A = 
0.680375   0.59688 -0.329554
-0.211234  0.823295  0.536459
0.566198 -0.604897 -0.444451

sinh(A) = 
0.682534  0.739989 -0.256871
-0.194928  0.826512  0.537546
0.562584  -0.53163 -0.425199

cosh(A) = 
1.07817    0.567068    0.132125
-0.00418614     1.11649    0.135361
0.128891   0.0659989    0.851201

cosh^2(A) - sinh^2(A) = 
1            0   8.9407e-08
1.29454e-07            1 -2.98023e-08
0 -2.83122e-07            1

\end{verbatim}




\subsection{Matrix Hyberbolic Cosine}


\begin{mpFunctionsExtract}
	\mpFunctionOne
	{MatCosh? mpNum? an expression representing the matrix hyperbolic cosine of the real matrix M.}
	{M? mpNum[,]? the real matrix of which we are computing the matrix hyperbolic cosine.}
\end{mpFunctionsExtract}

\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionOne
	{cplxMatCosh? mpNum? an expression representing the matrix hyperbolic cosine of the complex matrix M.}
	{M? mpNum[,]? the complex matrix of which we are computing the matrix hyperbolic cosine.}
\end{mpFunctionsExtract}

%
%\begin{tabular}{p{481pt}}
%\toprule
%\noindent \textsf{Function \textbf{MatCosh}(\textbf{A} As mpNum[,]) As mpNum[,]}\index{Multiprecision Functions!MatCosh} \\
%\noindent \textsf{Function \textbf{cplxMatCosh}((\textbf{A} As mpNum[,]) As mpNum[,]}\index{Multiprecision Functions!cplxMatCosh} \\
%\bottomrule
%\end{tabular}

\vspace{0.3cm}
Compute the matrix hyberbolic cosine.

Parameters: [in] M a square matrix.  

Returns expression representing $\cosh(M)$.

This function calls matrixFunction() with StdStemFunctions::cosh().

See Also sinh() for an example.





\chapter{Polynomials (based on Eigen)}
\label{Polynomials} % So I can \ref{altrings3} later.
%\lipsum[2-3]




\section{Polynomial Evaluation}
\label{PolynomialEvaluationPolynomials}

The functions described here evaluate the polynomial $c_0 + c_1 x + c_2 x^2 + \ldots + c_{n- 1} x^{n-1}$ using Horner's method for stability.

\subsection{Polynomial Evaluation, Real Coefficients and Argument}

\begin{mpFunctionsExtract}
	\mpFunctionTwo
	{PolynomialEvaluation? mpNum? the value of a polynomial for the real variable $x$ with real coefficients $c$.}
	{x? mpNum? A real number.}
	{c? mpNum[]? A vector of real coefficients.}
\end{mpFunctionsExtract}



\subsection{Polynomial Evaluation, Complex Coefficients and Argument}

\begin{mpFunctionsExtract}
	\mpFunctionTwo
	{cplxPolynomialEvaluation? mpNum? the value of a polynomial for the complex variable $z$ with complex coefficients $c$.}
	{z? mpNum? A complex number.}
	{c? mpNum[]? A vector of complex coefficients.}
\end{mpFunctionsExtract}





\subsection{Examples}
\label{PolynomialEvaluationPolynomialsExamples}

\lstset{language={[Visual]Basic}}
\begin{lstlisting}
Sub DemoPolyComplexEvalComplex()
Dim c() As mp_complex, n As Long
Dim x As mp_complex, y As mp_complex
n = 4
x.Real = 3.54: x.Imag = 2.66
ReDim c(0 To n - 1)
c(0).Real = 2: c(1).Real = 5: c(2).Real = 4: c(3).Real = 7
c(0).Imag = 2: c(1).Imag = 5: c(2).Imag = 4: c(3).Imag = 7
y = mp_complex_poly_complex_eval(c(0), n, x)
Debug.Print "x: ", x.Real, x.Imag, "y:", y.Real, y.Imag
End Sub
\end{lstlisting}

The output of the program is,
\begin{verbatim}
x:     3.54 + 2.66i
y:   -830.84176  + 482.955648i
\end{verbatim}






\section{Quadratic Equations}
\label{QuadraticEquationsPolynomials}

\subsection{Quadratic Equation, Real Coefficients and Zeros}

\begin{mpFunctionsExtract}
	\mpFunctionThree
	{QuadraticEquation? mpNum[]? a real vector containing the real roots of the quadratic equation.}
	{a? mpNum? A real number.}
	{b? mpNum? A real number.}
	{c? mpNum? A real number.}
\end{mpFunctionsExtract}


\vspace{0.3cm}

This function returns a real vector containing the real roots of the quadratic equation
\begin{equation}
a  + b x + c x^2  = 0
\end{equation}
where the coefficients $a, b, c$ are all real. 

The roots are returned in ascending order. If no real roots exist, then the function returns NaN.

The case of coincident roots is not considered special. For example $(x - 1)^2 = 0$ will have two roots, which happen to have exactly equal values.

The number of roots found depends on the sign of the discriminant $b^2 - 4ac$. This will be subject to rounding and cancellation errors when computed in mp\_real precision, and will also be subject to errors if the coefficients of the polynomial are inexact. These errors may cause a discrete change in the number of roots. However, for polynomials with small integer coefficients the discriminant can always be computed exactly.



\subsection{Quadratic Equation, Complex Coefficients and Zeros}

\begin{mpFunctionsExtract}
	\mpFunctionThree
	{cplxQuadraticEquation? mpNum[]? a complex vector containing the complex roots of the quadratic equation.}
	{a? mpNum? A real or complex number.}
	{b? mpNum? A real or complex number.}
	{c? mpNum? A real or complex number.}
\end{mpFunctionsExtract}


\vspace{0.3cm}
This function returns  a complex vector containing the complex roots of the quadratic equation
\begin{equation}
a  + b z + c z^2  = 0,
\end{equation}
where the coefficients $a, b, c$ can be either real or complex. 

The roots are returned in ascending order, sorted first by their real components and then by their imaginary components.






\section{Cubic Equations}
\label{CubicEquationsPolynomials}


\subsection{Cubic Equation, Real Coefficients and Zeros}

\begin{mpFunctionsExtract}
	\mpFunctionFour
	{CubicEquation? mpNum[]? a real vector containing the real roots of the cubic equation.}
	{a? mpNum? A real number.}
	{b? mpNum? A real number.}
	{c? mpNum? A real number.}
	{d? mpNum? A real number.}
\end{mpFunctionsExtract}


\vspace{0.3cm}
This function returns a real vector containing the real roots of the cubic equation
\begin{equation}
a  + b x + c x^2 + d x^3  = 0
\end{equation}
where the coefficients $a, b, c, d$ are all real. 

The roots (either one or three) are returned in ascending order. The case of coincident roots is not considered special. For example, the equation $(x-1)^3 = 0$ will have three roots with exactly equal values. As in the quadratic case, finite precision may cause equal or closely-spaced real roots to move off the real axis into the complex plane, leading to a discrete change in the number of real roots.




\subsection{Cubic Equation, Complex Coefficients and Zeros}

\begin{mpFunctionsExtract}
	\mpFunctionFour
	{cplxCubicEquation? mpNum[]? a complex vector containing the complex roots of the cubic equation.}
	{a? mpNum? A real or complex number.}
	{b? mpNum? A real or complex number.}
	{c? mpNum? A real or complex number.}
	{d? mpNum? A real or complex number.}
\end{mpFunctionsExtract}


\vspace{0.3cm}
This function returns  a complex vector containing the complex roots of the cubic equation
\begin{equation}
a  + b z + c z^2 + d z^3  = 0,
\end{equation}
where the coefficients $a, b, c, d$ can be either real or complex. 

The roots are returned in ascending order, sorted first by their real components and then by their imaginary components.






\section{Quartic Equations}
\label{QuarticEquationsPolynomials}

\subsection{Quartic Equation, Real Coefficients and Zeros}


\begin{mpFunctionsExtract}
	\mpFunctionFive
	{QuarticEquation? mpNum[]? a real vector containing the real roots of the quartic equation.}
	{a? mpNum? A real number.}
	{b? mpNum? A real number.}
	{c? mpNum? A real number.}
	{d? mpNum? A real number.}
	{e? mpNum? A real number.}
\end{mpFunctionsExtract}


\vspace{0.3cm}
This function returns a real vector containing the real roots of the quartic equation
\begin{equation}
a  + b x + c x^2 + d x^3  + e x^4  = 0
\end{equation}
where the coefficients $a, b, c, d, e$ are all real. 

The roots are returned in ascending order. If no real roots exist, then the function returns NaN.




\subsection{Quartic Equation, Complex Coefficients and Zeros}

\begin{mpFunctionsExtract}
	\mpFunctionFive
	{cplxQuarticEquation? mpNum[]? a complex vector containing the complex roots of the quartic equation.}
	{a? mpNum? A real or complex number.}
	{b? mpNum? A real or complex number.}
	{c? mpNum? A real or complex number.}
	{d? mpNum? A real or complex number.}
	{e? mpNum? A real or complex number.}
\end{mpFunctionsExtract}


\vspace{0.3cm}
This function returns  a complex vector containing the complex roots of the quartic equation
\begin{equation}
a  + b z + c z^2 + d z^3  + e z^4  = 0
\end{equation}
where the coefficients $a, b, c, d, e$ can be either real or complex. 

The roots are returned in ascending order, sorted first by their real components and then by their imaginary components.




\section{General Polynomial Equations}
\label{GeneralPolynomialEquationsPolynomials}

\subsection{General Polynomial Equation, Real Coefficients and Zeros}

\begin{mpFunctionsExtract}
	\mpFunctionOne
	{GeneralPolynomialEquation? mpNum[]? a real vector containing the real roots of the general real polynomial.}
	{a? mpNum[]? The real coefficients of the polynomial.}
\end{mpFunctionsExtract}


\vspace{0.3cm}
This function computes the real roots of the general real polynomial

\begin{equation}
P(x) = a_0 + a_1x + a_2x^2 + \ldots +  a_{n-1} x^{n-1}
\end{equation}

using balanced-QR reduction of the companion matrix (see \cite{Edelman_1995}. The coefficient of the highest order term must be non-zero. The roots (if any) are returned as  real vector.



\subsection{General Polynomial Equation, Complex Coefficients and Zeros}

\begin{mpFunctionsExtract}
	\mpFunctionOne
	{cplxGeneralPolynomialEquation? mpNum[]? a complex vector containing the complex roots of the general complex polynomial.}
	{c? mpNum[]? The complex coefficients of the polynomial.}
\end{mpFunctionsExtract}



\vspace{0.3cm}
This function computes the complex roots of the general complex polynomial

\begin{equation}
P(z) = c_0 + c_1z + c_2z^2 + \ldots +  c_{n-1} z^{n-1}
\end{equation}

using balanced-QR reduction of the companion matrix (see \cite{Edelman_1995}. The coefficient of the highest order term must be non-zero. The $n-1$ roots are returned as a complex vector.
The function returns mp\_SUCCESS if all the roots are found. If the QR reduction does not converge, the error handler is invoked with an error code of mp\_EFAILED. Note that due to finite precision, roots of higher multiplicity are returned as a cluster of simple roots with reduced accuracy. The solution of polynomials with higher-order roots requires specialized algorithms that take the multiplicity structure into account (see e.g. \cite{Zeng_2004, Zeng2005}


To demonstrate the use of the general polynomial solver we will take the polynomial $P(x) = x^5 - 1$ which has the following roots,

\begin{equation}
1, e^{2\pi i/5}, e^{4\pi i/5},e^{6\pi i/5},e^{8\pi i/5}.
\end{equation}


\lstset{language={[Visual]Basic}}
\begin{lstlisting}
Sub DemoComplexSolve()
Dim a() As mp_real, z() As mp_complex, w As mp_poly_complex_workspace
Dim n As Long, i As Long, status As Long
n = 6
ReDim a(0 To n - 1)
ReDim z(0 To n - 2)
a(0) = -1: a(1) = 0: a(2) = 0: a(3) = 0: a(4) = 0: a(5) = 1
w = mp_poly_complex_workspace_alloc(n)
status = mp_poly_complex_solve(a(), n, w, z())
Call mp_poly_complex_workspace_free(w)
For i = 0 To n - 2
Debug.Print z(i).Real, z(i).Imag
Next i
End Sub
\end{lstlisting}

The output of the program is
\begin{verbatim}
z0 = -0.809016994374947451 +0.587785252292473137
z1 = -0.809016994374947451 -0.587785252292473137
z2 = +0.309016994374947451 +0.951056516295153642
z3 = +0.309016994374947451 -0.951056516295153642
z4 = +1.000000000000000000 +0.000000000000000000
\end{verbatim}

which agrees with the analytic result, $z_n = e^{2n\pi i/5}$. 





\chapter{Fast Fourier Transform (based on Eigen)}
\label{FastFourierTransform} 
%\lipsum[2-3]




\section{Discrete Fourier Transforms}

In this section, we provide precise mathematical definitions for the transforms that FFTW
computes. These transform definitions are fairly standard, but some authors follow slightly
different conventions for the normalization of the transform (the constant factor in front)
and the sign of the complex exponent. We begin by presenting the one-dimensional (1d)
transform definitions, and then give the straightforward extension to multi-dimensional
transforms.

A good introduction is given in \cite{Arndt_2011}, chapter 21.

Another Reference is \cite{Kammler_2008}.

Based on the EIGEN implementation of KISSFFT.


\subsection{1d Complex Discrete Fourier Transform (DFT)}


\begin{mpFunctionsExtract}
	\mpFunctionOne
	{FFTW\_FORWARD? mpNum[]? a complex vector containing the forward complex discrete Fourier transform of $X$.}
	{X? mpNum[]? A complex vector.}
\end{mpFunctionsExtract}



\vspace{0.3cm}
The forward (\textsf{FFTW\_FORWARD}) discrete Fourier transform (DFT) of a 1d complex array $X$ of
size $n$ computes an array $Y$, where:
\begin{equation}
Y_k = \sum^{n-1}_{j=0} X_j e^{-2\pi jk \sqrt{-1}/n}.
\end{equation}


\begin{mpFunctionsExtract}
	\mpFunctionOne
	{FFTW\_BACKWARD? mpNum[]? a complex vector containing the backward complex discrete Fourier transform of $X$.}
	{X? mpNum[]? A complex vector.}
\end{mpFunctionsExtract}



The backward (\textsf{FFTW\_BACKWARD}) DFT computes:
\begin{equation}
Y_k = \sum^{n-1}_{j=0} X_j e^{2\pi jk \sqrt{-1}/n}.
\end{equation}

FFTW computes an unnormalized transform, in that there is no coefficient in front of
the summation in the DFT. In other words, applying the forward and then the backward
transform will multiply the input by $n$.

From above, an \textsf{FFTW\_FORWARD} transform corresponds to a sign of $-1$ in the exponent of the DFT. Note also that we use the standard “in-order” output ordering $-$ the $k$-th output
corresponds to the frequency $k/n$ (or $k/T$, where $T$ is your total sampling period). 

For those who like to think in terms of positive and negative frequencies, this means that the
positive frequencies are stored in the first half of the output and the negative frequencies
are stored in backwards order in the second half of the output. (The frequency $-k/n$ is the
same as the frequency $(n - k)/n$.)





\subsection{1d Real-data DFT}


\begin{mpFunctionsExtract}
	\mpFunctionOne
	{FFTW\_R2C? mpNum[]? a complex vector containing the forward complex discrete Fourier transform of $X$.}
	{X? mpNum[]? A real vector.}
\end{mpFunctionsExtract}


\vspace{0.3cm}
The real-input (r2c) DFT in FFTW computes the \textit{forward} transform Y of the size n real
array X, exactly as defined above, i.e.
\begin{equation}
Y_k = \sum^{n-1}_{j=0} X_j e^{-2\pi jk \sqrt{-1}/n}.
\end{equation}
This output array $Y$ can easily be shown to possess the "Hermitian" symmetry $Y_k = Y^*_{
	n-k}$, where we take $Y$ to be periodic so that $Y_n = Y_0$.
As a result of this symmetry, half of the output $Y$ is redundant (being the complex conjugate
of the other half), and so the 1d r2c transforms only output elements $0 \ldots n/2$ of $Y$ $(n/2+1$ complex numbers), where the division by 2 is rounded down.
Moreover, the Hermitian symmetry implies that $Y_0$ and, if $n$ is even, the $Y_{n/2}$  element, are purely real. So, for the R2HC r2r transform, these elements are not stored in the halfcomplex output format.

\vspace{0.6cm}
\begin{mpFunctionsExtract}
	\mpFunctionOne
	{FFTW\_C2R? mpNum[]? a real vector containing the backward  discrete Fourier transform of $X$.}
	{X? mpNum[]? A complex hermitian vector.}
\end{mpFunctionsExtract}



The c2r and H2RC r2r transforms compute the backward DFT of the \textit{complex} array $X$
with Hermitian symmetry, stored in the r2c/R2HC output formats, respectively, where the
backward transform is defined exactly as for the complex case:
\begin{equation}
Y_k = \sum^{n-1}_{j=0} X_j e^{2\pi jk \sqrt{-1}/n}.
\end{equation}

The outputs $Y$ of this transform can easily be seen to be purely real, and are stored as an
array of real numbers.
Like FFTW’s complex DFT, these transforms are unnormalized. In other words, applying the real-to-complex (forward) and then the complex-to-real (backward) transform will
multiply the input by $n$.



\subsection{1d Real-even DFTs (DCTs)}
%
%\begin{tabular}{p{481pt}}
%\toprule
%\noindent \textsf{Function \textbf{FFTW\_REDFT00}($\boldsymbol{X}\ As\ mpNum$) As mpNum}\index{Multiprecision Functions!FFTW\_REDFT00} \\
%\noindent \textsf{Function \textbf{FFTW\_REDFT10}($\boldsymbol{X}\ As\ mpNum$) As mpNum}\index{Multiprecision Functions!FFTW\_REDFT10} \\
%\noindent \textsf{Function \textbf{FFTW\_REDFT01}($\boldsymbol{X}\ As\ mpNum$) As mpNum}\index{Multiprecision Functions!FFTW\_REDFT01} \\
%\noindent \textsf{Function \textbf{FFTW\_REDFT11}($\boldsymbol{X}\ As\ mpNum$) As mpNum}\index{Multiprecision Functions!FFTW\_REDFT11} \\
%
%\bottomrule
%\end{tabular}
%
%\vspace{0.3cm}
The Real-even symmetry DFTs in FFTW are exactly equivalent to the unnormalized forward (and backward) DFTs as defined above, where the input array $X$ of length $N$ is purely real and is also even symmetry. In this case, the output array is likewise real and even symmetry.


For the case of REDFT00, this even symmetry means that $X_j = X_{N-j}$, where we take $X$ to
be periodic so that $X_N = X_0$. Because of this redundancy, only the first $n$ real numbers
are actually stored, where $N = 2(n - 1)$.


The proper definition of even symmetry for REDFT10, REDFT01, and REDFT11 transforms is
somewhat more intricate because of the shifts by $1/2$ of the input and/or output. Because of the even symmetry, however, the sine terms in the DFT all cancel and the remaining cosine terms are written explicitly below. This formulation often leads people to call such a transform a discrete cosine transform (DCT), although it is really just a special case of the DFT.

In each of the definitions below, we transform a real array $X$ of length $n$ to a real array $Y$ of length $n$:


\subsubsection{REDFT00 (DCT-I)}

\begin{mpFunctionsExtract}
	\mpFunctionOne
	{FFTW\_REDFT00? mpNum[]? a real vector containing the REDFT00 transform (type-I DCT) transform of $X$.}
	{X? mpNum[]? A real vector.}
\end{mpFunctionsExtract}

\vspace{0.3cm}
An REDFT00 transform (type-I DCT) in FFTW is defined by:
\begin{equation}
Y_k = X_0 + (-1)^k X_{n-1} + 2 \sum^{n-2}_{j=1} X_j \cos[\pi jk/(n-1)].
\end{equation}

Note that this transform is not defined for $n = 1$. For $n = 2$, the summation term above is
dropped as you might expect.




\subsubsection{REDFT10 (DCT-II)}
\begin{mpFunctionsExtract}
	\mpFunctionOne
	{FFTW\_REDFT10? mpNum[]? a real vector containing the REDFT10 transform (type-II DCT) transform of $X$.}
	{X? mpNum[]? A real vector.}
\end{mpFunctionsExtract}

\vspace{0.3cm}
An REDFT10 transform (type-II DCT, sometimes called "the" DCT) in FFTW is defined by:
\begin{equation}
Y_k = 2 \sum^{n-1}_{j=0} X_j \cos[\pi (j+1/2)k/n].
\end{equation}




\subsubsection{REDFT01 (DCT-III)}
\begin{mpFunctionsExtract}
	\mpFunctionOne
	{FFTW\_REDFT01? mpNum[]? a real vector containing the REDFT01 transform (type-III DCT) transform of $X$.}
	{X? mpNum[]? A real vector.}
\end{mpFunctionsExtract}

\vspace{0.3cm}
An REDFT01 transform (type-III DCT) in FFTW is defined by:
\begin{equation}
Y_k = X_0 + 2 \sum^{n-1}_{j=1} X_j \cos[\pi j(k+1/2)/n].
\end{equation}
In the case of $n = 1$, this reduces to $Y_0 = X_0$. Up to a scale factor (see below), this is the inverse of REDFT10 ("the" DCT), and so the REDFT01 (DCT-III) is sometimes called the
"IDCT".




\subsubsection{REDFT11 (DCT-IV)}

\begin{mpFunctionsExtract}
	\mpFunctionOne
	{FFTW\_REDFT11? mpNum[]? a real vector containing the REDFT11 transform (type-IV DCT) transform of $X$.}
	{X? mpNum[]? A real vector.}
\end{mpFunctionsExtract}

\vspace{0.3cm}
An REDFT11 transform (type-IV DCT) in FFTW is defined by:
\begin{equation}
Y_k = 2 \sum^{n-1}_{j=0} X_j \cos[\pi (j+1/2)(k+1/2)/n].
\end{equation}


\subsubsection{Inverses and Normalization}
These definitions correspond directly to the unnormalized DFTs used elsewhere in FFTW
(hence the factors of 2 in front of the summations). The unnormalized inverse of REDFT00
is REDFT00, of REDFT10 is REDFT01 and vice versa, and of REDFT11 is REDFT11. Each
unnormalized inverse results in the original array multiplied by $N$, where $N$ is the logical DFT size. For REDFT00, $N = 2(n - 1)$ (note that $n = 1$ is not defined); otherwise, $N = 2n$.


In defining the discrete cosine transform, some authors also include additional factors of
$\sqrt{2}$ (or its inverse) multiplying selected inputs and/or outputs. This is a mostly cosmetic change that makes the transform orthogonal, but sacrifices the direct equivalence to a symmetric DFT.






\subsection{1d Real-odd DFTs (DSTs)}

The Real-odd symmetry DFTs in FFTW are exactly equivalent to the unnormalized forward (and backward) DFTs as defined above, where the input array $X$ of length $N$ is purely real and is also odd symmetry. In this case, the output array is  odd symmetry and purely imaginary.

For the case of RODFT00, this odd symmetry means that $X_j = -X_{N-j}$, where we take $X$ to
be periodic so that $X_N = X_0$. Because of this redundancy, only the first $n$ real numbers
starting at $j = 1$ are actually stored (the $j = 0$ element is zero),, where $N = 2(n + 1)$.


The proper definition of odd symmetry for RODFT10, RODFT01, and RODFT11 transforms is
somewhat more intricate because of the shifts by $1/2$ of the input and/or output. Because of the odd symmetry, however, the cosine terms in the DFT all cancel and the remaining sine terms are written explicitly below. This formulation often leads people to call such a transform a discrete sine transform (DCT), although it is really just a special case of the DFT.

In each of the definitions below, we transform a real array $X$ of length $n$ to a real array $Y$ of length $n$:


\subsubsection{RODFT00 (DST-I)}

\begin{mpFunctionsExtract}
	\mpFunctionOne
	{FFTW\_RODFT00? mpNum[]? a real vector containing the RODFT00 transform (type-I DST) transform of $X$.}
	{X? mpNum[]? A real vector.}
\end{mpFunctionsExtract}

An RODFT00 transform (type-I DST) in FFTW is defined by:
\begin{equation}
Y_k = 2 \sum^{n-1}_{j=0} X_j \sin[\pi (j+1)(k+1)/(n+1)].
\end{equation}




\subsubsection{RODFT10 (DST-II)}

\begin{mpFunctionsExtract}
	\mpFunctionOne
	{FFTW\_RODFT10? mpNum[]? a real vector containing the RODFT10 transform (type-II DST) transform of $X$.}
	{X? mpNum[]? A real vector.}
\end{mpFunctionsExtract}

An RODFT10 transform (type-II DST) in FFTW is defined by:
\begin{equation}
Y_k = 2 \sum^{n-1}_{j=0} X_j \sin[\pi (j+1/2)(k+1/2)/n].
\end{equation}




\subsubsection{RODFT01 (DST-III)}

\begin{mpFunctionsExtract}
	\mpFunctionOne
	{FFTW\_RODFT01? mpNum[]? a real vector containing the RODFT01 transform (type-III DST) transform of $X$.}
	{X? mpNum[]? A real vector.}
\end{mpFunctionsExtract}

An RODFT01 transform (type-III DST) in FFTW is defined by:
\begin{equation}
Y_k = (-1)^k X_{n-1} + 2 \sum^{n-2}_{j=0} X_j \sin[\pi (j+1)(k+1/2)/n].
\end{equation}
In the case of $n = 1$, this reduces to $Y_0 = X_0$. 




\subsubsection{RODFT11 (DST-IV)}

\begin{mpFunctionsExtract}
	\mpFunctionOne
	{FFTW\_RODFT11? mpNum[]? a real vector containing the RODFT11 transform (type-IV DST) transform of $X$.}
	{X? mpNum[]? A real vector.}
\end{mpFunctionsExtract}

An RODFT11 transform (type-IV DST) in FFTW is defined by:
\begin{equation}
Y_k = 2 \sum^{n-1}_{j=0} X_j \sin[\pi (j+1/2)(k+1/2)/n].
\end{equation}


\subsubsection{Inverses and Normalization}
These definitions correspond directly to the unnormalized DFTs used elsewhere in FFTW
(hence the factors of 2 in front of the summations). The unnormalized inverse of RODFT00
is RODFT00, of RODFT10 is RODFT01 and vice versa, and of RODFT11 is RODFT11. Each
unnormalized inverse results in the original array multiplied by $N$, where $N$ is the logical DFT size. For RODFT00, $N = 2(n + 1)$; otherwise, $N = 2n$.


In defining the discrete sine transform, some authors also include additional factors of
$\sqrt{2}$ (or its inverse) multiplying selected inputs and/or outputs. This is a mostly cosmetic change that makes the transform orthogonal, but sacrifices the direct equivalence to a symmetric DFT.









\chapter{Minimization and Optimization: Procedures based on MINPACK}


Reference for  MINPACK: \cite{More_1980}.

Detailed Description

\#include <unsupported/Eigen/NonLinearOptimization>

This module provides implementation of two important algorithms in non linear optimization. In both cases, we consider a system of non linear functions. Of course, this should work, and even work very well if those functions are actually linear. But if this is so, you should probably better use other methods more fitted to this special case.

One algorithm allows to find an extremum of such a system (Levenberg Marquardt algorithm) and the second one is used to find a zero for the system (Powell hybrid "dogleg" method).

This code is a port of MINPACK . Minpack is a very famous, old, robust and well-reknown package, written in fortran. Those implementations have been carefully tuned, tested, and used for several decades.

The original fortran code was automatically translated using f2c in C, then c++, and then cleaned by several different authors. 

Finally, we ported this code to Eigen, creating classes and API coherent with Eigen. When possible, we switched to Eigen implementation, such as most linear algebra (vectors, matrices, stable norms).

Doing so, we were very careful to check the tests we setup at the very beginning, which ensure that the same results are found.


Tests

The tests are placed in the file unsupported/test/NonLinear.cpp.

There are two kinds of tests : those that come from examples bundled with cminpack. They guaranty we get the same results as the original algorithms (value for 'x', for the number of evaluations of the function, and for the number of evaluations of the jacobian if ever).

Other tests were added by myself at the very beginning of the process and check the results for levenberg-marquardt using the reference data on NIST. Since then i've carefully checked that the same results were obtained when modifiying the code. Please note that we do not always get the exact same decimals as they do, but this is ok : they use 128bits float, and we do the tests using the C type 'double', which is 64 bits on most platforms (x86 and amd64, at least). I've performed those tests on several other implementations of levenberg-marquardt, and (c)minpack performs VERY well compared to those, both in accuracy and speed.

The documentation for running the tests is on the wiki http://eigen.tuxfamily.org/index.php?title=Tests
API : overview of methods

Both algorithms can use either the jacobian (provided by the user) or compute an approximation by themselves (actually using Eigen Numerical differentiation module). The part of API referring to the latter use 'NumericalDiff' in the method names (exemple: LevenbergMarquardt.minimizeNumericalDiff() )

The methods LevenbergMarquardt.lmder1()/lmdif1()/lmstr1() and HybridNonLinearSolver.hybrj1()/hybrd1() are specific methods from the original minpack package that you probably should NOT use until you are porting a code that was previously using minpack. They just define a 'simple' API with default values for some parameters.

All algorithms are provided using Two APIs :

one where the user inits the algorithm, and uses '*OneStep()' as much as he wants : this way the caller have control over the steps

one where the user just calls a method (optimize() or solve()) which will handle the loop: init + loop until a stop condition is met. Those are provided for convenience.

As an example, the method LevenbergMarquardt::minimize() is implemented as follow :

\lstset{language={C++}}
\begin{lstlisting}
Status LevenbergMarquardt<FunctorType,Scalar>::minimize(FVectorType &x, const int mode)
{
Status status = minimizeInit(x, mode);
do {
status = minimizeOneStep(x, mode);
} while (status==Running);
return status;
}
\end{lstlisting}


The easiest way to understand how to use this module is by looking at the many examples in the file unsupported/test/NonLinearOptimization.cpp. 

\section{Multidimensional Rootfinding: Powell Hybrid}
This is a modified version of Powell’s Hybrid method as implemented in the hybrj
algorithm in minpack.  The Hybrid algorithm retains the fast convergence of Newton’s
method but will also reduce the residual when Newton’s method is unreliable.
The algorithm uses a generalized trust region to keep each step under control. In order
to be accepted a proposed new position $x′$ must satisfy the condition $|D(x′ −x)| < \delta$,
where $D$ is a diagonal scaling matrix and $\delta$ is the size of the trust region. The
components of $D$ are computed internally, using the column norms of the Jacobian
to estimate the sensitivity of the residual to each component of $x$. This improves the
behavior of the algorithm for badly scaled functions.
On each iteration the algorithm first determines the standard Newton step by solving
the system $Jdx = −f$. If this step falls inside the trust region it is used as a trial step
in the next stage. If not, the algorithm uses the linear combination of the Newton
and gradient directions which is predicted to minimize the norm of the function while
\begin{equation}
dx = −\alpha J^{−1}f(x) − \beta\nabla|f(x)|^2.
\end{equation}
staying inside the trust region,

This combination of Newton and gradient directions is referred to as a dogleg step.
The proposed step is now tested by evaluating the function at the resulting point, $x′$.
If the step reduces the norm of the function sufficiently then it is accepted and size of
the trust region is increased. If the proposed step fails to improve the solution then
the size of the trust region is decreased and another trial step is computed.
The speed of the algorithm is increased by computing the changes to the Jacobian approximately,
using a rank-1 update. If two successive attempts fail to reduce the residual
then the full Jacobian is recomputed. The algorithm also monitors the progress
of the solution and returns an error if several steps fail to make any improvement.







\section{Nonlinear LeastSquares: Levenberg-Marquardt }
The minimization algorithms described in this section make use of both the function and
its derivative. They require an initial guess for the location of the minimum. There is no
absolute guarantee of convergence—the function must be suitable for this technique and
the initial guess must be sufficiently close to the minimum for it to work.


lmsder [Derivative Solver]
This is a robust and efficient version of the Levenberg-Marquardt algorithm as implemented
in the scaled lmder routine in minpack. Minpack was written by Jorge
J. Mor´e, Burton S. Garbow and Kenneth E. Hillstrom.


The algorithm uses a generalized trust region to keep each step under control. In order
to be accepted a proposed new position $x′$ must satisfy the condition $|D(x′ −x)| < \delta$,
where D is a diagonal scaling matrix and $\delta$ is the size of the trust region. The
components of D are computed internally, using the column norms of the Jacobian
to estimate the sensitivity of the residual to each component of x. This improves the
behavior of the algorithm for badly scaled functions.


On each iteration the algorithm attempts to minimize the linear system $|F + Jp|$ subject to the constraint $|Dp| < $. The solution to this constrained linear system is found using the Levenberg-Marquardt method.

The proposed step is now tested by evaluating the function at the resulting point,
$x′$. If the step reduces the norm of the function sufficiently, and follows the predicted
behavior of the function within the trust region, then it is accepted and the size of
the trust region is increased. If the proposed step fails to improve the solution, or
differs significantly from the expected behavior within the trust region, then the size
of the trust region is decreased and another trial step is computed.
The algorithm also monitors the progress of the solution and returns an error if the
changes in the solution are smaller than the machine precision. The possible error
codes are,







\chapter{Procedures based on NLOPT}
\label{NonlinearOptimization} 

\section{Overview}

Reference to NLOPT is  is \cite{Johnson2012}


Nomenclature

Each algorithm in NLopt is identified by a named constant, which is passed to the NLopt routines in the various languages in order to select a particular algorithm. These constants are mostly of the form NLOPT\_{G,L}{N,D}\_xxxx, where G/L denotes global/local optimization and N/D denotes derivative-free/gradient-based algorithms, respectively. 

For example, the NLOPT\_LN\_COBYLA constant refers to the COBYLA algorithm (described below), which is a local (L) derivative-free (N) optimization algorithm. 

Two exceptions are the MLSL and augmented Lagrangian algorithms, denoted by NLOPT\_G\_MLSL and NLOPT\_AUGLAG, since whether or not they use derivatives (and whether or not they are global, in AUGLAG's case) is determined by what subsidiary optimization algorithm is specified. 

Many of the algorithms have several variants, which are grouped together below. 

Comparing algorithms

For any given optimization problem, it is a good idea to compare several of the available algorithms that are applicable to that problem—in general, one often finds that the "best" algorithm strongly depends upon the problem at hand. 

However, comparing algorithms requires a little bit of care because the function-value/parameter tolerance tests are not all implemented in exactly the same way for different algorithms. So, for example, the same fractional $10^{−4}$ tolerance on the function value might produce a much more accurate minimum in one algorithm compared to another, and matching them might require some experimentation with the tolerances. 

Instead, a more fair and reliable way to compare two different algorithms is to run one until the function value is converged to some value fA, and then run the second algorithm with the $minf_max$ termination test set to $minf_max=fA$. That is, ask how long it takes for the two algorithms to reach the same function value. 

Better yet, run some algorithm for a really long time until the minimum fM is located to high precision. Then run the different algorithms you want to compare with the termination test: $minf_max=fM+\Delta f$. That is, ask how long it takes for the different algorithms to obtain the minimum to within an absolute tolerance $\Delta f$, for some $\Delta f$. (This is totally different from using the $ftol_abs$ termination test, because the latter uses only a crude estimate of the error in the function values, and moreover the estimate varies between algorithms.) 




\section{Global optimization}
All of the global-optimization algorithms currently require you to specify bound constraints on all the optimization parameters. Of these algorithms, only ISRES and ORIG\_DIRECT support nonlinear inequality constraints, and only ISRES supports nonlinear equality constraints. (However, any of them can be applied to nonlinearly constrained problems by combining them with the augmented Lagrangian method below.) 

Something you should consider is that, after running the global optimization, it is often worthwhile to then use the global optimum as a starting point for a local optimization to "polish" the optimum to a greater accuracy. (Many of the global optimization algorithms devote more effort to searching the global parameter space than in finding the precise position of the local optimum accurately.) 




\subsection{DIRECT and DIRECT-L}
DIRECT is the DIviding RECTangles algorithm for global optimization, described in \cite{Jones_1993}

and DIRECT-L is the "locally biased" variant proposed by \cite{Gablonsky_2001}

These is are deterministic-search algorithms based on systematic division of the search domain into smaller and smaller hyperrectangles. The Gablonsky version makes the algorithm "more biased towards local search" so that it is more efficient for functions without too many local minima. NLopt contains several implementations of both of these algorithms. I would tend to try NLOPT\_GN\_DIRECT\_L first; YMMV. 

First, it contains a from-scratch re-implementation of both algorithms, specified by the constants NLOPT\_GN\_DIRECT and NLOPT\_GN\_DIRECT\_L, respectively. 

Second, there is a slightly randomized variant of DIRECT-L, specified by NLOPT\_GLOBAL\_DIRECT\_L\_RAND, which uses some randomization to help decide which dimension to halve next in the case of near-ties. 

The DIRECT and DIRECT-L algorithms start by rescaling the bound constraints to a hypercube, which gives all dimensions equal weight in the search procedure. If your dimensions do not have equal weight, e.g. if you have a "long and skinny" search space and your function varies at about the same speed in all directions, it may be better to use unscaled variants of these algorthms, which are specified as NLOPT\_GLOBAL\_DIRECT\_NOSCAL, NLOPT\_GLOBAL\_DIRECT\_L\_NOSCAL, and NLOPT\_GLOBAL\_DIRECT\_L\_RAND\_NOSCAL, respectively. However, the unscaled variations make the most sense (if any) with the original DIRECT algorithm, since the design of DIRECT-L to some extent relies on the search region being a hypercube (which causes the subdivided hyperrectangles to have only a small set of side lengths). 

Finally, NLopt also includes separate implementations based on the original Fortran code by Gablonsky et al. (1998-2001), which are specified as NLOPT\_GN\_ORIG\_DIRECT and NLOPT\_GN\_ORIG\_DIRECT\_L. These implementations have a number of hard-coded limitations on things like the number of function evaluations; I removed several of these limitations, but some remain. On the other hand, there seem to be slight differences between these implementations and mine; most of the time, the performance is roughly similar, but occasionally Gablonsky's implementation will do significantly better than mine or vice versa. 

Most of the above algorithms only handle bound constraints, and in fact require finite bound constraints (they are not applicable to unconstrained problems). They do not handle arbitrary nonlinear constraints. However, the ORIG versions by Gablonsky et al. include some support for arbitrary nonlinear inequality constraints. 



\subsection{Controlled Random Search (CRS) with local mutation}
My implementation of the "controlled random search" (CRS) algorithm (in particular, the CRS2 variant) with the "local mutation" modification, as defined by: 
\cite{Kaelo_2006}. 

The original CRS2 algorithm was described by: \cite{Price_1978, Price_1983} 


The CRS algorithms are sometimes compared to genetic algorithms, in that they start with a random "population" of points, and randomly "evolve" these points by heuristic rules. In this case, the "evolution" somewhat resembles a randomized Nelder-Mead algorithm. The published results for CRS seem to be largely empirical; limited analytical results about its convergence were derived in \cite{Hendrix_2001}


The initial population size for CRS defaults to 10×(n+1) in n dimensions, but this can be changed with the nlopt\_set\_stochastic\_population function; the initial population must be at least n+1. 

Only bound-constrained problems are supported by this algorithm. 

CRS2 with local mutation is specified in NLopt as NLOPT\_GN\_CRS2\_LM. 




\subsection{MLSL (Multi-Level Single-Limkage)}
This is my implementation of the "Multi-Level Single-Linkage" (MLSL) algorithm for global optimization by a sequence of local optimizations from random starting points, proposed by: \cite{RinnooyKan_1987a, RinnooyKan_1987b}


We also include a modification of MLSL use a Sobol' low-discrepancy sequence (LDS) instead of pseudorandom numbers, which was argued to improve the convergence rate by: \cite{Kucherenko_2005}


In either case, MLSL is a "multistart" algorithm: it works by doing a sequence of local optimizations (using some other local optimization algorithm) from random or low-discrepancy starting points. MLSL is distinguished, however by a "clustering" heuristic that helps it to avoid repeated searches of the same local optima, and has some theoretical guarantees of finding all local optima in a finite number of local minimizations. 

The local-search portion of MLSL can use any of the other algorithms in NLopt, and in particular can use either gradient-based (D) or derivative-free algorithms (N) The local search uses the derivative/nonderivative algorithm set by nlopt\_opt\_set\_local\_optimizer. 

LDS-based MLSL with is specified as NLOPT\_G\_MLSL\_LDS, while the original non-LDS original MLSL (using pseudo-random numbers, currently via the Mersenne twister algorithm) is indicated by NLOPT\_G\_MLSL. In both cases, you must specify the local optimization algorithm (which can be gradient-based or derivative-free) via nlopt\_opt\_set\_local\_optimizer. 

Note: If you do not set a stopping tolerance for your local-optimization algorithm, MLSL defaults to ftol\_rel=10−15 and xtol\_rel=10−7 for the local searches. Note that it is perfectly reasonable to set a relatively large tolerance for these local searches, run MLSL, and then at the end run another local optimization with a lower tolerance, using the MLSL result as a starting point, to "polish off" the optimum to high precision. 

By default, each iteration of MLSL samples 4 random new trial points, but this can be changed with the nlopt\_set\_population function. 

Only bound-constrained problems are supported by this algorithm. 




\subsection{StoGO}
This is an algorithm adapted from the code downloaded from 

StoGO global optimization library (link broken as of Nov. 2009, and the software seems absent from the author's web site) 
by Madsen et al. StoGO is a global optimization algorithm that works by systematically dividing the search space (which must be bound-constrained) into smaller hyper-rectangles via a branch-and-bound technique, and searching them by a gradient-based local-search algorithm (a BFGS variant), optionally including some randomness (hence the "Sto", which stands for "stochastic" I believe). 

StoGO is written in C++, which means that it is only included when you compile the C++ algorithms enabled, in which case (on Unix) you must link to -lnlopt\_cxx instead of -lnlopt. 

StoGO is specified within NLopt by NLOPT\_GD\_STOGO, or NLOPT\_GD\_STOGO\_RAND for the randomized variant. 

Some references on StoGO are: \cite{Gudmundsson1998}, \cite{Madsen_1998}, \cite{Zertchaninov1998}


Only bound-constrained problems are supported by this algorithm. 





\subsection{ISRES (Improved Stochastic Ranking Evolution Strategy)}
This is my implementation of the "Improved Stochastic Ranking Evolution Strategy" (ISRES) algorithm for nonlinearly-constrained global optimization (or at least semi-global; although it has heuristics to escape local optima, I'm not aware of a convergence proof), based on the method described in: \cite{Runarsson_2005}


It is a refinement of an earlier method described in: \cite{Runarsson_2000}



This is an independent implementation by S. G. Johnson (2009) based on the papers above. Runarsson also has his own Matlab implemention available from his web page here. 

The evolution strategy is based on a combination of a mutation rule (with a log-normal step-size update and exponential smoothing) and differential variation (a Nelder–Mead-like update rule). The fitness ranking is simply via the objective function for problems without nonlinear constraints, but when nonlinear constraints are included the stochastic ranking proposed by Runarsson and Yao is employed. The population size for ISRES defaults to 20×(n+1) in n dimensions, but this can be changed with the nlopt\_set\_stochastic\_population function. 

This method supports arbitrary nonlinear inequality and equality constraints in addition to the bound constraints, and is specified within NLopt as NLOPT\_GN\_ISRES. 





\section{Local derivative-free optimization}
\label{LocalDerivativeFreeOptimization}

Of these algorithms, only COBYLA currently supports arbitrary nonlinear inequality and equality constraints; the rest of them support bound-constrained or unconstrained problems only. (However, any of them can be applied to nonlinearly constrained problems by combining them with the augmented Lagrangian method below.) 


\subsection{COBYLA (Constrained Optimization BY Linear Approximations)}
This is a derivative of Powell's implementation of the COBYLA (Constrained Optimization BY Linear Approximations) algorithm for derivative-free optimization with nonlinear inequality and equality constraints, by M. J. D. Powell, described in: \cite{Powell_1994}


and reviewed in: \cite{Powell_1998}


It constructs successive linear approximations of the objective function and constraints via a simplex of n+1 points (in n dimensions), and optimizes these approximations in a trust region at each step. 

The original code itself was written in Fortran by Powell and was converted to C in 2004 by Jean-Sebastien Roy (js@jeannot.org) for the SciPy project. The version in NLopt was based on Roy's C version, downloaded from: 

http://www.jeannot.org/~js/code/index.en.html\#COBYLA 
NLopt's version is slightly modified in a few ways. First, we incorporated all of the NLopt termination criteria. Second, we added explicit support for bound constraints (although the original COBYLA could handle bound constraints as linear constraints, it would sometimes take a step that violated the bound constraints). Third, we allow COBYLA to increase the trust-region radius if the predicted improvement was approximately right and the simplex is OK, following a suggestion in the SAS manual for PROC NLP that seems to improve convergence speed. Fourth, we pseudo-randomize simplex steps in COBYLA algorithm, improving robustness by avoiding accidentally taking steps that don't improve conditioning (which seems to happen sometimes with active bound constraints); the algorithm remains deterministic (a deterministic seed is used), however. Also, we support unequal initial-step sizes in the different parameters (by the simple expedient of internally rescaling the parameters proportional to the initial steps), which is important when different parameters have very different scales. 

(The underlying COBYLA code only supports inequality constraints. Equality constraints are automatically transformed into pairs of inequality constraints, which in the case of this algorithm seems not to cause problems.) 

It is specified within NLopt as NLOPT\_LN\_COBYLA. 





\subsection{BOBYQA}
This is an algorithm derived from the BOBYQA subroutine of M. J. D. Powell, converted to C and modified for the NLopt stopping criteria. BOBYQA performs derivative-free bound-constrained optimization using an iteratively constructed quadratic approximation for the objective function. See: \cite{Powell2009}


(Because BOBYQA constructs a quadratic approximation of the objective, it may perform poorly for objective functions that are not twice-differentiable.) 

The NLopt BOBYQA interface supports unequal initial-step sizes in the different parameters (by the simple expedient of internally rescaling the parameters proportional to the initial steps), which is important when different parameters have very different scales. 

This algorithm, specified in NLopt as NLOPT\_LN\_BOBYQA, largely supersedes the NEWUOA algorithm below, which is an earlier version of the same idea by Powell. 




\subsection{NEWUOA + bound constraints}
This is an algorithm derived from the NEWUOA subroutine of M. J. D. Powell, converted to C and modified for the NLopt stopping criteria. I also modified the code to include a variant, NEWUOA-bound, that permits efficient handling of bound constraints. This algorithm is largely superseded by BOBYQA (above). 

The original NEWUOA performs derivative-free unconstrained optimization using an iteratively constructed quadratic approximation for the objective function. See: \cite{Powell_2004}



(Because NEWUOA constructs a quadratic approximation of the objective, it may perform poorly for objective functions that are not twice-differentiable.) 

The original algorithm is specified in NLopt as NLOPT\_LN\_NEWUOA, and only supports unconstrained problems. For bound constraints, my variant is specified as NLOPT\_LN\_NEWUOA\_BOUND. 

In the original NEWUOA algorithm, Powell solved the quadratic subproblems (in routines TRSAPP and BIGLAG) in a spherical trust region via a truncated conjugate-gradient algorithm. In my bound-constrained variant, we use the MMA algorithm for these subproblems to solve them with both bound constraints and a spherical trust region. In principle, we should also change the BIGDEN subroutine in a similar way (since BIGDEN also approximately solves a trust-region subproblem), but instead I just truncated its result to the bounds (which probably gives suboptimal convergence, but BIGDEN is called only very rarely in practice). 

Shortly after my addition of bound constraints to NEWUOA, Powell released his own version of NEWUOA modified for bound constraints as well as some numerical-stability and convergence enhancements, called BOBYQA. NLopt now incorporates BOBYQA as well, and it seems to largely supersede NEWUOA. 

Note: NEWUOA requires the dimension n of the parameter space to be $\geq 2$, i.e. the implementation does not handle one-dimensional optimization problems. 



\subsection{PRAXIS (Principal AXIS)}
"PRAXIS" gradient-free local optimization via the "principal-axis method" of Richard Brent, based on a C translation of Fortran code downloaded from Netlib: 

http://netlib.org/opt/praxis 
The original Fortran code was written by Richard Brent and made available by the Stanford Linear Accelerator Center, dated 3/1/73. The appropriate reference seems to be: \cite{Brent_1972}


Specified in NLopt as NLOPT\_LN\_PRAXIS 

This algorithm was originally designed for unconstrained optimization. In NLopt, bound constraints are "implemented" in PRAXIS by the simple expedient of returning infinity (Inf) when the constraints are violated (this is done automatically—you don't have to do this in your own function). This seems to work, more-or-less, but appears to slow convergence significantly. If you have bound constraints, you are probably better off using COBYLA or BOBYQA. 





\subsection{Nelder-Mead Simplex}
My implementation of almost the original Nelder-Mead simplex algorithm (specified in NLopt as NLOPT\_LN\_NELDERMEAD), as described in: \cite{Nelder_1965}


This method is simple and has demonstrated enduring popularity, despite the later discovery that it fails to converge at all for some functions (and examples may be constructed in which it converges to point that is not a local minimum). Anecdotal evidence suggests that it often performs well even for noisy and/or discontinuous objective functions. I would tend to recommend the Subplex method (below) instead, however. 

The main change compared to the 1965 paper is that I implemented explicit support for bound constraints, using essentially the method proposed in: \cite{Box_1965} 


and later reviewed in: \cite{Richardson_1973}


Whenever a new point would lie outside the bound constraints, Box advocates moving it "just inside" the constraints by some fixed "small" distance of 10−8 or so. I couldn't see any advantage to using a fixed distance inside the constraints, especially if the optimum is on the constraint, so instead I move the point exactly onto the constraint in that case. The danger with implementing bound constraints in this way (or by Box's method) is that you may collapse the simplex into a lower-dimensional subspace. I'm not aware of a better way, however. In any case, this collapse of the simplex is somewhat ameliorated by restarting, such as when Nelder-Mead is used within the Subplex algorithm below. 





\subsection{Sbplx (based on Subplex)}
This is my re-implementation of Tom Rowan's "Subplex" algorithm. As Rowan expressed a preference that other implementations of his algorithm use a different name, I called my implementation "Sbplx" (referred to in NLopt as NLOPT\_LN\_SBPLX). 

Subplex (a variant of Nelder-Mead that uses Nelder-Mead on a sequence of subspaces) is claimed to be much more efficient and robust than the original Nelder-Mead, while retaining the latter's facility with discontinuous objectives, and in my experience these claims seem to be true in many cases. (However, I'm not aware of any proof that Subplex is globally convergent, and perhaps it may fail for some objectives like Nelder-Mead; YMMV.) 

I used the description of Rowan's algorithm in his PhD thesis: \cite{Rowan_1990}

I would have preferred to use Rowan's original implementation, posted by him on Netlib: 

http://www.netlib.org/opt/subplex.tgz 
Unfortunately, the legality of redistributing or modifying this code is unclear, because it lacks anything resembling a license statement. After some friendly emails with Rowan in which he promised to consider providing a clear open-source/free-software license, I lost touch with him and his old email address now seems invalid. 

Since the algorithm is not too complicated, however, I just rewrote it. There seem to be slight differences between the behavior of my implementation and his (probably due to different choices of initial subspace and other slight variations, where his paper was ambiguous), but the number of iterations to converge on my test problems seems to be quite close (within ±10\% of the number of function evaluations for most problems). 

The only major difference between my implementation and Rowan's, as far as I can tell, is that I implemented explicit support for bound constraints (via the method in the Box paper as described above). This seems to be a big improvement in the case where the optimum lies against one of the constraints. 




\section{Local gradient-based optimization}
\label{LocalGradientBasedOptimization}

Of these algorithms, only MMA and SLSQP support arbitrary nonlinear inequality constraints, and only SLSQP supports nonlinear equality constraints; the rest support bound-constrained or unconstrained problems only. (However, any of them can be applied to nonlinearly constrained problems by combining them with the augmented Lagrangian method below.) 


\subsection{MMA (Method of Moving Asymptotes) and CCSA}
My implementation of the globally-convergent method-of-moving-asymptotes (MMA) algorithm for gradient-based local optimization, including nonlinear inequality constraints (but not equality constraints), specified in NLopt as NLOPT\_LD\_MMA, as described in: \cite{Svanberg_2002}


This is an improved CCSA ("conservative convex separable approximation") variant of the original MMA algorithm published by Svanberg in 1987, which has become popular for topology optimization. (Note: "globally convergent" does not mean that this algorithm converges to the global optimum; it means that it is guaranteed to converge to some local minimum from any feasible starting point.) 

At each point x, MMA forms a local approximation using the gradient of f and the constraint functions, plus a quadratic "penalty" term to make the approximations "conservative" (upper bounds for the exact functions). The precise approximation MMA forms is difficult to describe in a few words, because it includes nonlinear terms consisting of a poles at some distance from x (outside of the current trust region), almost a kind of Pade approximant. The main point is that the approximation is both convex and separable, making it trivial to solve the approximate optimization by a dual method. Optimizing the approximation leads to a new candidate point x. The objective and constraints are evaluated at the candidate point. If the approximations were indeed conservative (upper bounds for the actual functions at the candidate point), then the process is restarted at the new x. Otherwise, the approximations are made more conservative (by increasing the penalty term) and re-optimized. 

(If you contact Professor Svanberg, he has been willing in the past to graciously provide you with his original code, albeit under restrictions on commercial use or redistribution. The MMA implementation in NLopt, however, is completely independent of Svanberg's, whose code we have not examined; any bugs are my own, of course.) 

I also implemented another CCSA algorithm from the same paper, NLOPT\_LD\_CCSAQ: instead of constructing local MMA approximations, it constructs simple quadratic approximations (or rather, affine approximations plus a quadratic penalty term to stay conservative). This is the ccsa\_quadratic code. It seems to have similar convergence rates to MMA for most problems, which is not surprising as they are both essentially similar. However, for the quadratic variant I implemented the possibility of preconditioning: including a user-supplied Hessian approximation in the local model. It is easy to incorporate this into the proof in Svanberg's paper, and to show that global convergence is still guaranteed as long as the user's "Hessian" is positive semidefinite, and it practice it can greatly improve convergence if the preconditioner is a good approximation for the real Hessian (at least for the eigenvectors of the largest eigenvalues). 



\subsection{SLSQP}
Specified in NLopt as NLOPT\_LD\_SLSQP, this is a sequential quadratic programming (SQP) algorithm for nonlinearly constrained gradient-based optimization (supporting both inequality and equality constraints), based on the implementation by Dieter Kraft and described in: 

\cite{Kraft_1988, Kraft_1994}

(I believe that SLSQP stands for something like "Sequential Least-Squares Quadratic Programming," because the problem is treated as a sequence of constrained least-squares problems, but such a least-squares problem is equivalent to a QP.) The algorithm optimizes successive second-order (quadratic/least-squares) approximations of the objective function (via BFGS updates), with first-order (affine) approximations of the constraints. 

The Fortran code was obtained from the SciPy project, who are responsible for obtaining permission to distribute it under a free-software (3-clause BSD) license. 

The code was modified for inclusion in NLopt by S. G. Johnson in 2010, with the following changes. The code was converted to C and manually cleaned up. It was modified to be re-entrant (preserving the reverse-communication interface but explicitly saving the state in a data structure). The reverse-communication interface was wrapped with an NLopt-style interface, with NLopt stopping conditions. The inexact line search was modified to evaluate the functions including gradients for the first step, since this removes the need to evaluate the function+gradient a second time for the same point in the common case when the inexact line search concludes after a single step; this is motivated by the fact that NLopt's interface combines the function and gradient computations. Since roundoff errors sometimes pushed SLSQP's parameters slightly outside the bound constraints (not allowed by NLopt), we added checks to force the parameters within the bounds. We fixed a bug in the LSEI subroutine (use of uninitialized variables) for the case where the number of equality constraints equals the dimension of the problem. The LSQ subroutine was modified to handle infinite lower/upper bounds (in which case those constraints are omitted). 

Note: Because the SLSQP code uses dense-matrix methods (ordinary BFGS, not low-storage BFGS), it requires $O(n^2)$ storage and $O(n^3)$ time in $n$ dimensions, which makes it less practical for optimizing more than a few thousand parameters



\subsection{Low-storage BFGS}
This algorithm in NLopt (specified by NLOPT\_LD\_LBFGS), is based on a Fortran implementation of the low-storage BFGS algorithm written by Prof. Ladislav Luksan, and graciously posted online under the GNU LGPL at: 

http://www.uivt.cas.cz/~luksan/subroutines.html 
The original L-BFGS algorithm, based on variable-metric updates via Strang recurrences, was described by the papers: 

\cite{Nocedal_1980} and \cite{Liu_1989}.


I converted Prof. Luksan's code to C with the help of f2c, and made a few minor modifications (mainly to include the NLopt termination criteria). 

One of the parameters of this algorithm is the number M of gradients to "remember" from previous optimization steps: increasing M increases the memory requirements but may speed convergence. NLopt sets M to a heuristic value by default, but this can be changed by the set\_vector\_storage function. 





\subsection{Preconditioned truncated Newton}
This algorithm in NLopt, is based on a Fortran implementation of a preconditioned inexact truncated Newton algorithm written by Prof. Ladislav Luksan, and graciously posted online under the GNU LGPL at: 

http://www.uivt.cas.cz/~luksan/subroutines.html 

NLopt includes several variations of this algorithm by Prof. Luksan. First, a variant preconditioned by the low-storage BFGS algorithm with steepest-descent restarting, specified as NLOPT\_LD\_TNEWTON\_PRECOND\_RESTART. Second, simplified versions NLOPT\_LD\_TNEWTON\_PRECOND (same without restarting), NLOPT\_LD\_TNEWTON\_RESTART (same without preconditioning), and NLOPT\_LD\_TNEWTON (same without restarting or preconditioning). 

The algorithms are based on the ones described by: \cite{Dembo_1982}

I converted Prof. Luksan's code to C with the help of f2c, and made a few minor modifications (mainly to include the NLopt termination criteria). 

One of the parameters of this algorithm is the number M of gradients to "remember" from previous optimization steps: increasing M increases the memory requirements but may speed convergence. NLopt sets M to a heuristic value by default, but this can be changed by the set\_vector\_storage function. 





\subsection{Shifted limited-memory variable-metric}
This algorithm in NLopt, is based on a Fortran implementation of a shifted limited-memory variable-metric algorithm by Prof. Ladislav Luksan, and graciously posted online under the GNU LGPL at: 

http://www.uivt.cas.cz/~luksan/subroutines.html 
There are two variations of this algorithm: NLOPT\_LD\_VAR2, using a rank-2 method, and NLOPT\_LD\_VAR1, using a rank-1 method. 

The algorithms are based on the ones described by: \cite{Vlcek_2006}



I converted Prof. Luksan's code to C with the help of f2c, and made a few minor modifications (mainly to include the NLopt termination criteria). 

One of the parameters of this algorithm is the number M of gradients to "remember" from previous optimization steps: increasing M increases the memory requirements but may speed convergence. NLopt sets M to a heuristic value by default, but this can be changed by the set\_vector\_storage function. 




\section{NLOPT: Augmented Lagrangian algorithm}
\label{AugmentedLagrangian}

\subsection{Implementation}
There is one algorithm in NLopt that fits into all of the above categories, depending on what subsidiary optimization algorithm is specified, and that is the augmented Lagrangian method described in: \cite{Conn_1991} and \cite{Birgin_2008}



This method combines the objective function and the nonlinear inequality/equality constraints (if any) in to a single function: essentially, the objective plus a "penalty" for any violated constraints. This modified objective function is then passed to another optimization algorithm with no nonlinear constraints. If the constraints are violated by the solution of this sub-problem, then the size of the penalties is increased and the process is repeated; eventually, the process must converge to the desired solution (if it exists). 

The subsidiary optimization algorithm is specified by the nlopt\_set\_local\_optimizer function, described in the NLopt Reference. (Don't forget to set a stopping tolerance for this subsidiary optimizer!) Since all of the actual optimization is performed in this subsidiary optimizer, the subsidiary algorithm that you specify determines whether the optimization is gradient-based or derivative-free. In fact, you can even specify a global optimization algorithm for the subsidiary optimizer, in order to perform global nonlinearly constrained optimization (although specifying a good stopping criterion for this subsidiary global optimizer is tricky). 

The augmented Lagrangian method is specified in NLopt as NLOPT\_AUGLAG. We also provide a variant, NLOPT\_AUGLAG\_EQ, that only uses penalty functions for equality constraints, while inequality constraints are passed through to the subsidiary algorithm to be handled directly; in this case, the subsidiary algorithm must handle inequality constraints (e.g. MMA or COBYLA). 

While NLopt uses an independent re-implementation of the Birgin and Martínez algorithm, those authors provide their own free-software implementation of the method as part of the TANGO project, and implementations can also be found in semi-free packages like LANCELOT. 






